\input{../../common/summary.tex}

\title{Applied Bayesian Statistics Summary}

\begin{document}

\maketitle

\section{Probability and Bayes theorem for discrete observables}
\label{sec:prob-bayes-theorobservables}

\begin{defn}
  \label{sec:prob-bayes-theor-1}
  Suppose we want to predict a random quanitity $X$, and we do so by
  providign a probability distribution $P$.  Suppose we observed a
  specific value $x$, then a scoring rule $S$ provides a reward $S(P,
  x)$.  If the true distribution of $X$ is $Q$, then the expected
  score is denoted $S(P, Q)$, where $S(P, Q) = int SP(, x) Q(x) dx$.
  A proper scoring rule has $S(Q, Q) \geq S(P, Q)$ for all $P$, and is
  \textbf{strictly proper} if $S(Q, Q) = S(P, Q)$ if and only if $P = Q.$
\end{defn}

\begin{thm}
  \label{sec:prob-bayes-theor-2}
  For a null hypothesis $H_{0}, H_{1}$ as ``not $H_{0}$'',
  \begin{align}
    \label{eq:1}
    \frac{p(H_{0} | y)}{p(H_{1} | y)}  = \frac{p(y | H_{0})}{p(y |
      H_{1})} \times \frac{p(H_{0})}{p(H_{1})},
  \end{align}
  posterior odds equals the likelihood ratio times prior odds.
\end{thm}

\begin{defn}
  \label{sec:prob-bayes-theor-3}
  We have observed qunatities $y$ (the deta), have an unknonw
  qunaitity taking on a set of discrete values $\theta_{i}, i \in 1,
  \dots, n$.  We specify a sampling model $p(y | \theta)$, a
  probability distribution $p(\theta_{i})$, and together define $p(y,
  \theta_{i}) = p(y | \theta_{i}) p(\theta_{i})$ - a ``full
  probability model''.

  Then, use Bayes theorem to botain the conditional probability
  distribution for unobserved qunaitites given the data,
  \begin{equation}
    \label{eq:2}
    p(\theta_{i} | y) = \frac{p(y | \theta_{i})
      p(\theta_{i})}{\sum_{k}^{} p(y | \theta_{k}) p(\theta_{k})}
    \alpha p(y | \theta_{i}) p(\theta_{i})
  \end{equation}

  or equivalently, the posterior is propertional to the likeelihood
  times the prior.
\end{defn}

\begin{defn}
  \label{sec:prob-bayes-theor-4}
  $\theta \sim Beta(a, b)$ represents a Beta distribution with
  properties
  \begin{align}
    p(\theta | a, b) &= \frac{\Gamma(a, b)}{\Gamma(a) \Gamma(b)} \theta^{a-1} (1 - \theta)^{b-1}), \theta \in (0, 1) \\
    \E{\theta \ a, b} &= \frac{a}{a+b} \\
    \V{\theta | a, b} &= \frac{ab}{(a+b)^{2}(a+b+1)} \\
    \text{mode} &= \frac{a-1}{a+b - 2}  (a, b > 0) \\
  \end{align}
  where $\Gamma(a) = (a-1)!$ is $a$ is an integer.
\end{defn}

\begin{thm}
  \label{sec:prob-bayes-theor-5}
  Our parameteric sampling distribution $p(y | \theta)$ with
  uncertainty about $\theta$ given by a distribution $p(\theta)$ gives
  a predictive distribution $p(y) = \int p(y| \theta) p(\theta)
  d\theta$. 
  The mean and variance of a predictive distribution can be obtained
  using
  \begin{align}
    \label{eq:4}
    \E{Y} = \E_{\theta}(\E{Y | \theta}) \\
    \V{Y} = \E_{\theta}(\V{Y | \theta}) + \V_{\theta}(\E{Y | \theta})
  \end{align}
\end{thm}

\begin{thm}
  \label{sec:prob-bayes-theor-6}
  For two random variables with joint density $p(x, y)$, then $\E{Y} =
  \E_{X}(\E{Y | x})$ and $\V{Y} = \E_{X}(\V{Y | x}) + \V_{X}(\E{Y|x})$.
\end{thm}

\begin{defn}
  \label{sec:prob-bayes-theor-7}
  Suppose $\theta \sim Beta(a, b)$, $Y \sim Binomial(\theta, n)$.  The
  exact predictive distribution for $Y$ is known as the
  \textbf{Beta-Binomial} with
  \begin{equation}
    \label{eq:5}
    p(y) = \frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)}  {n \choose y}
    \frac{\Gamma(a+y) \Gamma(b + n - y)}{\Gamma(a + b + n)}, y = 0,1,2,\dots,n
  \end{equation}
  If $a = b = 1$ (the prior is uniform on $0, 1$), then $p(y)$ is
  uniform on $0, 1, \dots, n$.

  The mean and variance of the beta-binomial is given as $\E{Y} =
  \frac{na}{a+b}$ and $\V{Y} = n \frac{ab}{(a+b)^{2}} \frac{(n+a+b)}{(1+a+b)}$
\end{defn}

\begin{defn}
  \label{sec:prob-bayes-theor-8}
  The Gamma distribution is a flexibile distibution for positive
  quantities.  If $Y \sim Gamma(a, b)$, then
  \begin{align}
    \label{eq:6}
    p(y|a,b) = \frac{b^{a}}{\Gamma(a)} y^{a-1} e^{-by}, y \in (0,
    \infty) \\
    \E{Y|a, b} = \frac{a, b}{den} \\
    \V(Y|a, b) = \frac{a}{b^{2}}
  \end{align}

  The $Gamma(1, b)$ distribution is exponential with mean
  $\frac{1}{b}$. The $Gamma(\frac{\nu}{2}, \frac{1}{2})$ is a
  chi-squared $\chi^{2}_{\nu}$ with $\nu$ degrees of freedom.
\end{defn}

\begin{thm}
  \label{sec:prob-bayes-theor-9}
  Suppose $\theta \sim Gamma(a, b)$, $Y \sim Poisson(\theta)$, then
  the exact predictive distribution of $Y$ is
  \textbf{negative-binomial} with
  \begin{align}
    \label{eq:8}
    p(y) = \frac{\Gamma(a+y)}{\Gamma(a) \Gamma(y+1)}
    \frac{b^{a}}{(b+1)^{a+y}}, y= 0, 1, 2, \dots \\
    \E{Y} = \frac{a}{b} \\
    \V{Y} = \frac{a}{b} + \frac{a}{b^{2}}
  \end{align}
\end{thm}

\begin{thm}
  \label{sec:prob-bayes-theor-10}
  Consider the general one-parameter exponential family
  \begin{equation}
    \label{eq:9}
    p(y|\theta) = \exp(a(y) + b(\theta) + u(\theta)t(y))
  \end{equation} where $u(\theta)$ is a \textbf{natural} or canoniacl
  parameter, and $t(y)$ is the \textbf{natural sufficient statistic}.

  Suppose we have a conjugate prior distribution of the form
  $p(\theta) = \frac{1}{c(n_{0}, t_{0})} \exp(n_{0} b(\theta) + t_{0}
  \mu(0))$ where $c(n_{0}, t_{0}) = \int \exp(n_{0} b(\theta) + t_{0}
  u(\theta)) d\theta$.  Then the predictive distribution iss
  \begin{align}
    \label{eq:10}
    p(y) = e^{a(y)} \frac{c(n_{0} + 1, t_{0} + t(y))}{c(n_{0}, t_{0})}.
  \end{align}
\end{thm}


\section{Conjugate Analysis}
\label{sec:conjugate-analysis}

\begin{thm}
  \label{sec:conjugate-analysis-1}
  SUppose we have a indenpendent sample of data $y_{i} \sim
  Normal(\mu, \sigma^{2})$, $i = 1, \dots, n$, with $\sigma^{2}$ known
  and $\mu$ unknown.  The conjugate prior for the normal mean is also
  normal, $\mu$, $\mu \sim Normal(\gamma, \tau^{2})$, where $\gamma$
  and $\tau^{2} = \frac{\sigma^{2}}{n_{0}} $ are specified.  The posterior distribution is
  \begin{align}
    \label{eq:11}
    p(\mu \ y) \alpha p(\mu) \prod_{i=1}^{n} p(y_{i} | \mu) =
    Normal(y_{n}, \tau^{2}_{n})
  \end{align} where $\gamma_{n} = \frac{n_{0} \gamma + n \overline
    y}{n_{0} + n}$ and $\tau^{2}_{n} = \frac{\sigma^{2}}{n_{0} + n}$.

  The posterior predictive distribution is thus $Normal(\gamma_{n},
  \sigma^{2} + \tau_{n}^{2})$.
\end{thm}

\begin{thm}
  \label{sec:conjugate-analysis-2}
  Suppose again $y_{i} \sim N(\mu, \sigma^{2})$, but $\mu$ is known
  $\sigma^{2}$ is unknown.

  If we use precision $\omega = \frac{1}{\sigma^{2}}$, we have the
  conjugate prior for $\omega$ as $\omega \sim Gamma(\alpha, \beta)$,
  so $p(\omega) \alpha w^{\alpha-1} \exp(-\beta \omega)$.
  $\sigma^{2}$ has an \textbf{inverse-gamma} distribution.

  The posterior distribution has the form $p(\omega | \mu, y) =
  Gamma(\alpha + \frac{n}{2}, \beta + \frac{1}{2} \sum_{i=1}^{n}
  (y_{i} - \mu)^{2})$.
\end{thm}

\begin{thm}
  \label{sec:conjugate-analysis-3}
  If we have $I$ possible prior distributions $p_{i}(\theta)$ with
  weights $q_{i}$, then the mixture prior is $p(\theta) = \sum_{i}^{}
  q_{i} p_{i}(\theta)$.  If we now observe data $y$, the posterior for
  $\theta$is $p(\theta | y) = q'_{i}p(\theta | y, H_{i})$, where
  $p(\theta | y, H_{i}) \alpha p(y | \theta) p(\theta | H_{i})$, where
  $q'_{i} = p(H_{i} | y) = \frac{q_{i} p(y | H_{i})}{\sum_{i} q_{i} p
    (y|H_{i})}$ where $p(y | H_{i}) = \int p(y | \theta) p(\theta |
  H_{i}) d\theta$ is the predictive probability of the data $y$
  assuming $H_{i}$.
\end{thm}

\begin{thm}
  \label{sec:conjugate-analysis-4}
  In a general one-parameter exponential family, we have $p(y |
  \theta) = \exp(\sum_{i} a(y_{i}) + nb(\theta) + u(\theta)
  \sum_{i}^{} t(y_{i}))$ and prior $p(\theta) \alpha \exp(n_{0}
  b(\theta) = t_{0} u(\theta))$ so the posterior distribution is
  \begin{equation}
    \label{eq:12}
    p(\theta | y) \alpha \exp((n + n_{0})b(\theta) +
    u(\theta)(\sum_{i}^{} t(y_{i}) + t_{0}))
  \end{equation} which is in the same family as the prior
  distribution.  $t_{0}$ can be thought of as the sum of $n_{0}$
  imaginary distributions.
\end{thm}

\section{BUGS/Monte Carlo sampling, etc.}
\label{sec:bugsm-carlo-sampl}

\todo{Fill this in?}

\section{Prior Distributions}
\label{sec:prior-distributions}

\begin{thm}
  \label{sec:prior-distributions-2}
  If $\frac{1}{\sigma^{2}}|y \sim Gamma(\alpha, \beta)$, then $\frac{2
    \beta}{\sigma^{2}} \sim \chi^{2}_{2 \alpha}$.

  If $Z \sim Normal(0, 1)$, $X \sim \frac{\chi^{2}_{\nu}}{\nu} \sim
  t_{\nu}$.
\end{thm}

\begin{defn}
  \label{sec:prior-distributions-3}
  A jeffreys prior is compatible with a Jeffrey's prior for any $1-1$
  transformation $\phi = f(\Theta)$.
\item $p_{}(\theta) \alpha I(\theta)^{\frac{1}{2}}$ where $I(\theta)$
  is the Fisher information for $\theta$,
  \begin{equation}
    \label{eq:13}
    I(\theta) = -\E_{Y|\theta}(\frac{\partial^{2} \log
      p(Y|\theta)}{\partial \theta^{2}}) =
    E_{Y|\theta}((\frac{\partial \log p(Y|\theta)}{\partial \theta})^{2})
  \end{equation}
  This is invariant to reparameterization as
  \begin{align}
    \label{eq:14}
    \E_{Y | \phi}(\frac{\partial \log p(Y| \phi)}{\partial
      \phi})^{2} = \E_{Y|\theta}(\frac{\partial \log
      p(Y|\theta)}{\partial \theta})^{2} |\frac{\partial
      \theta}{\partial \phi} |^{2} = I(\theta) | \frac{\partial
      \theta}{\partial \phi} |^{2}
  \end{align}
\end{defn}

\begin{defn}
  \label{sec:prior-distributions-4}
  For location parameters, $p(y|\theta)$ is a function of $y -
  \theta$, and the distribution of $y - \theta$ is independent of
  $\theta$, hence $p_{J}(\theta) \alpha C$ constant. Can us
  \texttt{dflat()} in winbugs or a proper distribution such as
  \texttt{dunif(-100, 100)}
\end{defn}

\begin{defn}
  \label{sec:prior-distributions-5}
  For count/rate parameters, the Fisher information for Poisson data
  is $I(\theta) = \frac{1}{\theta}$, and so the Jeffreys prior is
  $p_{J}(\theta) \alpha \frac{1}{\sqrt{\theta}}$, which can be
  approximated by a \texttt{dgamma(0.5, 0.000001)} distribution in
  BUGS.

  This same prior is appropriate if $\theta$ is a rate parameter per
  unit time - so $Y \sim Poission(\theta t)$.
\end{defn}

\begin{defn}
  \label{sec:prior-distributions-6}
  $\sigma$ is a scal parameter if $p(y | \sigma) = \frac{1}{\sigma}
  f(\frac{y}{\sigma})$ for some function $f$, so that the distribution
  of $\frac{Y}{\sigma} $ does not depend on $\sigma$. The Jeffreys
  prior is $p_{J}(\sigma) \alpha \sigma^{-1}$. This implies that
  $p_{j}(\sigma^{k}) \alpha \sigma^{-k}$, for any choice of $k$, and
  thus for the precision of the normal distribution, we should have
  $p_{J}(\omega) \alpha \omega^{-1}$, which can be approximated by
  \texttt{dgamma(0.0001, 0.0001)} in BUGS (an inverse-gamma
  distribution on the variance $\sigma^{2}$).
\end{defn}

\todo{Power calculations, etc?}

\section{Multivariate Distributions}
\label{sec:mult-distr}

\begin{defn}
\label{sec:mult-distr-1}
  Array of counts $(n_{1}, \dots, n_{k})$ in $K$ categories - the
  multinomial density is $p(n|q) = \frac{(\sum_{}^{} n_{k})!}{\prod
    n_{k}!} \prod_{k=1}^{K} q_{k}^{n_{k}}$, with likelihood
  propertional to $\prod_{k=1}^{K} q_{k}^{n_{k}}$. The conjugate prior
  is a $Dirichlet(\alpha_{1}, \dots, \alpha_{k})$ distribution with
  \begin{equation}
    \label{eq:15}
    p(q) = \frac{\Gamma(\sum_{}^{} \alpha_{k})}{\prod \Gamma(a_{k}!)}
    q_{k}^{a_{k} - 1}
  \end{equation}  with $\sum_{k}^{} q_{k} = 1$.  The posterior is $p(q|n
  = Dirichlet(\alpha_{1} + n_{1}, \dots, \alpha_{k}))$.  The Jeffreys
  prior is $p(q) \alpha \prod_{k} q_{k}^{-\frac{1}{2}}$.
\end{defn}

\begin{defn}
  \label{sec:mult-distr-2}
  The multivariate normal for a $p$-dimensional vector $y \sim
  Normal_{p}(\mu, \Sigma)$, or using $\Omega = \Sigma^{-1}$, so $p(y |
  \mu, \Omega) \alpha \exp(-\frac{1}{2} (y-\mu)^{T} \Omega (y -
  \mu))$, and conjugate prior for $\mu$ is also a multivariate normal,
  $p(\mu | \psi_{0}, \Omega_{0}) \alpha \exp(-\frac{1}{2} (\mu -
  \gamma_{0})^{T} \Omega_{0} (\mu - \gamma_{0}))$.

  We then have $\mu \sim Normal_{p}(\gamma_{n}, \Omega_{n}^{-1})$
  where $\Omega_{n} = \Omega_{0} + n \Omega$ and $\gamma_{n} =
  (\Omega_{0} + n \Omega)^{-1}(\Omega_{0} \gamma_{0} + n \Omega
  \overline y)$.
\end{defn}

\begin{defn}
  \label{sec:mult-distr-3}
  The conjugate prior on the precision matrix of a multivariate normal
  is the Wishart distribution (analogous to $Gamma$/$\chi^{2}$).

  The Wishart distribution $W_{p}(k, R)$ for a symmetric positive
  definite $p \times p$ matrix $\Omega$ is $p(\Omega) \alpha
  |R|^{\frac{k}{2}} |\Omega|^{\frac{k-p-1}{2}} \exp (-\frac{1}{2}
  \tr(R \Omega))$.

  The sampling density of a MVN with known mean and unknown matrix is
  $p(y_{1}, \dots, y_{n} | \mu, \Omega) \alpha |\Omega|^{\frac{n}{2}}
  \exp(-\frac{1}{2} \tr(S \Omega))$ where $S = \sum_{i}^{} (y_{i} -
  \mu)(y_{i} - \mu)^{T}$, and therefore
  \begin{equation}
    \label{eq:16}
    p(\Omega | y) \alpha |\Omega|^{\frac{n+k-p-1}{2} \exp(-\frac{1}{2} \tr((S+R)\Omega))}
  \end{equation} which is a $W_{p}(k+n), R+S$ distribution.

  The Jeffreys prior is $p(\Sigma) \alpha |\Sigma|^{-\frac{p+1}{2}}$,
  equivalently $k \rightarrow 0$.
\end{defn}


\section{Regression Models}
\label{sec:regression-models}

Assume for a set of covariates $x_{i1}, \dots, x_{ip}$, $\E{Y_{i}} =
x_{i}' \beta$, and $Y_{i} \sim N(\sum \beta_{i} x_{i}, \sigma^{2})$.
Assuming $Y_{i}$are conditionally independent given $\beta,
\sigma^{2}$, we can write $Y \sim N_{n}(X \beta, \sigma^{2} I_{n})$.
The least squares estimate and MLE is
\begin{align}
  \label{eq:17}
  \hat \beta = (X^{T} X)^{-1} X^{T} y \\
  \label{eq:18}
  \hat \beta \sim N_{p}(\beta, \sigma^{2}(X^{T} X)^{-1})
\end{align}

With known variances, assume $\beta \sim N_{p}(\gamma_{0}, \sigma^{2}
V)$.  Then $p(\beta | y) \alpha \exp(-\frac{1}{2 \sigma^{2}} ((\beta -
\gamma_{n})^{T} D^{-1} (\beta - \gamma_{n}))$ where $D^{-1} = X^{T} X
+ V^{-1}$, $\gamma_{n} = D(X^{T} y + V^{-1} \gamma_{0}) = D(X^{T} X
\hat \beta + V^{-1} \gamma_{0})$, so $\beta | y \sim N_{p}(\gamma_{n},
\sigma^{2} D)$.  As $V^{-1} \rightarrow 0$, we have $B|y \sim
N_{p}(\hat \beta, (X^{T} X)^{-1} \sigma^{2})$.

With $p(\beta) \alpha C$ and $p(\sigma^{2}) \alpha \sigma^{-2}$, then
conditional on $\sigma^{2}$, from the preceding general model $\beta |
y, \sigma^{2} \sim N_{n}(\hat \beta, (X^{T} X)^{-1} \sigma^{2})$ where
$\hat \beta = (X^{T} X)^{-1} X^{T} y$.

Since $\beta | y, \sigma^{2} \sim N_{p}(\hat \beta, (X^{T} X)^{-1}
\sigma^{2})$, a single regression coefficient $\beta_{i}$ has
posterior $\beta_{i} | y, \sigma^{2} \sim N(\hat \beta_{i}, s_{i}^{2}
\sigma^{2})$, where $s_{i}^{2} = (X^{T} X)^{-1}_{ii}$.


\section{Categorial Data, Prediction, and Ranking}
\label{sec:categ-data-pred}

Suppose $N$ individuals are classified according to two binary
variables, into a $2 \times 2$table.  We have three situations - one
margin fixed, both margins fixed, and the overall total fixed.

If One
margin is fixed, then $n_{i}$, and $n_{2}$ are fixed.  Then $y_{i1}
\sim Binomial(n_{i}, p_{i})$.

If no margins are fixed, we only fix the total $N = sum y_{ij}$. With
a full multinomail model $Y \sim Multinomial(q, N)$. Note if we just
take a signle row, we have standard Beta-Binomal updateing, as $Y_{11}
| n_{1} \sim Binomial(n_{1}, \frac{q_{11}}{q_{1.}} )$ from the
properties of the mutinomial, and $\frac{q_{11}}{q_{1.}}$ from the
properties of the Dirichlet.

\begin{defn}
  \label{sec:categ-data-pred-1}
  Recall if $Y_{k} \sim Poisson(\mu_{k})$, and $\sum_{k}^{} Y_{k} =
  N$, then $Y|N \sim Multinomial(q, N)$.  Letting $Y_{k} \sim
  Poission(\mu_{k})$ and using log-link function $\log u_{k} = \lambda
  + \alpha_{k}$, give a uniform prior to $\lambda$.  This is
  equivalent to assuming a multinomail distribution for $Y$ with
  parameters $q_{k} = \frac{e^{\alpha_{k}}}{sum_{k}} e^{\alpha_{k}}$,
  $N = \sum_{k}^{} Y_{k}$.
\end{defn}

For a $2 \times 2$ table, we can assume $Y_{ij} \sim
Poisson(\mu_{ij})$ and assume $\log \mu_{ij} = \phi + \alpha_{i} +
\beta_{j} + \gamma_{ij}$ with the corner constraints $\alpha_{1} =
\beta_{1} = \gamma_{12} + \gamma_{11} = \gamma_{21} = 0$.

Assuming we have multinomial observations $Y_{i} \sim
Multinomial(q_{i}, N_{i})$ with covariates $x_{i} = x_{i1}, \dots,
x_{iP}$.  Then we can express log odds of a category $k$ relative to a
baseline category as $\phi_{k1} = \log \frac{q_{ik}}{q_{i1}} =
\sum_{p=1}^{P} \beta_{kp} x_{ip}$, with category probabilities $q_{ik}
= \frac{\exp(sum_{p} \beta_{kp} x_{ip})}{\sum_{k}^{} \exp(\sum_{p}^{}
  \beta_{kp} x_{ip})}$.

\begin{defn}
  \label{sec:categ-data-pred-2}
  For ranking, assume $O_{i} \sim Poisson(\lambda_{i} E_{i})$, with
  $\lambda_{i}$ a standardized mortality rate, with Jeffreys prior
  $\alpha \frac{1}{\sqrt{\lambda_{i}}}$.
\end{defn}

\section{Sampling Properties in Relation to Other Methods}
\label{sec:sampl-prop-relat}

\begin{defn}
  Formally, an exchangeable sequence of random variables is a finite
  or infinite sequence $X_{1}, X_{2}, \dots$ of random variables such
  that for any finite permutation $\sigma$ of the indices $1, 2, 3,
  \dots$, (the permutation acts on only finitely many indices, with
  the rest fixed), the joint probability distribution of the permuted
  sequence

  $X_{\sigma(1)}, X_{\sigma(2)}, X_{\sigma(3)}, \dots$ is the same as
  the joint probability distribution of the original
  sequence.
\end{defn}

\begin{thm}
  \label{sec:sampl-prop-relat-2}
  If an infinite sequence of binary variables is exchangable, then it
  implies that any finite set $p(y_{1}, \dots, y_{n}) = \int
  \prod_{i=1}^{n} p(y_{i} | \theta) p(\theta) d \theta$ for some
  density $p(\theta)$ (with regularity conditions)
\end{thm}

\begin{defn}
  \label{sec:sampl-prop-relat-3}
  The likelihood principle: all information about $\theta$ provided by
  data $y$ is contained in the likelihood $\alpha p(y | \theta)$.
\end{defn}


\begin{thm}
  \label{sec:sampl-prop-relat-5}
  The statistic $t(Y)$ is sufficient for $\theta$ if and only if we
  can express the density $(y | \theta)$ in the form $p(y | \theta) = 
  h(y) g(t(y) | \theta)$.

  Trivially, the Bayesian posterior distribution only depends on the
  sufficient statistic.
\end{thm}

\section{Criticism and Comparison}
\label{sec:criticism-comparison}

\begin{defn}
  \label{sec:criticism-comparison-1}
  The Bayes factor comparison of models $M_{0}$ and $M_{1}$ are given
  as
  \begin{equation}
    \label{eq:19}
    \frac{p(M_{0} | y)}{p(M_{1} | y)} = \frac{p(M_{0})}{p(M_{1})}
    \frac{p(y | M_{0})}{p(y | M_{1})}
  \end{equation}
  or in words - posterior odds of $M_{0}$ equals the Bayes factor ($B_{01}$)
  times the prior odds of $M_{0}$.  This quantifies the weight of
  evidence in favour of the hypothesis $H_{0}: M_{0}$ is true.

  If both models are equally likely a priori, the Bayes factor is the
  posterior odds in favour of $M_{0}$.
\end{defn}

\begin{defn}
  \label{sec:criticism-comparison-2}
  The Bayesian Information Criterion (BIC) is
  \begin{equation}
    \label{eq:20}
    BIC = -2 \log p(y | \hat \theta) + k \log n
  \end{equation} where $\hat \theta$ is the MLE.
  $BIC_{0} - BIC_{1}$ is intendented to approximate $-2 \log B_{01}$
\end{defn}

\begin{defn}
  \label{sec:criticism-comparison-3}
  The deviance of a sampling distribution is defnied as $D(\theta) = 2
  \log p(y | \theta)$.
\end{defn}

\begin{defn}
  \label{sec:criticism-comparison-4}
  The AIC is given as $-2 \log p(y | \hat \theta) + 2k$ where $k$is
  the dimensionality of $\theta$.

  Asmyptitocally, AIC is equivalent to leave-on-out cross-validation.
\end{defn}

\begin{defn}
  \label{sec:criticism-comparison-5}
  Model dimensionality can be measured as $p_{D} = \E_{\theta | y}(-2
  \log p(y | \theta)) + 2 \log p(y | \tilde \theta(y))$.  If we take
  $\tilde \theta = \E{\theta | y}$, then $P_{D}$ is equal to the
  posterior mean deviance minus the deviance of the posterior means.

  We can approximate $P_{D} \approx \tr(-L_{\theta}'' C)$, where $C =
  \E{(\theta - \overline \theta)(\theta - \overline \theta)^{T}}$ is
  the posterior covariance matrix of $\theta$.

  Thus $p_{D}$ can be thought of the ratio of infomration in the
  likelihood about the parameters as a fraction of the total
  information in the posterior.  We an also think of $p_{D}$ as the
  franction of total information in the posteriro that is identified
  for the prior.

  For general normal regression models, we have this is exact, and
  $p_{D} = \tr((X^{T} X)(X^{T}X + V^{-1})^{-1})$.

  If there is \textbf{vague} prior infomration, $\hat \theta \approx
  \overline \theta$ (the MLE), and so $D(\theta) \approx D(\overline
  \theta) - (\theta - \overline \theta)^{T} L''(\hat \theta)(\theta -
  \overline \theta) = D(\overline \theta) + \chi_{p}^{2}$, and so
  $p_{D} = \E{\chi^{2}_{p}} = p$, the true number of parameters.
\end{defn}


\begin{defn}
  \label{sec:criticism-comparison-6}
  The \textbf{DIC} is defined as $DIC = D(\overline \theta)+ 2 p_{D} =
  \overline D + p_{D}$.
\end{defn}

\section{Heirachcial Models}
\label{sec:heirachcial-models}

\begin{defn}
  \label{sec:heirachcial-models-1}
  Suppose $y_{ij}$ is outcome for individual $j$, unit $i$, with
  unit-specific parameter $\theta_{i}$.  The assumption of partial
  exchangability of individuals within units can be represented by the
  following model - $y_{ij} \sim p(y_{ij} | \theta_{i}, x_{ij})$,
  $\theta_{i} \sim p(\theta_{i})$.

  Assumption of exchangability of units can be represented by the
  model $\theta_{i} \sim p(\theta_{i} | \phi)$, $\phi \sim p(\phi)$ -
  a common prior for all units (but a prior with unknown parameters.)

  Excchangability is a \textbf{judgement} based on our knowledge of
  the context.

  Assuming $\theta_{1}, \dots, \theta_{I}$ are drawn from some common
  prior distribution whose parameters are unknown is known as a
  \textbf{hierarchical} model.
\end{defn}


\begin{defn}
  \label{sec:heirachcial-models-2}
  The normal-normal model is givens $y_{ij} \sim N(\theta_{i},
  \sigma^{2})$, $j = 1, \dots, n_{i}$, $i = 1, \dots, I$, $\theta_{i}
  \sim N(\mu, \tau^{2})$, $i = 1, \dots, I$, $\mu \sim Uniform$.
  Assume $\sigma, \tau$ known for the moment and express $\tau^{2}$
  as $\tau^{2} = \frac{\sigma^{2}}{n_{0}}$.  From standard results,
  \begin{equation}
    \label{eq:21}
    p(\theta_{i} | y, \mu, \tau, \sigma) = Normal(\frac{n_{0} \mu +
      n_{i} \overline y_{i}}{n_{0} + n_{i}}, \frac{\sigma^{2}}{n_{0} +
      n_{i}})
  \end{equation}

  Now the marginal distribution of $\overline Y_{i.}$ is $\overline
  Y_{i.} \sim N(\mu, \sigma^{2}(n_{i}^{-1} + n_{0}^{-1}))$.  Writing
  $[\sigma^{2}(n_{i}^{-1} + n_{0}^{-1})]^{-1}$ as $\pi_{i}$, the
  precision, we have $\mu | y, \tau \sim N(\hat \mu, V_{\mu})$ where
  $\hat \mu = \frac{\sum_{i}^{} \pi_{i} \overline y_{i.}}{\sum_{i}^{}
    \pi_{i}} $, $V_{\mu} = \frac{1}{\sum_{i}^{} \pi_{i}}$.

  We can then show (reasonably easily) that $\E{\pi_{i}| y, \tau,
    \sigma} = \frac{n_{0} \hat \mu + n_{i} \overline y_{i}}{n_{0} +
    n_{i}}$ - an appropriate weighted average of the observed
  individual group mean and esatimated population mean.
\end{defn}

\begin{defn}
  \label{sec:heirachcial-models-3}
  For normal heircarchical mdoels the Jeffreys prior can be
  inconvenient.  Assume $y_{i} \sim N(\theta_{i}, \sigma_{i}^{2})$,
  $\sigma_{i}^{2}$ known, and $\theta_{i} \sim N(\mu, \tau^{2})$, $i =
  1, \dots, I$.  Then, integrating out the $\theta_{i}$, we get $y_{i}
  | \mu, \tau^{2} \sim N(\mu, \sigma_{i}^{2} + \tau^{2})$ which are
  conditionally independent given $\mu, \tau^{2}$.

  The posterior is $p(\tau^{2} | y) \alpha p(y | \mu, \tau^{2})
  p(\tau^{2})$ where $p(y | \mu, \tau^{2}) \alpha \prod_{i}
  (\sigma_{i}^{2} + \tau^{2})^{-\frac{1}{2}} \exp(-\frac{1}{2}
  \\frac{(y_{i} - \mu)^{2}}{\sigma_{i}^{2} + \tau^{2}})$.

  Lettting $\tau^{2} \rightarrow 0$, $p(y | \mu, \tau^{2})$ tends to a
  non-zero constant $c$, so $p(\tau^{2} < \epsilon | y) \alpha
  cP(\tau^{2} < \epsilon)$.

  Using an improper Jeffreys prior $p(\tau^{2} \alpha \tau^{-2})$,
  $p(\tau^{2} < \epsilon)$ is unbounded, and so $p(\tau^{2} < \epsilon
  | y)$ is unbounded, hence the posterior is improper.  

  Note that $\frac{1}{\tau^{2}} \sim Gamma(\epsilon, \epsilon)$ is
  proper, but inference can be sensitive to the choice of $\epsilon$.
\end{defn}

\begin{defn}
  \label{sec:heirachcial-models-4}
  Empirical Bayes methods proceed as before $y_{ij} \sim p(y_{ij} |
  \theta_{i})$, $\theta_{i} \sim p(\theta_{i} | \phi)$, but do not put
  a prior on $\phi$.  Estimate $\phi$ by, for example, maximum
  marginal likelihood - the value $\hat \phi$ that maximimizes the
  marginal likelihook
  \begin{align}
    \label{eq:22}
    p(y | \phi) = \prod_{i} \int \prod_{j} p(y_{ij} | \theta_{i})
    p(\theta_{i} | \phi) d \theta_{i},
  \end{align}  known as the \textbf{Type II Maximum Likelhood}.  Then
  use $\hat \phi$ as a ``plug-in'' estimate, as if the prior
  distribution was known.

  Can think of it as \textbf{estimating} prior from the data -
  understates uncertaintly since it ignores uncertainty in $\hat \phi$
  - for large number of units and overservations, have similar results
  to the ``full Bayes'' approach.
\end{defn}

\todo{Further content on heirarchical model comparison}

\section{Robustness and Outlier Detection}
\label{sec:robustn-outl-detect}

\begin{defn}
  \label{sec:robustn-outl-detect-1}
  If we assume, say $Y \sim t_{k}(\theta, \tau)$, then estimates will
  be less influenced by outliers.  If we want to simultaneously find
  outliers, we can fit a $t$-distribution as a mixture of nromals.
  Recall if $Y \sim Norm(\theta, \sigma^{2})$, and $\sigma^{2} =
  \frac{\tau^{2} k}{X^{2}} $, where $X^{2} \sim \chi^{2}_{k}$, then $Y
  \sim t_{k}(\theta, \tau)$.    So an equivalent model to $Y \sim
  t_{k}(\theta, \tau)$ is to assume $Y \sim normal(\theta,
  \sigma_{i}^{2})$, $\sigma_{i}^{2} = \frac{\tau^{2} k }{X_{i}^{2}} $,
  $X_{i}^{2} \sim \chi^{2}_{k}$, and monitor $s_{i} =
  \frac{k}{X_{i}^{2}} $ - values of $s_{i}$ much great than $1$
  indicate outlier.
\end{defn}


\bibliographystyle{plainnat}
\bibliography{../../common/bibliography}
\end{document}

