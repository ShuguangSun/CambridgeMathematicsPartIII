
\chapter{Extreme Value Theory}
\label{cha:extreme-value-theory}

Let $X_{n}$ be an IID sample from a distribution function $F$, and
denote $X_{(n)} = \max \{ X_{1}, \dots, X_{n} \}$ as the maximum order
statistic.

Without any normalization, $X_{(n)} \rightarrow x_{\star} = \inf \{ x:
F(x) = 1 \}$.

This is not overly interesting, since the limit distribution is
degenerate (we call $F$ non-degenerate if there does not exists $a \in
\R$ such that $F(x) = \I{x \geq a}$)

We may ask if there exists $\{ a_{n} \} > 0$, $\{ b_{n} \} > 0$, and a
nondegenerate $G$ such that
\begin{equation}
  \label{eq:118}
  \Prob{\frac{X_{(n)} - b_{n}}{a_{n}} \leq x} \rightarrow G(x)
\end{equation} for all continuity points $x$ of $G$

Classical extreme value theory starts by asking:
\begin{enumerate}
\item What kind of $G$ appears in the limit of \eqref{eq:118}?
\item Cna we characterize $F$ such that \eqref{eq:118} holds for a
  specific limit distribution $G$?
\end{enumerate}

For the first question, we have the Extremal Types theorem. For the
second question, we have the ``domain of attraction'' problem.


\section{Prelimiaries}
\label{sec:prelimiaries}

Recall that $\Prob{X_{(n)} \leq x} = F(x)^{n}$.  We say that $F$ is in
the domain of attraction of $G$ ($F \in D(G)$) if there exists $\{ a_{n} \} > 0$, $\{
b_{n} \} $ and a non-degenerate $G$ such that
\begin{equation}
  \label{eq:119}
  \Prob{\frac{X_{(n)} - b_{n}}{a_{n}} \leq x} = [\text{$F(a_{n} x +
    b_{n})^{n} \rightarrow G(x)$ for all continuity points $x$ of $G$}].
\end{equation} and write $F(a_{n} x + b_{n})^{n} \hookrightarrow G(x)$.

We say that $G_{1}$ and $G_{2}$ are of same type if $G_{1}(ax + b) =
G_{2}(x)$ for some $a > 0, b$.

The next lemma shows that if $F \in D(G_{1})$ and $F \in D(G_{2})$,
then $G_{1}$ and $G_{2}$ are of the same type.

\begin{lem}
  Suppose $X_{n}$ is an IID sample from $F$ and there exists $\{ a_{n}
  \} > 0, \{ b_{n} \}$ and non-degenerate $G$ such that $F(a_{n} x
  +b_{n})^{n} \hookrightarrow G(x)$. Then there exists $\{ \alpha_{n}
  \} > 0, , \{ \beta_{n} \}$ and non-degenerate $G_{\star}$ such that
  $F(\alpha_{n} x _{ \beta_{n}})^{n} \hookrightarrow G_{\star}(x)$. if
  and only if $\frac{\alpha_{n}}{a_{n}} \rightarrow a$ for some $a >
  0$, and $\frac{\beta_{n} - \beta}{a_{n}} \rightarrow b$ for some
  $b$.

Then we can let $G_{\star}(x) = G(ax + b)$.
\end{lem}

\begin{proof}
  See Galambos (1978), Lemma 2.2.3
\end{proof}

\begin{defn}
  \label{defn:extreme_value_theory:1}
  $G$ is \textbf{max-stable} if for every $n \in \N$, there exists
  $\{ a_{n} \} > 0, \{ b_{n} \} $ such that $G^{n}(a_{n}x + b_{n}) = G(x)$
\end{defn}

\begin{thm}
  \label{defn:extreme_value_theory:2}
  $D(g)$ is non-empty if and only if $G$ is max-stable.
\end{thm}

\begin{proof}
  (<=) If $G$ is max-stable, $G^{n}(a_{n} x + b_{n}) \hookrightarrow
  G(x)$.  Thus, by definitino, $G \in D(G)$.

  (=>) Let $F \in D(G)$. Then, there exists $\{ a_{n} \} > 0, \{ b_{n}
  \}$ such that $F^{n}(a_{n} x + b_{n}) \hookrightarrow G(x)$.  For
  each $k \in \N$, we replace $n$ by $nk$, and then
  \begin{align}
    \label{eq:121}
    F^{nk}(a_{nk} x + b_{nk}) \hookrightarrow G(x)
  \end{align}
  Thus $F^{n}(a_{nk} x + b_{nk}) \hookrightarrow G^{\frac{1}{k}}(x)$.
  Since $G^{\frac{1}{k}}$ is also non-degenerate, $G^{\frac{1}{k}}(x)
  = G(a_{k} x + b_{k})$, which implies $G(x) = G^{k}(a_{k} x + b_{k})$
  as they are of the same type.
\end{proof}

\begin{thm}
  \label{defn:extreme_value_theory:3}
  If $F \in D(G)$, then $G$ must belong to the following distributions
  (within type):

  \begin{enumerate}
  \item Frechet - $G_{1, \alpha}(x) = \exp(-x^{-\alpha})$, $x > 0$,
    $\alpha > 0$
  \item Negative Weibull - $G_{2, \alpha} = \exp(- (-x)^{\alpha})$, $x
    < 0$, $\alpha > 0$
  \item Gumbel - $G_{3}(x) = \exp(-\exp(-x))$, $x \in \R$.
  \end{enumerate}

  Conversely, these distributions can appear as such limits in \eqref{eq:118}.
\end{thm}

\begin{remark}
  We have
  \begin{enumerate}
  \item Using $X_{(1)} = -\max \{ -X_{1}, \dots, -x_{n} \} $, we have
    equivalent theorems in terms of normalized minima.
  \item Sometimes, we cannot have nondegenerate $G$ of normalized
    maxima  - for example $X_{1}, \dots, X_{n} \sim
    Bern(\frac{1}{2})$, $X_{(n)}$.
  \item We can combine these three types into Generalized Extreme
    Value Distribution (GEV) ---
    \begin{equation}
      \label{eq:122}
      G(x; \mu, \sigma, \gamma) = \exp(- (1 + \gamma(\frac{x-\mu}{\sigma}))^{-\frac{1}{\gamma}})
    \end{equation} with $1 + \gamma(\frac{x - \mu}{\sigma}) > 0$, $\mu
    \in \R, \gamma \in \R, \sigma > 0$.
  \end{enumerate}

  We have Frechet corresponds to $\gamma > 0$, $\alpha =
  \frac{1}{\gamma} $, NW is $\gamma < 0$, $\alpha =
  -\frac{1}{\gamma}$, and Gumbel corresponds to the case where $\gamma
  \rightarrow 0$.
\end{remark}

\begin{proof}[non-examinable]
  We show $Y_{n} = \frac{X_{(n)} - b_{n}}{a_{n}} \cd Y$, with
  $G_{\gamma}(x) = \exp(- (1 + rx)^{-\frac{1}{r}})$

  Then, using Helly's theorem, we have $\E{z(Y_{n})} \rightarrow
  \E{z(Y)}$ for all continuous bounded $z$.  Then the LHS is given by
  \begin{align}
    \label{eq:123}
    \int_{}^{} z \frac{x - b_{n}}{a_{n}} d F_{X_{(n)}}(x) = n
    \int_{}^{} z(\frac{x - b_{n}}{a_{n}}) F(x)^{n-1} dF(x)
  \end{align} and changinge variables so $F(x) = 1 - \frac{v}{n}, x = \dots$
\end{proof}

\section{Necessary and Sufficient Conditions for Convergence}
\label{sec:necess-suff-cond}

We say a function $l: [C, \infty] \rightarrow (0, \infty)$ is ``slowly
varying'' if $\lim_{x \rightarrow \infty} \frac{l(tx)}{l(x)} = 1$ for
all $t > 0$.  For example, $l (x) = \log x, \log \log x, (\log
x)^{\alpha}$.

We say a function $r_{\alpha}: [C, \infty) \rightarrow (0, \infty)$ is
``regularly varying'' with an index $a \in \R$ if $r_{\alpha}(x) =
x^{-\alpha}l(x)$ where $l$ is slowly varying - so $r_{2}(x) = x^{-2}
\log x$.

We define an \textbf{expected residual lifetime} as
\begin{align}
  \label{eq:120}
  R(x) = \E{X - x | X > x} = \frac{1}{1 - F(x)} \int_{x}^{x_{\star}}
  (1 - F(y)) dy
\end{align} where $x_{\star} = \inf \{ x : F(x) = 1 \}$, and
$\overline F(x) = 1 - F(x)$

\begin{thm}
  \label{defn:extreme_value_theory:4}
  $F \in D(G_{1, \alpha})$ if and only if $x_{\star} = \infty$,
  $\overline F(x) = x^{-\alpha} l(x)$ where $l$ is slowly varying.  We
  can choose $b_{n} = 0$, $a_{n} = F^{-1}(1 - \frac{1}{n})$ for which
  $F^{n}(a_{n} x + b_{n}) \hookrightarrow G_{1, \alpha}(x)$ is
  satisfied.

  $F \in D(G_{2, \alpha})$ if and only if $x_{\star} < \infty$,
  $\overline F(x_{\star} - \frac{1}{x} ) = x^{-\alpha} l(x)$, with $l$
  slowly varying for $x > 0$.  We can choose $b_{n} = x_{\star}$,
  $a_{n} = x_{\star} - F^{-1}(1 - \frac{1}{n})$ for convergence.

  $F \in D(G_{3})$ if and only if
  \begin{align}
    \label{eq:124}
    \frac{\overline F(x + t R(x))}{\overline F(x)}  \rightarrow e^{-t}
  \end{align} We can choose $b_{n} = F^{-1}(1 - \frac{1}{n})$, $a_{n}
  = R(b_{n})$.
\end{thm}

\begin{exmp}
  \label{sec:f-in-dg_1}
  \begin{enumerate}
  \item Let $F(x) = 1 - \frac{\log_{2}(x+1)}{x^{2}}$ where $ x\ geq
    1$. Then $F \in G_{1, 2}$.
  \item Let $F(x) = 1 - (x_{\star} - x)^{3}$ where $x_{\star} - 1 \leq
    x \leq x_{\star}$ for some $x_{\star} \in \R$.  Then $F \in G_{2, 3}$.
  \item Let $F(x) = 1 - \frac{1}{1 + e^{x}}$.  Then $F \in G_{3}$.
  \end{enumerate}
\end{exmp}

\begin{lem}
  Suppose there exists $a_{n} > 0$, $b_{n}$ such that $n(1 - F(a_{n}
  x + b_{n})) \rightarrow{ u(x)}$.  Then
  \begin{align}
    \label{eq:125}
    F^{n}(a_{n} x + b_{n}) \hookrightarrow \exp(-u(x))
  \end{align}
\end{lem}

\begin{proof}
  Taking the log of the left hand side, we have
  \begin{align}
    \label{eq:126}
    n \log F(a_{n} x + b_{n}) &= n \log (1 - (1 - F(a_{n} x +
    b_{n}))) \\
    &= n (-(1 - F(a_{n} x + b_{n})) - \frac{1}{2} (1 - F(a_{n}x +
    b_{n}))^{2} + \dots) \\
    &= -u(x)
  \end{align}
  Thus the left hand side converges to $\exp(-u(x))$.
\end{proof}


\begin{proof}[Proof of sufficient part of first part of theorem]
  Proof of (1) - the sufficient part.  Supopse $x_{\star} = \infty$,
  $\overline F(x) = x^{-\alpha} l(x)$.  Use $a_{n}$ and $b_{n}$ as in
  the theorem.  Then we want to prove $F^{n}(a_{n} x + b_{n})
  \hookrightarrow G_{1, \alpha}(x) = \exp(-x^{-\alpha} \I{x > 0})$.

  Using the lemma, we instead prove
  \begin{align}
    \label{eq:127}
    n(1 - F(a_{n} x)) \rightarrow x^{-\alpha} \I{x > 0} + \infty \I{x
      < 0}.
  \end{align}

  Let $x < 0$.  Note that $a_{n} = F^{-1}(1 - \frac{1}{n}) \rightarrow
  x_{\star} = \infty$.  Thus $a_{n} x \rightarrow - \infty$, and $n(1
  - F(a_{n} x)) \rightarrow \infty$.

  Let $x > 0$. Note that $F(a_{n}) = F(F^{-1}(1 - \frac{1}{n})) \geq 1
  - \frac{1}{n}$, and $F(a_{n} - \delta) \leq 1 - \frac{1}{n}$.
  Rearranging, this gives $n \geq \frac{1}{1 - F(a_{n} - \delta)}$

  Note also we have
  \begin{align}
    \label{eq:128}
    n \frac{(1-F(a_{n} x))}{(1 - F(a_{n} x))} (1 - F(a_{n}))
  \end{align}
  which converges to $x^{-\alpha}$, as $\overline F = x^{-\alpha} l(x)$.

  Thus, it suffices to show that $n(1 - F(a_{n})) \rightarrow 1$.
  Note that
  \begin{align}
    \label{eq:129}
    1 &\geq n(1 - F(a_{n})) \\
    &\geq \frac{1-F(a_{n})}{1 - F(a_{n} - \delta)} \\
    &\geq \frac{1 - F(a_{n})}{1 - F(a_{n}(1 - \epsilon))}  \\
    &= \frac{a_{n}^{-\alpha} l(a_{n})}{a_{n}^{-\alpha}
      (1-\epsilon)^{-\alpha}l(a_{n}(1 - \epsilon))} \\
    &= (1 - \epsilon)^{\alpha}
  \end{align}
  and as $\epsilon$ can be made arbitrarily close to zero, we obtain
  our result.
\end{proof}

\begin{proof}[Proof of sufficient part of third part of theorem]
  Suppose
  \begin{align}
    \label{eq:130}
    \frac{\overline F(x + t R(x))}{\overline F(x)} \rightarrow e^{-t}
  \end{align} and we use $a_{n}$, $b_{n}$ as in the theorem.  As in
  the lemma, we seek to prove
  \begin{align}
    \label{eq:131}
    n(1 - F(a_{n} x + b_{n})) = n(1 - F(R(b_{n}) x_{n} + b_{n})) \rightarrow e^{-x}.
  \end{align}

  To use the condition, note that the left hand side is given as
  \begin{align}
    \label{eq:132}
    \frac{n(1 - F(b_{n} + x R(b_{n})))}{1 - F(b_{n})} (1 - F(b_{n}))
  \end{align} and the inner term converges to $e^{-x}$ by assumption.

  Thus, it suffices to prove $n(1 - F(b_{n})) \rightarrow 1$.
  \begin{align}
    \label{eq:133}
    1 &\geq n(1 - F(b_{n})) \\
    &\geq \frac{1 - F(b_{n})}{1 - F(b_{n} - \delta)} \\
  &\geq \frac{1 - F(b_{n})}{1 - F(b_{n} - \epsilon R(b_{n}))} \\
  &\rightarrow \frac{1}{e^{-(-\epsilon)}}  = e^{-\epsilon} \rightarrow 1
  \end{align} Choose $\epsilon$ such that $1 - F(b_{n} - \delta) \leq 1
  - F(b_{n} - \epsilon R(b_{n}))$.
\end{proof}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "master"
%%% End:
