\chapter{Introduction}
\label{cha:introduction}

\begin{itemize}
\item Sixteen lectures
\item Three example sheets, three example classes.
\item A closed book exam in term three.
\end{itemize}

Old lectures
\begin{itemize}
\item Tue 21 Jan => Mon 20 Jan 4PM
\item Thu 23 Jan => Fri 24 Jan 2PM
\item  Tu 11 Feb => Fri 7 Feb 12pm
\end{itemize}

References

\begin{itemize}
\item Serftag, (1980) Approximation Theorems of Mathematical Statistics
\item Van der Vaart, (1995) Asymptotic Statistics
\item Wood and Jones, (1996) Kernel Smoothing
\item Fan and Gijlets (1996) Local Polynomial Modelling and it's Applications
\item Duroye, Gryorf and Lagos, (1996) A probabilistic theory of pattern recognition

\end{itemize}

\section{Basic Concepts}
\label{sec:basic-concepts}

\subsection{Parametric vs Nonparametric models}
\label{sec:param-vs-nonp}

A statistical model postulates a family of possible data generating
mechanisms.  Examples include:
\begin{enumerate}
\item Let $X_{1}, \dots, X_{n} \sim T(m, \theta)$ \iid, with $m$ known
  and $\theta \in (0, \infty) = \Theta$ an unknown parameter.
\item Let $Y_{i} = \alpha + \beta x_{i}+ \epsilon_{i}, i = 1, \dots,
  n$ where $x_{i}$ are known and $\epsilon_{i}$ are \iid with
  $\E{e_{i}} = 0, \Var{\epsilon_{i}} = \sigma^{2}$.  Here, the unknown
  parameter is $\theta =
  \begin{pmatrix}
    \alpha \\
    \beta \\
    \sigma^{2}
  \end{pmatrix} \in \R \times \R \times (0, \infty) = \Theta$.
\end{enumerate}

If the parameter space $\Theta$ is finite dimensional, we speak of a
\textbf{parametric model}.  In such situations, typically we can
estimate $\theta$ using the MLE $\hat \theta_{n}$, and have $\hat
\theta_{n} - \theta = O_{p}(n^{-\frac{1}{2}})$.\footnote{Definition of
  $O_{p}$ - TODO}

This assumes the model contains the true data generating process, if
not, inference can be misleading.

Examples of nonparametric models include:
\begin{enumerate}
\item \label{item:1} Let $X_{1}, \dots, X_{n}, i = 1, \dots, n$ be
  \iid with arbitrary distribution function $F$.
\item \label{item:2} Let $X_{1}, \dots, X_{n}, i = 1, \dots, n$ be
  \iid with twice continuously differentiable density $f$.
\item \label{item:3} Let $Y_{i} = m(x_{i}) + v(x_{i})^{\frac{1}{2}}, i
  = 1, \dots, n$ where $m$ is twice continuously differentiable and
s  $\epsilon_{1}, \dots, \epsilon_{n}$ are \iid with $\E{\epsilon_{i}}
  = 0, \Var{\epsilon_{i}} = 1$.
\end{enumerate}

Such infinite-dimensional models are much less vulnerable to model
misspecification, typically, however we pay a price for our generality
in terms of a slower convergence rate - e.g. $O_{p}(n^{-\frac{2}{3}})$
in problems \ref{item:2} and \ref{item:3} above.

\subsection{Estimating an arbitrary distribution function}
\label{sec:estim-an-arbitr}

Let $X_{1}, \dots, X_{n}$ be \iid on a probability space $(\Omega,
\mathcal{F}, \Prob)$ with distribution function $F$.  The
\textbf{empirical distribution function} $\hat F_{n}$ is defined by
\begin{equation}
  \label{eq:1}
  \hat F_{n}(x) = \frac{1}{n} \sum_{i=1}^{n} \I{X_{i} \leq x}.
\end{equation}


\begin{thm}[Glivenko-Cantelli (1933) - The Fundamental Theorem of Statistics]
  \label{defn:Introduction:2}
  \begin{equation}
    \label{eq:2}
    \sup_{x \in \R} \left| \hat F_{n}(x) - F(x) \right| \cas 0.
  \end{equation}
\end{thm}

\begin{proof}
  Given $\epsilon > 0$, choose a partition $-\infty = x_{0} < x_{1} <
  \dots < s_{k} = \infty$ such that, for each $i = 1, \dots, k$, we
  have $F(x_{i}-) - F(x_{i-1}) \leq \epsilon$, where $F(x-) = \lim_{y
    \uparrow x} F(y)$.

  Note that any point at which $F$ jumps by more than $\epsilon$ must
  be in the partition.  By the strong law of large numbers, there
  exists an event $\Omega_{\epsilon}$ with $\Prob{\Omega_{\epsilon}} =
  1$ such that for all $\omega \in \Omega_{\epsilon}$, there exists
  $n_{0} = n_{0}(\omega, \epsilon)$ with
  \begin{align}
    \label{eq:3}
    \left| \hat F_{n}(x_{i}) - F(x_{i}) \right| \leq \epsilon, i = 1,
    \dots, k - 1, n \geq n_{0}, \\
    \left| \hat F_{n}(x_{i}-) - F(x_{i}-) \right| \leq \epsilon, 1 =
    i, \dots, k-1, n \geq n_{0}.
  \end{align}

  Now, fix $x \in \R$, and find $i \in \{ 1, \dots, k \}$ with $x \in
  [x_{i-1}, \dots, x_{i})$.  Then for $\omega \in \Omega_{\epsilon}$
  and $n \geq n_{0}$,
  \begin{align}
    \label{eq:4}
    \hat F_{n}(x) - F(x) \leq \hat F_{n}(x_{i}-) - F(x_{i-1}) = \hat
    F_{n}(x_{i}-) - F(x_{i}-) + F(x_{i}-) - F(x_{i-1}) \leq \epsilon +
    \epsilon = 2\epsilon
  \end{align}

  Similarly, we have
  \begin{align}
    \label{eq:5}
    F(x) - \hat F_{n}(x) \leq F(x_{i}-) - \hat F_{n}(x_{i-1}) =
    F(x_{i}-) - F(x_{i-1}) + F(x_{i-1}) - \hat F_{n}(x_{i-1}) \leq
    \epsilon + \epsilon = 2 \epsilon
  \end{align}

  We deduce that
  \begin{align}
    \label{eq:6}
    \Prob{\sup_{x \in \R} \left| \hat F_{n}(x) - F(x) \right|
    \rightarrow 0} &= \Prob{\cap_{m=1}^{\infty} \cup_{n_{0} =
      1}^{\infty} \cap_{n=n_{0}}^{\infty} \{ \sup_{x \in \R} \left|
      \hat F_{n}(x) - F(x) \right| \leq \frac{1}{m} \}} \\
  &= \lim_{m \rightarrow \infty} \Prob{\Omega_{\frac{1}{2m}}} = 1
\end{align}

\end{proof}


\begin{thm}
  \label{defn:Introduction:1}
  Let $X_{1}, \dots, X_{n} \sim F$ \iid.  Then for every $\epsilon >
  0$,
  \begin{align}
    \label{eq:7}
    \Prob{\sup_{x \in \R}|\hat F_{n}(x) - F(x)| \geq \epsilon} \leq 2 e^{-2n\epsilon^{2}}.
  \end{align}
\end{thm}

An application is to consider the problem of finding a confidence band
for $F$ at $1-\alpha$. Given $\alpha \in (0, 1)$, set $\epsilon_{n}
=(-\frac{1}{2n} \log \frac{\alpha}{2})^{\frac{1}{2}}$.  Then by
\ref{defn:Introduction:1},
\begin{align}
  \label{eq:8}
  (\max(\hat F_{n}(x) - \epsilon_{n}, 0), \min(\hat F_{n}(x), 1))
\end{align} is a $1-\alpha$ confidence interval for $F$.

In fact, let $U_{1}, \dots, U_{n} \sim U(0, 1)$ \iid, and let $\hat
G_{n}$ denote their empirical distribution function.  Then
\begin{align}
  \label{eq:9}
  \hat G_{n}(F(x)) = \frac{1}{n} \sum_{i=1}^{n} \I{U_{i} \leq F(x)} =
  \frac{1}{n} \sum_{i=1}^{n} \I{F^{-1}(u_{i}) \leq x} = \frac{1}{n}
  \sum_{i=1}^{n} \I{X_{i} \leq x} = \hat F_{n}(x)
\end{align}

It follows that
\begin{align}
  \label{eq:10}
  \sup_{x \in \R} |\hat F_{n}(x)- F(x)| = \sup_{x \in R} |\hat
  G_{n}(F(x)) - F(x)| \leq \sup_{t \in (0, 1)} |\hat G_{n}(t) - t |
\end{align} with equality if $F$ is continuous.  We deduce that, if
$F$ is continuous, the distribution of $\sup_{x \in \R} | \hat
F_{n}(x) - F(x)|$ does not depend on $F$.

Other examples include Uniform Laws of Large Numbers (ULLN).  Let
$X, X_{1}, X_{2}, \dots$ be \iid taking values in a measurable space
$(\mathcal{X}, \mathcal{A})$, and let $\mathcal{G}$ denote a class of
measurable functions on $\mathcal{X}$.  We say that $\mathcal{G}$
satisfies a ULLN if
\begin{equation}
  \label{eq:11}
  \sup_{g \in \mathcal{G}} | \frac{1}{n} \sum_{i=1}^{n} g(X_{i}) -
  \E{g(X)}| \cas 0.
\end{equation}
Thus Theorem 1 shows that the class $\mathcal{G} = \{ \I{\cdot \leq
  x}: x \in \R \}$ satisfies a ULLN.  In general, proving a ULLN
amounts to controlling the \textbf{size} of $\mathcal{G}$, which can
be done by using the idea of entropy (c.f. Statistical Theory).

Further results start with the observation that
\begin{align}
  \label{eq:12}
  n^{\frac{1}{2}} (\hat F_{n} - F(x)) \cd N(0, F(x)(1-F(x)))
\end{align} by the central limit theory.  This result can be
strengthened by studying $\{ n^{\frac{1}{2}}(\hat F_{n}(x) - F(x)), x
\in \R \}$ as a stochastic process.

\begin{proposition}
  Let $U_{1}, \dots, U_{n} \sim U(0, 1)$ \iid.  Let $Y_{1}, \dots,
  Y_{n+1} \sim \textsc{Exp}(1)$ \iid and let $S_{j} = \sum_{i=1}^{j}
  Y_{i}$ for $j = 1, \dots, n+1$.  Then
  \begin{align}
    \label{eq:13}
    U_{j} =^{d} \frac{S_{j}}{S_{n+1}} \sim \textsc{Beta}(j, n-j+1).
  \end{align}
\end{proposition}

\todo{Fill in lecture from Friday on KDE (variance and bias)}

\section{Density Estimators}
\label{sec:density-estimators}

Usually, we prefer to choose $h$ to minimize some expression measuring
how well $\hat f_{h}$ estimates $f$ as a function.  We therefore
define the Mean Integrated Squared Error ($MSIE$) as
\begin{align}
  \label{eq:14}
  MSIE(\hat f_{h}) &= \E{\int_{-\infty}^{\infty} \{ \hat f_{h}(x) -
    f(x) \}^{2} dx} \\
  &= \int_{-\infty}^{\infty} MSE(\hat f_{h}(x)) dx \\
  &= \int_{\infty}^{\infty} ((K_{h} \star f)(x) - f(x))^{2} +
  \frac[1]{h} ((K^{2}_{n} \star f)(x) - (K_{h} \star f)^{2}(x)) dx
\end{align} which is justified by Fubini's theorem as the integrand
is non-negative.
Although exact, this expression depends on $h$ in a complicated way.
We therefore seek asymptotic approximation to clarify this dependence
and facilitate an asymptotically optimal choice of $h$.

\section{Asymptotic MSE and MSIE approximation}
\label{sec:asynptotic-mse-msie}

We need the following conditions:
\begin{enumerate}
\item \label{item:4} $f$ is twice differentiable, $f'$is bounded, and $R(f) =
  \int_{-\infty}^{\infty} f''(x)^{2} dx < \infty$.
\item \label{item:5} $h = h_{n}$ is a non-random sequence with $h \rightarrow 0$ and
  $nh \rightarrow \infty$ as $n \rightarrow \infty$.
\item \label{item:6} $K$ is non-negative, $\int_{-\infty}^{\infty} K(x) dx = 1$,
  $\int_{-\infty}^{\infty} x K(x) dx = 0$, $\mu_{2}(K) =
  \int_{-\infty}^{\infty} x^{2} K(x) dx < \infty$, and $R(x) < \infty$.
\end{enumerate}

\begin{thm}
  \label{defn:Introduction:3}
  Assume that the previous conditions hold. Then, for all $x \in \R$,
  \begin{align}
    \label{eq:15}
    MSE(\hat f_{n}(x)) = \frac{R(K) f(x)}{nh} + \frac{1}{4} h^{4}
    \mu_{2}^{2}(K) f''(x)^{2} + o(\frac{1}{nh} + h^{4})
  \end{align} as $n \rightarrow \infty$.
\end{thm}

\begin{proof}
  We first claim that $f$ is bounded. Otherwise, there would exists
  $(x_{n})$ such that $f(x_{n}) \geq n$.  Since $f$ is a density, the
  exists $x_{n, l} \in [x_{n} - \frac{2}{n}, x_{n}]$ such that
  $f(x_{n, l}) \leq \frac{n}{2}$  and there exists $x_{n, m} \in
  [x_{n}, x_{n} + \frac{2}{n}]$ such that $f(x_{n, m}) \leq
  \frac{n}{2}$.  y the mean value theorem, there exists $x^{\star}_{n,
  l} \in [x_{n, l}, x_{n}]$ such that $f'(x^{\star}_{n, l}) \geq
\frac{n^{2}}{4}$ and there exists $x^{\star}_{n, m} \in [x_{n}, x_{n,
  m}]$ such that $f'(x^{\star}_{n, m}) \leq -\frac{n^{2}}{4}$.  By the
mean value theorem again, we have that there exists $x_{n}^{\star
  \star} \in [x_{n, l}^{\star}, x_{n, m}^{\star}]$ such that
$f''(x_{n}^{\star \star}) \leq -\frac{n^{3}}{8}$, contradicting
boundedness of $f''$.

We can therefore define $C_{0} = \sup_{x \in \R} f(x)$ and $C_{2} =
\sup_{x \in \R} |f''(x)|$.

Now,
\begin{align}
  \label{eq:16}
  \E{\hat f_{h}(x)} &= \int_{-\infty}^{\infty} \frac{1}{h}
  K(\frac{x-y}{h}) f(y) dy \\
  &= \int_{-\infty}^{\infty}  K(z) f(x - hz) dz \\
  &= \int_{-\infty}^{\infty} K(z)(f(x) - hz f'(x) + \frac{1}{2} h^{2}
  z^{2} f''(x)) dz + REM_{1} \\
  &= f(x) + \frac{1}{2} h^{2} \mu_{2}(K) f''(x) + REM_{1}.
\end{align}

To control the remainder, given $\epsilon > 0$, choose $\delta > 0$
such that
\begin{align}
  \label{eq:17}
  |f(x - hz) - (f(x) - hz f'(x) + \frac{1}{2} h^{2} z^{2} f''(x))|
  \leq \epsilon h^{2} z^{2}
\end{align} for all $|hz| \leq \delta$.

Then
\begin{align}
  \label{eq:18}
  |REM_{1}| &= | \int_{-\infty}^{\infty} K(z) f(x - hz) dz -
  \int_{-\infty}^{\infty} K(x)(f(x) + \frac{1}{2} h^{2} z^{2} f''(x))
  dz | \\
  &\leq | \int_{|z| > \frac{\delta}{h}} K(z) f(x - hz) dz| +
  \int_{|z| \leq \frac{\delta}{h}} K(z)|f(x-hz) - (f(z) +
  \frac{1}{2}h^{2} z^{2} f''x)| dz + | \int_{|z| > \frac{\delta}{h}}
  K(z) (f(x) + \frac{1}{2} h^{2} z^{2} f''(x)) dz| \\
  &\leq C_{0} \frac{h^{2}}{\delta^{2}} \int_{|z| > \frac{\delta}{h}}
  z^{2} K(x) dz + \epsilon h^{2} \int_{|z| \leq \frac{\delta}{h}}
  z^{2} K(z) dz + C_{0} \frac{h^{2}}{\delta^{2}} \int_{|z| >
    \frac{\delta}{h}} z^{2} K(z) dz + \frac{1}{2} h^{2} C_{2}
  \int_{|z| > \frac{\delta}{h}} z^{2} K(z) dz \\
  &\leq \epsilon h^{2}{1 + \mu_{2}(K)}
\end{align}
since $\int_{-\infty}^{\infty} z K(z) dx = 0$, Markov's inequality, etc.
Thus,
\begin{equation}
  \label{eq:19}
  BIAS(\hat f_{h}(x)) = \frac{1}{2} h^{2} \mu_{2}(K) f''(x) + o(h^{4}).
\end{equation}

For the variance,
\begin{align}
  \label{eq:20}
  \Var{\hat f_{h}(x)} &= \frac{1}{nh^{2}} \int_{-\infty}^{\infty}
  K^{2}(\frac{x-y}{h}) f(y) dy - \frac{1}{n} \{\E{\hat f_{h}(x)}
  \}^{2} \\
  &= \frac{1}{nh} \int_{-\infty}^{\infty} K^{2}(z) f(x - hz) dz -
  \frac{1}{n} (f(x) + o(1))^{2} \\
  &= \frac{1}{nh} \int_{-\infty}^{\infty} K^{2}(z) f(x) dz + REM_{2} +
  O(\frac{1}{n}) \\
  &= \frac{R(K) f(x)}{nh} + REM_{2} + O(\frac{1}{n})
\end{align}

To control $REM_{2}$, given $\epsilon > 0$, choose $y > 0$ such that
$|f(x - hz) - f(x)| \leq \epsilon$ for $|hz| \leq y$.  Then
\begin{align}
  \label{eq:21}
  nh | REM_{2} | = | \int_{-\infty}^{\infty} K^{2}(z) (f(x - hz) -
  f(x)) dz | \\
  &\leq \epsilon \int_{|z| \leq \frac{y}{h}} K^{2}(z) + 2 C_{0}
  \int_{|z| > \frac{y}{h}} K^{2}(z) dz \\
  &\leq \epsilon(R(K) + 1)
\end{align}
for large $n$.

We deduce that $\Var{\hat f_{h}(x)} = \frac{R(K) f(x)}{nh} +
o(\frac{1}{nh})$ and
\begin{align}
  \label{eq:22}
  MSE( \hat f_{h}(x)) = \frac{R(K) f(x)}{nh} + \frac{1}{4} h^{4}
  \mu_{2}^{2}(K) f''(x)^{2} + o(\frac{1}{nh} + h^{2})
\end{align}

The hope is that to compute the MSIE, we can just integrate the MSE
over range of the RV.  We need to be careful - in general we cannot
integrate asymptotic pointwise estimates - need to understand
dependency on $x$.

With mild additional conditions and further work (see the example
sheet), it can be shown that
\begin{align}
  \label{eq:23}
  MISE(\hat f_{h})= \frac{R(K)}{nh} + \frac{1}{4} h^{4}
  \mu_{2}^{2}(K)R(f'') + o(\frac{1}{nh} + h^{4})
\end{align}

We see that asymptotically the integrated variance term decreases with
$h$ while the integrated squared bias term increases with $h$.  This
is the \textbf{bias-variance tradeoff}.

This \textbf{bias-variance tradeoff} summarizes the critical role of
the bandwidth.

\end{proof}

Consider now minimizing the asymptotic MISE (AMISE) $\frac{R(K)}{nh} +
\frac{1}{4} h^{4} \mu_{2}^{2}(K) R(f'')$ with respect to $h$, yielding
the asymptotically optimal bandwidth
\begin{align}
  \label{eq:24}
  h_{AMISE} = (\frac{R(K)}{\mu_{2}^{2}(K) R(f'') n})^{\frac{1}{5}}
\end{align}

Substituting back, we obtain
\begin{align}
  \label{eq:25}
  AMISE(\hat f_{AMISE}) = \frac{5}{4} R(K)^{\frac{4}{5}}
  \mu_{2}(K)^{\frac{2}{5}} R(f'')^{\frac{1}{5}} n^{-\frac{4}{5}}.
\end{align}

Notice the slower rate than the typical $O(n^{-1})$
parametric rate.  Notice that for the ``rough'' densities, with larger
$R(f'')$, we should use a smaller bandwidth, and these densities are
harder to estimate.


\section{Pointwise asymptotic distribution}
\label{sec:pointw-asympt-distr}

\begin{thm}
  \label{defn:Introduction:4}
  Assume the previous assumptions \ref{item:4}, \ref{item:5},
  \ref{item:6} and that $K$ is bounded.  Then, for all $x \in \R$,
  \begin{equation}
    \label{eq:26}
    n^{\frac{2}{5}}(\hat f_{h_{AMISE}}(x) - f(x)) \cd
    N(\frac{1}{2}\mu_{2}(K) f''(x), R(K) f(x))
  \end{equation}
\end{thm}

\begin{proof}
  First, observe that from the proof of the previous theorem,
  \begin{align}
    \label{eq:27}
    n^{\frac{2}{5}} (\E{\hat f_{h_{AMISE}}(x) - f(x)}) \rightarrow
    \frac{1}{2} \mu_{2} K f''(x)
  \end{align}

  For the stochastic term, let $Y_{ni} = \frac{1}{h^{\frac{1}{2}}}
  K(\frac{x - X_{i}}{h})$.  We have
  \begin{align}
    \label{eq:28}
    \Var{Y_{ni}} = \frac{1}{h} \int_{-\infty}^{\infty}
    K^{2}(\frac{x-y}{n}) f(y) - h(\E{\hat f_{h}(x)})^{2} \\
    &= \int_{-\infty}^{\infty} K^{2}(z) f(x - hz) dz - h(f(x) +
    o(1))^{2} \\
    &\rightarrow R(K) f(x)
  \end{align} as $n \rightarrow \infty$.

  Moreover,
  \begin{align}
    \label{eq:29}
    \E{Y^{2}_{ni} \I{|Y_{ni} \geq \epsilon n^{\frac{1}{2}}}} =
    \int_{-\infty}^{\infty} \frac{1}{n} K^{2}(\frac{x - y}{h}) f(y)
    \I{K(\frac{x - y}{h}) \geq \epsilon (nh)^{\frac{1}{2}}} dy \\
    &= 0
  \end{align} for $n$ large enough such that $\sup_{z \in R} K(z) < e
  (nh)^\frac{1}{2}$.

  Thus by the Linderberg-Feller central limit theorem, we have our
  required result.
\end{proof}

\section{Bandwidth Selection}
\label{sec:bandwidth-selection}

Since $h_{AMISE}$ depends on $f$ through $R(f'')$, we still require
practical bandwidth selection algorithms.

\subsection{Normal Scale rules}
\label{sec:normal-scale-rules-1}

If $f$ is the $N(0, \sigma^{2})$ density, then $R(f'') = \frac{3}{8
  \sqrt{\pi}} \sigma^{-5}$.    The normal scale rate $\hat h_{NS}$
consists of replacing $R(f'')$ in $h_{AMISE}$ with $\frac{3}{8
  \sqrt{\pi}} \hat \sigma^{-5}$, where $\hat \sigma$ is an estimate of
$\sigma$.  This tends to over-smooth.

\subsection{Least-squares Cross-Validation}
\label{sec:least-squares-cross}

Recall that
\begin{equation}
  \label{eq:30}
  MISE(\hat f_{h}) = \E{\int_{-\infty}^{\infty} \hat f(x)^{2} dx} - 2
  \E{\int_{-\infty}^{\infty} \hat f_{h}(x) f(x)} +
  \int_{-\infty}^{\infty} f(x)^{2} dx.
\end{equation}

Observe that it suffices to minimize the sum of the first two terms.
This depend on the unknown $f$, but an unbiased estimate is given by
$LSCV(h)$, with
\begin{align}
  \label{eq:31}
  LSCV(h) = \int_{-\infty}^{\infty} \hat f_{h}(x)^{2} dx - \frac{2}{n}
  \sum_{i=1}^{n} f_{-i, h}(x_{i})
\end{align} with
\begin{align}
  \label{eq:32}
  \hat f_{-i, h}(x) = \frac{1}{(n-1)h} \sum_{j \neq i} K(\frac{x - x_{j}}{h})
\end{align}

Minimization of $LSCV(h)$ yields $\hat h_{LSCV}$.

\subsection{Biased Cross-Validation}
\label{sec:bias-cross-valid}

Under regularity conditions,
\begin{align}
  \label{eq:34}
  \E{R(\hat f_{h})} = R(f'') + \frac{R(K'')}{n h^{5}} + O(h^{2}).
\end{align}

We can therefore define
\begin{align}
  \label{eq:36}
  BCV(h) = \frac{R(K)}{nh} + \frac{1}{4} \mu_{2}^{2}(K) \widetilde{R(f'')}
\end{align}

where
\begin{align}
  \label{eq:35}
  \widetilde{R(f'')} = R(\hat f_{h_{1}}) - \frac{R(K'')}{nh_{1}^{5}}
\end{align} with $h_{1}$ a ``pilot'' bandwidth (c.f Ward and Jones,
1995).  Minimization of $BCV(h)$ yields $\hat h_{BCV}$.

\subsection{Solve-the-equation Rules}
\label{sec:solve-equation-rules}

Under smoothness assumptions, we can integrate by parts to obtain
\begin{align}
  \label{eq:37}
  R(f'') = \int_{-\infty}^{\infty} f''''(x) f(x) dx = \E{f''''(X)}
\end{align}

We can therefore estimate $R(f'')$ by using
\begin{align}
  \label{eq:38}
  \hat R_{h_{2}} = \frac{1}{n} \sum_{i=1}^{n} \hat
  f_{h_{2}}''''(x_{i})
\end{align} where again $h_{2}$ is a pilot bandwidth.  By exploiting
the relationship between $h_{AMISE}$ and the $AMISE$-optimal bandwidth
for estimating $R(f'')$ in this way, we obtain an equation which can
be solved numerically to yield $\hat h_{SJE}$.


\section{Other Topics}
\label{sec:other-topics}

\subsection{Choice of Kernel}
\label{sec:choice-kernel}

The choice of kernel is coupled with the choice of bandwidth, because
if we replace $K(x)$ by $\frac{1}{2} K(\frac{1}{2})$ and we halve the
bandwidth, the estimate is unchanged.  We therefore fix the scale by
setting $\mu_{2}(K) = 1$.  Minimizing $AMISE(\hat f_{h})$ over $K$ the
amounts to minimizing $R(K)$ subject to
\begin{align}
  \label{eq:40}
  \int_{-\infty}^{\infty} K(x) dx = 1 \\
  \int_{-\infty}^{\infty} x K(x) dx = 0 \\
  \mu_{2}(K) = 1 \\
  K(x) \geq 0
\end{align}

The solution is given by the Epanechnikov kernel (1969).
\begin{align}
  \label{eq:41}
  K_{E}(x) = \frac{3}{4 \sqrt{5}}(1 - \frac{x^{2}}{5}) \I{|x| \leq \sqrt{5}}
\end{align}

The ratio $\frac{R(K_{E})}{R(K)}$ is called the \textbf{efficiency} of
a kernel $K$, because it represents the ratio of the sample sizes
needed to obtain the same $AMISE$ when using $K_{E}$ compared with $K$.


\begin{center}
% BEGIN RECEIVE ORGTBL testtbl
\begin{tabular}{lr}
Kernel & Efficiency \\
\hline
Epachnikov & 1.0 \\
Normal & 0.951 \\
Triangular & 0.986 \\
Uniform & 0.930 \\
\end{tabular}
% END RECEIVE ORGTBL testtbl
\end{center}

\subsection{Derivative Estimation}
\label{sec:deriv-estim-1}


A natural estimator of the $r$-th derivative $f^{(r)}$ of $f$ is given
by
\begin{align}
  \label{eq:33}
  \hat f^{(r)}_{h}(x) = \frac{1}{n h^{r+1}} \sum_{i=1}^{n} K(\frac{x -
  x_{i}}{h})
\end{align}
obtained from differentiating the standard KDE for $\hat f$.

Under regularity conditions,
\begin{align}
  \label{eq:39}
  MSE(\hat f^{(r)}_{h}(x)) = \frac{R(K^{(r)})}{nh^{2r+1}}f(x) +
  \frac{1}{4} h^{4} \mu_{2}^{2} f^{(r-2)}(x)^{2} + o(\frac{1}{nh} + h^{4}).
\end{align}

This leads to an optimal bandwidth of order $n^{-\frac{1}{2r + 5}}$
and a rate of converge of $n^{-\frac{4}{2r+5}}$.

\begin{boxthm}
  Estimating derivatives of densities is harder than estimating
  densities themselves.
\end{boxthm}

\subsection{Higher Order Kernels}
\label{sec:higher-order-kernels}

It is possible to make the dominant integrated squared bias term of
$MISE(\hat f_{h})$ vanish by choosing $\mu_{2}(K) = 0$.  This means
we have to allow the Kernel to take negative values, so the resulting
estimate need not be a density.

We can set $\hat f_{h}(x) = \max (\hat f_{h}(x), 0)$ and then
renormalize, but then we lose smoothness.  Nevertheless, we define $K$
to be a $k$-th order kernel if writing $\mu_{j}(K) =
\int_{-\infty}^{\infty} x^{j} K(x) dx$, we have
\begin{align}
  \label{eq:42}
  \mu_{0}(K) = 1
\end{align} and $\mu_{j}(K) = 0$ for $j = 1, \dots, k - 1$,
$\mu_{k}(K) \neq 0$, and
\begin{align}
  \label{eq:43}
  \int_{-\infty}^{\infty} |x|^{k} |K(x)| dx < \infty
\end{align} If $f$ has $k$ continuous bounded derivatives with
$R(f^{(k)}) < \infty$, then it is shown (example sheet) that
$h_{AMISE} = c n^{-\frac{1}{2k+1}}$ and
\begin{align}
  \label{eq:44}
  AMISE(\hat f_{h_{AMISE}}) = O(n^{-\frac{2k}{2k+1}})
\end{align}

Thus, under increasingly strong smoothness assumptions, convergence
rates arbitrarily close to the parametric rate of $O(n^{-1})$ can be
obtained.

The practical benefit of higher order kernels is not always apparent,
and the negativity/smoothness/bandwidth selection problems mean that
they are rarely used in practice.

\subsection{Local Bandwidths}
\label{sec:local-bandwidths}

Choosing $h = h(x)$  is problematic, because the resulting estimate
need not be a density.  However, we can define
\begin{align}
  \label{eq:45}
  \hat f(x) = \frac{1}{n} \sum_{i=1}^{n} K_{h(x)}(x - X_{i})
\end{align}

Theory suggests that we should choose $h(X_{i}) = h_{0}
f^{-\frac{1}{2}}(X_{i})$, and, with four derivatives and a second
order kernel, one can attain a ``fourth-order kernel'' rate of
$O(n^{-\frac{8}{9}})$.  There is no negativity problem, but we do
require pilot bandwidth selection.  Difficult to tune well and rarely
used in practice.

\subsection{Transformation Methods}
\label{sec:transf-meth}

It may be that $f$ is difficult to estimate, but it may be that we
can construct a strictly increasing, continuously differentiable
function $t$ on the support of $f$, such that, setting $Y_{i} =
t(X_{i})$, the density of $Y_{1}, \dots, Y_{n}$ is easier to estimate.
We ``back transform'' the estimate to obtain
\begin{align}
  \label{eq:46}
  \bar f_{h}(x) = \frac{1}{n} \sum_{i=1}^{n} K_{h}(t(x) - t(X_{i})) t'(x)
\end{align}

\subsection{Multi-Dimensional Density Estimation}
\label{sec:multi-dimens-dens}

The general $d$-dimensional kernel estimator is of the form
\begin{align}
  \label{eq:47}
  \hat f_{H}(x) = \frac{1}{n} (\det H)^{\frac{1}{2}} \sum_{i=1}^{n}
  K(H^{-\frac{1}{2}}(x - X_{i}))
\end{align} where $H$ is positive definite symmetric bandwidth matrix.
The difficulties of choosing the $\frac{1}{2} d(d+1)$ independent
entries mean that we often restrict attention to the diagonal $H$, or
even $H = h^{2} I$.  In this latter case,
\begin{align}
  \label{eq:48}
  AMISE(\hat f_{h^{2}I}) = \frac{R(K)}{n h^{d}} + \frac{1}{4} h^{4}
  \mu_{2}^{2}(K) \int_{\R^{d}} \{ \Delta_{f}(x) \}^{2} dx
\end{align} where $\Delta_{f}(x) = \sum_{j=1}^{d} \frac{\partial^{2}
  f}{\partial x_{j}^{2}}(x)$ is the Laplacian of $f$ at $x$.  This leads
to an
\begin{align}
  \label{eq:49}
  AMISE(\hat f_{h^{2}_{AMISE}I}) = O(n^{-\frac{4}{d + 4}})
\end{align}

Thus the ``curse of dimensionality'', together with bandwidth
selection problems, means that this is only really feasible for $d
\leq 4$.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "master"
%%% End:

\iffalse
#+ORGTBL: SEND testtbl orgtbl-to-latex :splice nil :skip 0
| Kernel     | Efficiency |
|------------+------------|
| Epachnikov |        1.0 |
| Normal     |      0.951 |
| Triangular |      0.986 |
| Uniform    |      0.930 |
\fi