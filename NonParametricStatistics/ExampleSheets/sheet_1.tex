
\chapter{Example Sheet 1}
\label{cha:example-sheet-1}

\begin{exercises}
\item
  By basic analysis, recall that if $X \sim \textsc{Gamma}(\alpha, 1)$
  and $Y \sim \textsc{Gamma}(\beta, 1)$, then
  \begin{equation}
    \label{eq:1}
    \frac{X}{X+Y} \sim \textsc{Beta}(\alpha, \beta)
  \end{equation}
  Since $S_{j} \sim \textsc{Gamma}(j, 1)$ and $S_{n+1} - S_{j} \sim
  \textsc{Gamma}(n + 1 - j)$, we have our result.

  Now, consider the distribution of $U_{(k)}$.  Consider the density $f_{(k)}(x)$  Then we have
  \begin{align}
    \label{eq:2}
    f_{(k)}(x) = C x * x^{k-1} (1-x)^{n-k} 
  \end{align} which is of the form of a $\textsc{Beta}(k, n - k + 1)$
  distribution as required.
\item
  \begin{enumerate}
  \item $f(x) = e^{tx}$ is convex on $[a, b]$.  So, by definition,
    \begin{equation}
      \label{eq:3}
      f(ta + (1-t)b) \leq tf(a) + (1-t)f(b)
    \end{equation} for all $t \in [0, 1]$.

    Then letting $x = ta + (1-t)b$, we have
    \begin{align}
      \label{eq:4}
      f(x) \leq \frac{b-x}{b-a} f(a) + \frac{x-a}{b-a}f(b)
    \end{align} and taking expectations yields
    \begin{align}
      \label{eq:5}
      \E{f(x)} \leq \frac{b}{b-a}e^{ta} + \frac{-a}{b-a}e^{tb}
    \end{align}

    Letting $p = -\frac{a}{b-a}$, we have $1-p = \frac{b}{b-a}$, and
    so ...
    \todo{Finish writing this section up}
  \item We have
    \begin{align}
      \label{eq:6}
      \Prob{\sum Y_{i} > \epsilon} &= \Prob{e^{t \sum Y_{i}} > e^{t
          \epsilon}} \\
      &\leq \frac{\E{\prod e^{t Y_{i}}}}{e^{t \epsilon}} \\
      &= \frac{\prod \E{e^{tY_{i}}}}{e^{t \epsilon}} \\
      &\leq \frac{\prod e^{t^{2}(b_{i} - a_{i})^{2} / 8}}{e^{t
          \epsilon}} \\
      &= e^{\frac{t^{2}}{8}\sum (b_{i} - a_{i})^{2} - t \epsilon}
    \end{align}
    and letting
    \begin{equation}
      \label{eq:7}
      s = \frac{4t}{\sum (b_{i} - a_{i})^{2e}}
    \end{equation} and using the union bound we obtain our result.    
  \end{enumerate}
\item
  This follows easily from the previous result.
  \begin{align}
    \label{eq:8}
    \Prob{|\hat P_{n}(A) - \Prob{A}| > \epsilon} &= \Prob{|\sum
      (\I{X_{i} \in A} - \Prob{A})| > n \epsilon} \\
    &\leq 2 e^{-\frac{2 (n \epsilon)^{2}}{\sum 1^{2}}} \\
    &= 2 e^{-2 n \epsilon^{2}}
  \end{align}
\item
  \begin{enumerate}
  \item
      Distribution is multinomial (multivariate binomial) with
      \begin{align}
        \label{eq:9}
        MN(n, F(t_{1}), F(t_{2}) - F(t_{1}), \dots, 1 - F(t_{k}))
      \end{align}
    \item Consider the distribution of $(\hat F_{n}(t_{1}), \dots,
      \hat F_{n}(t_{k}))$.  Then from statistics of the multinomial
      distribution, we have that this has mean $(F(t_{1}), \dots,
      F(t_{k}))$, and covariance $\Sigma_{ii} = nF(t_{i \vee j})(1 -
      F(t_{i \wedge j}))$.

      Thus, by the multivariate CLT, this merely converges $MN(0, \Sigma_{ii})$
  \end{enumerate}
\item
  We have $\E{W_{t}} = 0$ and
  \begin{align}
    \label{eq:12}
    \Cov{W_{t}}{W_{s}} &= \Cov{B_{t} - t B_{1}}{B_{s} - s B_{1}} \\
    &= \Cov{B_{t}}{B_{s}} + st\Cov{B_{1}}{B_{1}} - s \Cov{B_{t}}{B_{s}} - t \Cov{B_{t}}{B_{s}} \\
    &= t \vee s + ts - t(t \vee s) - s(t \vee s) \\
    &= (t \vee s)(1 - t \wedge s)
  \end{align}
\item
  \begin{enumerate}
  \item \todo{Type up this long computation}
  \item \todo{Type up this long computation}
  \end{enumerate}
\item
  \todo{Type up this long computation - and double-check we take the
    Taylor expansion about h = 0}
\item
  This follows quite easily, we have
  \begin{align}
    \label{eq:13}
    p_{b}(x) &= \Prob{X_{1} \in I_{b}(x)} \\
    &= \int_{t_{b}(x)}^{t_{b}(x) + b} f(y) dy \\
    &= \int_{t_{b}(x)}^{t_{b}(x) + b} f(x) + f'(y-x)(y-x) + O(b^{2}) \\
    &= bf(x) + f'(x) \int_{t_{b}(x)}^{t_{b}(x) + b} (y-x) dy +
    O(b^{3}) \\
    &= bf(x) + \frac{1}{2} f'(x)[b^{2} - 2b(x - t_{b}(x))] + O(b^{3})
  \end{align}

  Now, we have
  \begin{align}
    \label{eq:14}
    \E{\tilde f_{b}(x)} = \frac{\sum \Prob{X_{i} \in I_{b}(x)}}{nb} = \frac{p_{b}(x)}{b}
  \end{align} and
  \begin{align}
    \label{eq:15}
    \Var{\tilde f_{b}(x)} &= \frac{1}{n^{2}b^{2}} n \Var{\I{X_{1} \in
        I_{b}(x)}} \\
    &= \frac{1}{nb^{2}} p_{b}(x) (1-p_{b}(x)) \\
    &= \frac{1}{nb}(f(x)) + O(\frac{1}{n})
  \end{align}

  Then using the bias variance decomposition,
  \begin{align}
    \label{eq:16}
    \frac{f(x)}{nb} + \left(\frac{1}{2} f'(x)[b - 2(x -
      t_{b}(x))]\right)^{2} + O(\frac{1}{n} + b^{3}) \\
  \end{align} which expands out to the correct solution.
\item
  Follows from integrating over all $x$, Taylor expanding out the
  integrands after summing over all bins.

  Taking derivatives, we obtain
  \begin{align}
    \label{eq:17}
    -\frac{1}{nb^{2}} + \frac{2b}{12} R(f') = 0
  \end{align} with solution
  \begin{align}
    \label{eq:19}
    \left(\frac{6}{nR(f')} \right)^{\frac{1}{3}}
  \end{align}
  and so our AMISE scales with
  \begin{align}
    \label{eq:18}
    n^{-\frac{2}[3]} R(f)^{\frac{1}{3}}
  \end{align}
\item
  $|f - f_{n}|$ is dominated by $f + f_{n}$, which is integrable.
  Thus, the DCT gives the required result.
\item
  \begin{align}
    \label{eq:20}
    h \int_{-\infty}^{\infty} (K_{h}^{2} \star f)(x) dx &= h
    \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \frac{1}{h^{2}}
    K^{2}(\frac{x-y}{h}) f(y) dy dx \\
    &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} K^{2}(u) f(x -
    hu) du dx \\
    &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} K^{2}(u) f(x -
    hu) dx du \\
    &= \int_{-\infty}^{\infty} K^{2}(u) du \\
    &= R(K)
  \end{align} by Fubini, since all terms are non-negative.

  Let $\epsilon > 0$.  Then
  \begin{align}
    \label{eq:21}
    \int_{-\infty}^{\infty} K(u)f(x-hu) du - f(x) &=
    \int_{-\infty}^{\infty} K(u)(f(x - hu) - f(x)) du
  \end{align} and taking absolute values gives
  \begin{align}
    \label{eq:22}
    \int_{-\infty}^{\infty} |K(u)| |f(x-hu) du - f(x)| &\leq
    \int_{-\infty}^{\infty} |K(u)| \epsilon
  \end{align} for some $\epsilon'$ and all $n > N_{\epsilon}$. Then as
  $|K(u)| = K(u)$, and $\int_{-\infty}^{\infty} K(u) du = 1$, we have
  \begin{align}
    \label{eq:23}
    |(K_{h} \star f)(x) - f(x)| < \epsilon
  \end{align} for all $n > N_{\epsilon}$. Thus, we obtain the required
  result.

  Assuming otherwise, there would exist some $x'$ and some sequence
  $h_{n}$ such that $(K_{h} \star f)(x') \rightarrow \infty$.
  However, by the previous result, $f(x') \rightarrow \infty$, and by
  the condition of boundedness of the second derivative, $f$ is
  bounded.  By contradiction, no such $x'$ exists.

  Thus, we  can apply this result to the functions $g_{n} = (K_{h}
  \star f)^{2}$ which converges pointwise to $f(x)^{2}$ (by continuous
  mapping theorem and the previous result).  Thus, by the dominated
  convergence theorem, we have our result,
  \begin{align}
    \label{eq:24}
    \int_{-\infty}^{\infty} (K_{h} \star f)^{2}(x) dx \rightarrow
    \int_{-\infty}^{\infty} f(x)^{2} dx
  \end{align}

  We have
  \begin{align}
    \label{eq:25}
    \Var{\hat f_{h}(x)} &= \frac{1}{n} \hat \Var{\frac{1}{h}K(\frac{x
        - X}{h})} \\
    &= \frac{1}{n} \E{\frac{1}{h^{2}} K^{2}(\frac{x-X}{h})} -
    \frac{1}{n} \E{\frac{1}{h} K(\frac{x - X}{h})}^{2} \\
    &= \frac{1}{hn}R(K) + O(\frac{1}{n})
  \end{align}
\item
  \todo{This comes from the exact Taylor expansion of $f(x - hu) =
    f(x) - \int_{0}^{hu} f''(t) t$, but it's pretty painful.}
\end{exercises}

%%% Local Variables: 
%%% TeX-master: "master""
%%% End: 
