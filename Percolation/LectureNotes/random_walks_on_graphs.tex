
\chapter{Random Walks on Graphs}
\label{cha:random-walks-graphs}

Our basic setting is the (hyper-)cubic lattice on $\R^{d}, d \geq 1$.
This is the graph with vertex set $\Z^{d}$, edges $\IP{x, y} \iff \| x
- y \|_{1} = 1$, and edge set denoted $E^{d}$.  A lattice is $L^{d} =
(\Z^{d}, E^{d})$.

\section{Percolation}
\label{sec:percolation}

Let $0 < p < 1$. Let $e \in E^{d}$, and with probability $p$
independently for each edge, declare $e$ to be \textbf{open} else
\textbf{closed}. Consider $x \leftrightarrow y$ if there exists an
open path from $x$ to $y$. The \textbf{open cluster} at $x$ is $C_{x}
= \{ y: x \leftrightarrow y \}$.

\begin{boxthm}
  For a given $p$, what can be said about the $C_{x}$?
\end{boxthm}

For $p = 1$, $C_{x} = Z_{d}$.  For $p = 0$, $C_{x} = \{ x \}$.

\begin{defn}[Percolation probability]
  \label{defn:random_walks_on_graphs:1}
  Let $\theta(p) = \Prob{|C_{\theta}|} = \Prob_{p}$. Note that
  $\theta$ is non-decreasing.

  Let $p_{c} = \sup \{ p : \theta(p) = 0 \}$.

  It is known that $\theta$ is $C^{\infty}$ on $(p_{c}, 1]$, and that
  $\theta$ is right-continuous on $[0, 1]$.

  It is believed that $\theta$ is concave on $(p_{c}, 1]$, and that
  $\theta$ is real-analytic on $(p_{c}, 1]$, and that $\theta(p_{c}) =
  0$ (known for $d = 2$, and $d \geq 16$).
\end{defn}

\begin{defn}
  \label{defn:random_walks_on_graphs:2}
  Probability theorey.  Let $\Omega = \{ 0, 1\}^{E^{d}}$,
  $\mathcal{F}$ be the $\sigma$-filed generated by the
  finite-dimensional cylinder ... of form .
  $\{ \omega \in \Omega: \omega = \text{$\xi$ on $\mathcal{F}$}\} =
  E_{F}(\xi)$.
  ...
  \todo{Fill in from lecture notes}
\end{defn}

\begin{thm}
  \label{defn:random_walks_on_graphs:3}
  For $d \geq 2$, $0 < p_{c} < 1$.
\end{thm}

\todo{Fill in lecture notes from Chapter 3 of Probability on Graphs}

Consider $\Z^{d}$, with $\kappa_{n} = \mu^{n(1 + o(1))}$ as $n
\rightarrow \infty$, $\mu = \mu(\Z^{d})$

We have $\kappa_{n} \sim A n^{c} \mu^{n}$ for some $A = A(d), c =
c(d)$ where $a_{n} \sim b_{n}$ means $\frac{a_{n}}{b_{n}} \rightarrow
1$.

$c_{n}$ is called the \textbf{critical exponent}.  People are hoping
to show that for $d=2$, $c=\frac{11}{32}$.  $c$ is expected to be
\textbf{universal} in that it depends on $d$ but not each
$d$-dimensional graph.


\section{Coupling}
\label{sec:coupling}

Let $L^{d} = (\Z^{d}, E^{d})$ consider $P_{p}$ on $\Omega = \{ 0,
1\}^{E^d}$.

Let $(U_{e}, e \in E)$ be independent uniform random variables $U(0,
1)$.

Let $p \in (0, 1)$.  Then
\begin{equation}
  \label{eq:1}
  \mu_{p}(e) =
  \begin{cases}
    0 & U_{e} \geq p \\
    1 & U_{e} < p
  \end{cases}  
\end{equation}
if $p_{1} \leq p_{2}$ then $\mu_{p_{1}}(e) \leq \mu_{p_{2}}(e)$.

$\mu_{p}: 0 < p < 1$ is a coupling of percolations, containing all
interesting, ``universal'' in $p$.

\begin{thm}
  \label{defn:random_walks_on_graphs:4}
  For any increasing function $f: \Omega \rightarrow \R$,
  \begin{equation}
    \label{eq:2}
    \E{f}{p_{1}} \leq \E{f}{p_{2}}
  \end{equation} for $p_{1} \leq p_{2}$.
\end{thm}

\begin{exmp}
  \label{defn:random_walks_on_graphs:5}
  For example, $u, v \in \Z^{d}$, $f(u) = \I{u \leftrightarrow v}$.
  Then $\Prob{u \leftrightarrow v}{p_{1}} \leq \Prob{u \leftrightarrow
  v}{p_{2}}$.
\end{exmp}

\section{Oriented/Directed Percolations}
\label{sec:orient-perc}

Consider the standard percolation, and define
\begin{align}
  \label{eq:20}
  \overrightarrow{\theta}(p) = \Prob{\text{there exists an infinite
  directed path through the origin to $p$}}.
\end{align} Then $\overrightarrow{p_{c}} = \sup \{ p:
\overrightarrow{\theta}(p) = 0 \}$. As $\overrightarrow{\theta}(p)
\leq \theta(p)$, we have $\overrightarrow{p_{c}} \geq p_{c}$.


\section{Correlation Inequalities}
\label{sec:corr-ineq}

Consider a set $E$ be nonempty and finite, and $\Omega = \{ 0, 1
\}^{E}$.  The sample space $\Omega$ is partially ordered by
$\omega_{1} \leq \omega_{2}$ if $\omega_{1}(e) \leq \omega_{2}(e)$ for
all $e \in E$.

Event $A \subseteq \Omega$ is called \textbf{increasing} if $w \in A$,
$w \leq w' \Rightarrow w' \in A$ and decreasing if $\overline A =
\Omega \backslash A$ is increasing.

\begin{defn}
  \label{defn:random_walks_on_graphs:6}
  With two probability measures $\mu_{1}, \mu_{2}$, we write $\mu_{1}
  \leq_{st} \mu_{2}$ if $\mu_{1}(A) \leq \mu_{2}(A)$ for all
  increasing events $A$.

  Equivalently, $\mu_{1} \leq_{st} \mu_{2}$ if and only if $\mu_{1}(f)
  = \sum_{\Omega} f(\omega) \mu_{1}(\omega) \leq \mu_{2}(f)$ for all
  increasing functions $f: \Omega \rightarrow \R$.
\end{defn}

Let $S \subseteq \Omega^{2}$ given by $S = \{ (\pi, \omega) \in
\Omega^{2}: \pi \leq \omega \}$.

\begin{thm}[Strassen]
  \label{defn:random_walks_on_graphs:7}
  The following are equivalent:
  \begin{enumerate}
  \item $\mu_{1} \leq \mu_{2}$
  \item There exists a probability measure $\kappa$ on $\Omega^{2}$ such
    that
    \begin{enumerate}
    \item $\kappa(S) = 1$
    \item Marginals of $\kappa$ are $\mu_{1}$ and $\mu_{2}$.
    \end{enumerate}
  \end{enumerate}
\end{thm}

\begin{proof}
  From reference in the back.
\end{proof}

\begin{thm}[Holley's inequality]
  \label{defn:random_walks_on_graphs:8}
  Let $\mu_{1}, \mu_{2}$ be probability measures which are positive
  (in that $\mu_{i}(\omega) > 0$ for all $i$ and $\omega \in \Omega$).
  If
  \begin{align}
    \label{eq:3}
    \mu_{2}(\omega_{1} \vee \omega_{2}) \mu_{1}(\omega_{1} \wedge
    \omega_{2}) \geq \mu_{1}(\omega_{1})\mu_{2}(\omega_{2})
  \end{align} for all $\omega_{1}, \omega_{2} \in \Omega$, then
  $\mu_{1} \leq \mu_{2}$.

  The notation is
  \begin{align}
    \label{eq:4}
    (\omega_{1} \vee \omega_{2})(e) &= \max{\omega_{1}(e),
      \omega_{2}(e)} \\
    (\omega_{1} \wedge \omega_{2})(e) &= \min{\omega_{1}(e), \omega_{2}(e)}
  \end{align} 
\end{thm}

\begin{proof}
  See Probability and Random Processes (Stirzaker), the section on
  Markov chains in continuous time for the necessary background.

  \begin{defn}[Markov Chains]
    \label{defn:random_walks_on_graphs:9}
    $(X_{t}, t \geq 0)$ taking values in a state space $S$, which is
    finite satisfying the Markov property 
  \end{defn}

  \begin{defn}[Markov Property]
    \label{defn:random_walks_on_graphs:10}
    For all $x, y \in S, x \neq y$,
    \begin{align}
      \label{eq:5}
      \Prob{X_{t + h} = y | X_{t} = x} = h G(x, y) + o(h)
    \end{align} as $h \downarrow 0$

    The matrix $G = (G(x, y))_{x, y \in S}$ is the \textbf{generator}
    of the Markov chain.  The diagonal elements $G(x, x)$ are chosen
    such that the row sums are all zero,
    \begin{align}
      \label{eq:6}
      \sum_{y \in S} G(x, y) = 0
    \end{align} for all $x \in S$.
  \end{defn}

  \begin{defn}[Invariant distribution]
    \label{defn:random_walks_on_graphs:11}
    $\pi$ on $S$ is an invariant distribution if it satisfies if
    $X_{0}$ has distribution $\pi$, then $X_{t}$ has distribution
    $\pi$ for all $t \geq 0$.
  \end{defn}

  \begin{lem}
    $\pi$ is invariant if and only if $\pi G = 0$.
  \end{lem}

  \begin{defn}
    \label{defn:random_walks_on_graphs:12}
    $X$ is \textbf{time reversible} if $\pi(x) G(x, y) = \pi(y) G(y,
    x)$ for all $x, y \in S$ where $\pi$ is (say) invariant.

    If detailed balance holds for some $\pi$ then $\pi$ is invariant.
  \end{defn}

  Let $\mu$ be a positive probability measure on $\Omega$.

  For $\omega \in \Omega$ and $e \in E$, define the configurations
  $\omega^{e}, \omega_{e}$ by
  \begin{align}
    \label{eq:7}
    \omega^{e}(f) = \begin{cases}
      w(f) & f \neq e \\
      1 & f = e
    \end{cases} \\
    \omega^{e}(f) = \begin{cases}
      w(f) & f \neq e \\
      0 & f = e
    \end{cases}
  \end{align}

  Let $G: \Omega^{2} \rightarrow \R$ be given by $G(\omega_{e},
  \omega^{e}) = 1$, $G(\omega^{e}, \omega_{e}) =
  \frac{\mu(\omega_{e})}{\mu(\omega^{e})}$, for all $\omega \in
  \Omega$ and $e \in E$.  Set $G(w, w') = 0$ for all other elements
  (coordinate distance greater than 2), and $G(w, w) = - \sum_{\omega'
    \neq \omega} G(\omega, \omega')$.

  $G$ is the generator for a Markov chain $X$ on $\Omega$.
  We then have
  \begin{align}
    \label{eq:8}
    \mu(\omega) G(\omega, \omega') = \mu(\omega') G(\omega', w)
  \end{align} (trivial from the construction of $G$).

  Thus, $\mu$ is invariant for $X$.

  Now, construct a Markov chain $((X_{t}, Y_{t}) t \geq 0)$ taking
  values in $S = \Omega^{2}$.  Let $\mu_{1}, \mu_{2}$ be positive
  probability measures on $\Omega$, assumed positive.

  Let $G$ be given by
  \begin{align}
    \label{eq:9}
    G((\pi_{e}, \omega), (\pi^{e}, \omega^{e})) &= 1 \\
    G((\pi, \omega^{e}), (\pi_{e}, \omega_{e}))
    &= \frac{\mu_{2}(\omega_{e})}{\mu_{2}(\omega^{e})} \\
    G((\pi^{e}, \omega^{e}), (\pi_{e}, \omega^{e}))
    &= \frac{\mu_{1}(\pi_{e})}{\mu_{1}(\pi_{e})} -
    \frac{\mu_{2}(\omega_{e})}{\mu_{2}(\omega^{e})} \geq 0
  \end{align} from the conditions of the theorem.

  Defining $G(x, y) = 0$ otherwise and $G(x, x)$ to satisfy the zero
  row-sum condition, we have that $G$ is a Markov chain.  Thus it has
  an invariant measure $\mu$.  Then $X$ is a Markov chain, having
  measure $\mu_{1}$.  $Y$ is a Markov chain, having invariant measure
  $\mu_{2}$.

  Then by Strassen's theorem (~\ref{defn:random_walks_on_graphs:7}),
  $\mu_{1} \leq \mu_{2}$.

  Choose $\mu_{1}, \mu_{2}$ satisfying the condition.  Let $Z = (X,
  Y)$ a Markov chain on $\Omega^{2}$, in fact on $S = \{ (\pi,
  \omega): \pi \leq \omega \}$.

  $X$ is a Markov chain with invarnaitn measure $\mu_{1}$.  $Y$ is a
  Markov chain with invariant distirbuiton $\mu_{2}$.
  
  Then $Z$ has an invariant measure $\kappa$ on $S$.  Let $f: \Omega
  \rightarrow \R$ be increasing.  Then $\mu_{1}(f) = \kappa(f(\pi))
  \leq \kappa(f(\omega)) = \mu_{2}(f)$.

  This completes the proof.
\end{proof}

\begin{thm}[FKG inequality]
  \label{defn:random_walks_on_graphs:13}
  Let $\mu$ be a probability measure on $\Omega = \{ 0, 1 \}^{E}$ with
  $|E| < \infty$ such that $\mu$  is positive and
  \begin{align}
    \label{eq:10}
    \mu(\omega_{1} \vee \omega_{2}) \mu(\omega_{1} \wedge \omega_{2})
    \geq \mu(\omega_{1}) \mu(\omega_{2}),
  \end{align} known as the \textbf{FKG lattice condition}.

  Then $\mu$ is \textbf{positively associated} in that
  \begin{align}
    \label{eq:11}
    \mu(fg) \geq \mu(f) \mu(g)
  \end{align} for all increasing random variables $f, g: \Omega
  \rightarrow \R$ or equivalently,
  \begin{align}
    \label{eq:12}
    \mu(A \cap B) \geq \mu(A) \mu(B)
  \end{align} for all increasing events $A, B$.
\end{thm}

\begin{exmp}
  \label{defn:random_walks_on_graphs:14}
  Consider a percolation, with $A = \{ x \leftrightarrow \}$, $B = \{
  u \leftrightarrow v \}$. Then we have

  \begin{align}
    \label{eq:13}
    \Prob{x \leftrightarrow | u \leftrightarrow v}{p} \geq \Prob{x
      \leftrightarrow y}{p}.
  \end{align}
\end{exmp}

\begin{history}
  When $\mu$ is a product measure, this was first proven by Harris
  (1961) by induction on $|E|$.  
\end{history}

\begin{proof}
  Let $\mu_{1} = \mu$. Note that \eqref{eq:11} is invariant under $g
  \mapsto g + c$, for $c \in \R$.  Thus we may assume that $g$ is
  strictly positive.  Then
  \begin{align}
    \label{eq:14}
    \mu_{2}(\omega) = \frac{\mu(\omega) g(\omega)}{\sum_{w'}
      g(\omega') \mu(\omega')}  
  \end{align}

  Since $g$ is increasing, $\mu_{1} \leq \mu_{2}$ follows by the FKG
  lattice condition. By the Holley inequality, $\mu_{1}(f) \leq
  \mu_{2}(f)$ for $f$ increasing. Therefore, $\mu(f) \leq
  \frac{\mu(fg)}{\mu(g)}$ as required.
\end{proof}

\subsection{The BK Inequality}
\label{sec:bk-inequality}

Consider $\Omega = \{ 0, 1 \}^{E}, |E| < \infty$.  Let $\omega \in
\Omega, F \subseteq E$.  The consider
\begin{align}
  \label{eq:15}
  C(\omega, F) = \{ w' \in \Omega: \omega'(e) = \omega(e) \forall e
  \in F \} = (w(e): e \in F) \times \{ 0, 1 \}^{E \backslash F}
\end{align}

Let $A, B \subseteq \Omega$.  Then define
\begin{align}
  \label{eq:16}
  A \square B = \{ \omega \in \Omega: \exists F \subseteq E, C(\omega, F)
  \subseteq A, C(\omega, \overline F) \subseteq B \} \subseteq A \cap B.
\end{align}

If $A, B$ are increasing, then $C(\omega, F) \subseteq A$ if and only
if $\omega_{F} \in A$, where
\begin{align}
  \label{eq:17}
  \omega_{F}(e) =
  \begin{cases}
    w(e) & e \in F \\
    0 & e \notin F
  \end{cases}
\end{align}
In this case $A \square B = \{ \omega : \exists F \subseteq E s.t. \omega_{F}
\in A, \omega_{E \backslash F} \in B\}$.

\begin{thm}[BK inequality]
  \label{defn:random_walks_on_graphs:15}
  For increasing subsets For product measure $\Prob$ (say $\Prob{w(e)
    = 1}{p_{e}}$ for some given $(p_{e}, e \in E)$),

  \begin{align}
    \label{eq:18}
    \Prob{A \square B} \leq \Prob{A} \Prob{B}
  \end{align} for all increasing events $A, B$.
\end{thm}

\begin{thm}[Reimer's inequality]
  \label{defn:random_walks_on_graphs:16}
  \begin{align}
    \label{eq:19}
    \Prob{A \square B} \leq \Prob{A} \Prob{B}
  \end{align} for all $A, B \subseteq \Omega$ and product measures $\Prob$.
\end{thm}

\section{Influence}
\label{sec:influence}

\begin{question}
  What is the influence of an individual in an election?
\end{question}

\begin{question}
  An increasing event $A$, a sequence of measures $\Prob_{p}$, and
  consider $g(p) = \Prob_{p}(A)$.
\end{question}

For example, consider a problem from reliability theory - an
electrical network has every link cut with probability $1 - p$, and
what is the probability that the network is still connected? This
class of theorems are called ``$S$-shaped theorems''.

$\Omega = \{ 0, 1 \}^{E}$, $|E| < \infty$, $|E| = N$, $A \subseteq
\Omega$.

Let $e \in E$.

\begin{defn}
  \label{defn:random_walks_on_graphs:17}
  The influence of $e$ on $A$  is
  \begin{align}
    \label{eq:21}
    I_{A}(e) = \Prob_{p}(\I{A}(\omega^{e}) \neq \I_{A}(\omega_{e})).
  \end{align}

  If $A$ is increasing, then
  \begin{align}
    \label{eq:22}
    I_{A}(e) = \Prob_{p}(A^{e}) - \Prob_{p}(A_{e}).
  \end{align} where
  \begin{align}
    \label{eq:23}
    A^{e} = \{ \omega : \omega^{e} \in A \} \\
    A_{e} = \{ \omega : \omega_{e} \in A \} \\
  \end{align}
\end{defn}

\begin{thm}[Kahn-Kalani-Limial, Talagrand]
  \label{defn:random_walks_on_graphs:18}
  There exists $c > 0$ such that for all $\epsilon, A$ and $0 < p <
  1$.  Then
  \begin{align}
    \label{eq:24}
    \sum_{e \in E} I_{A}(e) \geq c[\Prob_{p}(A)\Prob_{p}(\overline A)] \log
    \frac{1}{\max_{e \in E} I_{A}(e)}.
  \end{align}
\end{thm}

\begin{proof}
  One uses discrete Fourier analysis (but non-examinable).
\end{proof}

\begin{thm}
  \label{defn:random_walks_on_graphs:20}
  It is interesting if we have  uniform upper bound $M_{p}$ for the
  $I_{A}(e)$.

  Let $m = \max_{e \in E} I_{A}(e)$.  Then we can write
  \begin{align}
    \label{eq:25}
    mN \geq  [\cdots] \log \frac{1}{m} \\
    m \geq \frac{[\cdots]}{N} \log \frac{1}{m} \geq [\cdots]'
    \frac{\log N}{N}.
  \end{align}
\end{thm}

\begin{thm}[Restatement of KKL]
  \label{defn:random_walks_on_graphs:21}
  The maximum influence $M$ satisfies
  \begin{align}
    \label{eq:26}
    m \geq c' \Prob_{p}(A) \Prob_{p}(\overline A) \frac{\log N}{N}
  \end{align}
  for some universal $c' > 0$.

  The $\frac{\log N}{N}$ is optimal.
\end{thm}

\begin{exmp}[Tribes]
  \label{defn:random_walks_on_graphs:22}
 Consider $N$ people partitioned into $t$ tribes, each of size $s =
 \log N - \log \log N + \alpha$, and let $p = \frac{1}{2}$.

 Then let
 \begin{equation}
   \label{eq:27}
   A = \{ \text{There exists a tribe all of whose elements are
     1} \}   
 \end{equation}

 Then
 \begin{align}
   \label{eq:28}
   I_{A}(e) \sim c \Prob{A} \Prob{\overline A} \frac{\log N}{N}
 \end{align} for all $e$.
\end{exmp}

\begin{thm}[Symmetric Case]
  \label{defn:random_walks_on_graphs:23}
  If $I_{A}(e)$ is a constant for $e \in E$,
  \begin{align}
    \label{eq:29}
    \sum_{e \in E} I_{A}(e) \geq c[\Prob_{p}(A) \Prob_{p}(\overline
    A)] \log N.
  \end{align}
\end{thm}

\section{Sharp Threshold}
\label{sec:sharp-threshold}

Let $\Omega$ as before, $A \subseteq \Omega$.  Then
\begin{thm}[Rousseau, Margoulis]
  \label{defn:random_walks_on_graphs:24}
  \begin{align}
    \label{eq:31}
    \frac{d}{dp} \Prob_{p}(A) = \sum_{e \in E} \Prob_{p}(A^{e}) - \Prob_{p}(A^{e}).
  \end{align}
  Note this is equal to $\sum_{e \in E} I_{A}(e)$ if $A$ is increasing.
\end{thm}

\begin{proof}
  Need to only consider
  \begin{align}
    \label{eq:32}
    \Prob_{p}(A) = \sum_{\omega} \I{A}(\omega) p^{|\eta|}(1-p)^{N - |\eta|}
  \end{align} where $N = |E|$, $\eta = \{ e: \omega(e)= 1 \}$.

  Then
  \begin{align}
    \label{eq:33}
    \frac{d}{dp}\Prob_{p}(A) = \sum_{\omega} \I{A}(\omega)
    (\frac{|\eta|}{p} - \frac{N - |\eta|}{1 - p}) p^{|\eta|}(1-p)^{|N
      - |\eta|}
  \end{align}
  and so
  \begin{align}
    \label{eq:34}
    p(1-p) \frac{d}{dp}\Prob_{p}(A) = \sum_{\omega}
    \I{A}(\omega)(|\eta| - Np) p^{|\eta|}(1-p)^{N - |\eta|} \\
    = \Prob_{p}(\I{A}(|\eta| - Np)) \\
    = \sum_{e} \Prob_{p}(\I{A}(\I{e} - p)) \\
    = \sum_{e} \Prob_{p}(\I{A}\I{e}) - p \Prob_{p}(A) \\
    = \sum_{e} p \Prob_{p}(A^{e}) - p(p \Prob_{p}(A^{e}) + (1-p) \Prob_{p}(A_{e}))
  \end{align} where $\I{e} = \I{\text{$e$ open}} = \omega(e)$, so
  $|\eta| = \sum_{e} \I{e}$.

  This completes the proof.
\end{proof}

\section{Back to Percolation}
\label{sec:back-percolation}

Let $L^{d} = (\Z^{d}, \E^{d}), 0 < p < 1$, and measure $\Prob_{p}$.
Let $N$ be the number of open clusters.  Then
\begin{align}
  \label{eq:30}
  \Prob_{p}(N \geq 1) =
  \begin{cases}
    \label{eq:38}
    0 & p < p_{c} \\
    \label{eq:39}
    1 & p > p_{c}
  \end{cases}
\end{align}

Then $\theta(p) = \Prob_{p}(0 \in \text{infinite open cluster})$. So
\begin{align}
  \label{eq:35}
  \theta =
  \begin{cases}
    \label{eq:36}
    0 & p  < p_{c} \\
    \label{eq:37}
    > 0 & p > p_{c}
  \end{cases}
\end{align}

To show \eqref{eq:36} implies \eqref{eq:38}, we have $\Prob_{p}(N \geq
1) \leq \sum_{x \in \Z^{d}}\Prob_{p}(x \in \text{infinite open
  cluster}) = \sum_{x} 0 = 0$.

To show \eqref{eq:37} implies \eqref{eq:39}, by Kolmogrov's zero-one law, we have $\Prob_{p}(N \geq 1) \in \{ 0, 1
\}$, but $\Prob_{p}(N \geq 1) \geq \theta(p) > 0$ for $p > p_{c}$.

\begin{thm}[Uniqueness of infinite cluster]
  \label{defn:random_walks_on_graphs:19}
  For all $0 < p < 1$, either
  \begin{align}
    \label{eq:41}
    \Prob_{p}(N = 0) = 1
  \end{align} or
  \begin{align}
    \label{eq:42}
    \Prob_{p}(N = 1) = 1
  \end{align}
\end{thm}

\begin{proof}
  Fix $p \in (0, 1)$.

  \begin{lem}[Part A]
    There exists $k = k_{p} \in \{ 0, 1, ,2, \dots, \} \cup \{ \infty
    \}$ with $\Prob_{p}(N = k) = 1$.
  \end{lem}

  \begin{proof}
    $\L^{d}$ comes equipped with a shift translation, and the measure
    is invariant under this shift.  Thus $N = N(\omega)$ is invariant
    under the shift.
  \end{proof}

  This proof requires this lemma.
  \begin{lem}
    Any shift-invariant random variable on $(\Omega, \mathcal{F},
    \Prob_{p})$ is almost surely constant.
  \end{lem}

  \begin{proof}
    Elementary application of measure theory. 
  \end{proof}

  \begin{lem}[Part B]
    $k_{p} \in \{ 0, 1, \infty \}$ - the ``finite-energy property''.
  \end{lem}

  \begin{proof}
    Suppose $2 \leq k_{p} < \infty$.

    Find $n$ such that $\Prob_{p}(\Lambda_{n} \text{intersections
      $\geq 2$ infinite open clusters}) > \frac{1}{2}$.
    \todo{Follow this argument? p93 in Probability on Graphs}
  \end{proof}

  \begin{lem}[Part C]
    $k_{p} \neq \infty$.
  \end{lem}

  \begin{proof}
    Say $x$ is a trifurcation if
    \begin{enumerate}
    \item $|C_x| = \infty$.
    \item The removal of $x$ breaks $C_{x}$ into three disjoint
      infinite clusters.
    \end{enumerate}

    Then $\tau = \Prob_{p}(\text{$x$ is a trifurcation})$ is
    independent of $x$.

    We claim $\tau > 0$.  To show this, take a large diamond box that
    intersects with at least three open clusters.  Then there exists
    $n$ such that $\Prob_{p}(\text{$S_{n}$ intersects $\geq 3$
      infinite open clusters}) > \frac{1}{2}$.

    Thus $\tau > 0$.

    The argument is then that we use the ration beween boundary and
    volume to bound the number of trifuricatinos in $A_n$, and show
    that this leads to a contradiction for large $n$.
  \end{proof}
  

  \todo{Fill in rest of proof.  Required to understand high-level
    ideas and key steps around the graphs}

  N.B. - consider the corresponding proof for site percolation.  For
  $x, y, z \in \partial S$, does there exist open paths to zero?
\end{proof}

\section{Percolation in Two Dimensions}
\label{sec:perc-two-dimens}

There are two models, bond percolation on $L^{2}$, and site
percolation on $\Pi$, the triangular lattice.

The triangular lattice is ``self-matching'', in that the dual
construction is  on the same lattice as the primal (c.f. the dual of
the square lattice).

\subsection{Bond percolation on $\Z^2$, Site Percolation on $\mathbb{T}$}
\label{sec:bond-percolation-z2}

\begin{thm}
  \label{defn:random_walks_on_graphs:26}
  For bond percolation on $\Z^{2}$, $\Theta(\frac{1}{2}) = 0$.
\end{thm}

\begin{proof}[Proof of Zhang]
  Let $p = \frac{1}{2}$ and suppose $\Theta(\frac{1}{2}) > 0$.  Since
  $\Theta(\frac{1}{2}) > 0$, then the probability there exists an
  infinite open cluster is one.

  Let $T_{n} = [0, n]^{2}$.  As $n$ goes to infinity, then the
  probability that $T_{n}$ intersects with the infinite open cluster
  tends to one. Thus, find $N$ such that for all $n > N$,
  $\Prob{\text{$T_n$ intersects the infinite open cluster}}$ is
  greater than $1 - \frac{1}{8}^4$.

  Consider $A^{t}$ be the event that the \textbf{top} of $T_{n}$ is joined to
  the infinite open cluster.  Define $A^{b}, A^{l}, A^{r}$ to be the
  \textbf{bottom}, \textbf{left}, and \textbf{right} analogues.  Then
  $\Prob{\text{$T_{n}$ does not intersects the infinite cluster}}$ is
  $\Prob{\overline{A^{t}} \cap \overline{A^{b}} \cap \overline{A^{l}} \cap
    \overline{A^{r}}} \geq \Prob{\overline{A^{u}}}^{4}$ for $u = t, b,
  l, r$.

  Then we have $\Prob{A^{u}} \geq \frac{7}{8}$ by the given result.

  Let $n = N + 1$.  Pass to the dual percolation, ...

  \todo{Fill this in from the Probability on Graphs book.  Doesn't
    look _too_ difficult.}
\end{proof}

\subsection{Site percolation on $\Pi$}
\label{sec:site-percolation-pi}

$\Pi$ has the vertex set $\{ m \tilde i + n \tilde j: m, n \in \Z\}$,
$\tilde i = (1, 0)$, $\tilde j = \frac{1}{2}(1, \sqrt{3})$ when
embedded into $\R^{2}$.

Now, consider a box in $\R^{2}$, with vertices $(0, 0)$ and $(a, b)$
with $a \in \N, b \in \sqrt{3}{2} \N$.

Each site is black with probability $\frac{1}{2}$, and white
otherwise.  Let $H_{a, b} = \{ L \leftrightarrow^{black} R \in R_{a,
  b}$ where $L$ is the left edge and $R$ is the right edge.  That is,
$H_{a, b}$ is the event that there exitss a black path that traverses
$R_{a, b}$ from $L(R_{a, b})$ to $R(R_{a, b})$.

Then we have the lemma as follows:

\begin{lem}[RSW Lemma]
  \begin{align}
    \label{eq:40}
    \Prob{H_{2a, b}} \geq \frac{1}{4} \Prob{H_{a, b}}^{2}.
  \end{align}
\end{lem}

\todo{Proof on p100-105 in book}

\begin{thm}
  \label{defn:random_walks_on_graphs:25}
  $p_{c}(\text{bond}, \mathbb{T}) = \frac{1}{2}$
\end{thm}

\begin{thm}
  \label{defn:random_walks_on_graphs:27}
  $p_{c} \geq \frac{1}{2}$, and in fact $\Theta(\frac{1}{2}) = 0$ for
  the bond model on $\Z^{2}$.
\end{thm}

\newmathcommand{\rad}{Rad}

\begin{proof}
  Following p 122 of the book.
  
  We need to prove that $p_{c} \leq \frac{1}{2}$ - that is $\Theta(p)
  > 0$ for $p > \frac{1}{2}$.


  Let $H_{n} = H_{16 n, n \sqrt{3}}$ be the event that a black
  crossing of $R_{16n, n \sqrt{3}}$ exists. By the previous lemma,s
  there exists $\tau > 0$ such that $\Prob_{\frac{1}{2}}(H_{n}) \geq
  \tau$ for some $\tau > 0$. Let $\frac{1}{2} \leq p \leq
  \frac{3}{4}$.

  Then
  \begin{equation}
    \label{eq:43}
    (1-p) I_{n, p}(x) \leq \Prob_{1-p}(\rad(C_{x}) \geq n)
  \end{equation}
  where $\rad(C_{x}) = \max \{ |y-x| : x \leftrightarrow y \}$.
\end{proof}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "master"
%%% End: 
