
\section{Estimation of ARMA Processes}
\label{sec:estim-arma-proc}

\subsection{Yule-Walker Equations}
\label{sec:yule-walk-equat}

Consider estimating a causal AR($p$) process.  We can write
\begin{equation}
  \label{eq:27}
  X_{t} = \sum_{j=0}^{\infty} \psi_{j} Z_{t-j}
\end{equation} where $\sum_{j=0}^{\infty} \psi_{j} z^{j} =
\frac{1}{\phi(z)}$ for $z \leq 1$.

Multiplying each side by $Z_{t-j}$, and taking expectations, we obtain
the Yule-Walker equations
\begin{equation}
  \label{eq:34}
  \Gamma_{p} \mathbf{\phi} = \mathbf{\gamma}_{p}
\end{equation} and $\sigma^{2} = \gamma(0) - \IP{\mathbf{\phi},
  \mathbf{\gamma}_{p}}$ where $\Gamma_{p} = [\gamma(i -j)]^{p}_{i, j =
1}$  and $\mathbf{\gamma}_{p} = (\gamma(1), \gamma(2), \dots,
\gamma(p))$.

If we replace the covariances by the sample covariances $\hat
\gamma(j)$, we obtain a set of equations for the so-called Yule-Walker
estimators $\hat{\mathbf{\phi}}$ and $\hat \sigma^{2}$, given by
\begin{equation}
  \label{eq:43}
  \hat \Gamma_{p} \hat{\mathbf{\phi}} = \hat{\mathbf{\gamma}}_{p}
\end{equation}
and $\hat \sigma^{2} = \hat \gamma(0) - \IP{\hat{\mathbf{\phi}}, \hat{\mathbf{\gamma}}_{p}}$

\begin{thm}
  \label{defn:estimation_arma:1}
  If $X_{t}$ is the causal AR($p$) process and $\hat{\mathbf{\phi}}$ is
  the Yule-Walker estimator of $\mathbf{\phi}$, then
  \begin{equation}
    \label{eq:44}
    n^{\frac{1}{2}}(\hat{\mathbf{\phi}} - \mathbf{\phi}) \cd N(0,
    \sigma^{2} \Gamma_{p}^{-1})
  \end{equation}

  Moreover, $\hat \sigma^{2} \cp \sigma^{2}$.
\end{thm}

\begin{thm}
  \label{defn:estimation_arma:2}
  If $X_{t}$ is a causal AR($p$) process and $\hat{\mathbf{\phi}}_{m}$
  is the Yule-Walker estimate of order $m > p$, then
  \begin{equation}
    \label{eq:45}
    n^{\frac{1}{2}}(\hat{\mathbf{\phi}}_{m} - \mathbf{\phi}_{m}) \cd
    N(0, \sigma^{2} \Gamma_{m}^{-1})
  \end{equation} where $\hat{\mathbf{\phi}}_{m}$ is  the coefficient
  vector of the best linear predictor $\IP{\mathbf{\phi}_{m}, \mathbf{X}_{m}}$
  of $X_{m+1}$ based on $X_{m}, \dots, X_{1}$.  So $\mathbf{\phi}_{m}
  = R^{-1}_{m} \mathbf{\rho}_{m}$.  In particular, for $m > p$,
  \begin{equation}
    \label{eq:46}
    n^{\frac{1}{2}} \hat \phi_{mm} \cd N(0, 1)
  \end{equation}
\end{thm}

\begin{thm}[Durbin-Levinson Algorithm for AR models]
  \label{defn:estimation_arma:3}
  Consider fitting an AR($m$) process
  \begin{equation}
    \label{eq:49}
    X_{t} - \hat \theta_{m1} X_{t-1} - \cdots - \hat \theta_{mm}
    X_{t-m} = Z_{t}
  \end{equation} with $Z_{t} \sim WN(0, \hat v_{m})$.
  
  If $\hat \gamma(0) > 0$, then the fitted autoregressive models for
  $m = 1, 2, \dots, n - 1$ can be determined recursively from the
  relations
  \begin{align}
    \label{eq:48}
    \hat \phi_{11} = \hat \rho(1) \\
    \hat v_{1} = \hat \gamma(0) (1 - \hat \rho^{2})(1) \\
    \hat \phi_{mm} = \frac{\hat \gamma(m) - \sum_{j=1}^{m-1} \hat
      \phi_{m-1, j} \hat \gamma(m - j)}{\hat v_{m-1}} \\
    \begin{Bmatrix}
      \hat \phi_{m1} \\
      \vdots \\
      \hat \phi_{m, m - 1}
    \end{Bmatrix}
    = \hat{\mathbf{\phi}}_{m-1} - \hat \phi_{mm}
    \begin{Bmatrix}
      \hat \phi_{m-1, m-1} \\
      \vdots\ \\
      \hat \phi_{m-1, 1}
    \end{Bmatrix} \\
    \hat v_{m} = \hat v_{m-1}(1 - \hat \phi^{2}_{mm})
  \end{align}
\end{thm}


\begin{thm}[Confidence intervals for AR($p$) estimation]
  \label{defn:estimation_arma:4}
  Under the assumption that the order $p$ of the fitted model is the
  correct value, for large sample-size $n$, the region
  \begin{equation}
    \label{eq:50}
    \{ \mathbf{\phi} \in \R^{p} | (\mathbf{\phi} - \hat \phi_{p})'
    \hat \Gamma_{p} (\mathbf{\phi} - \hat{\mathbf{\phi}}_{p}) \leq
    \frac{1}{n} \hat v_{p} \chi^{2}_{1 - \alpha}(p) \}
  \end{equation} contains $\mathbf{\phi}_{p}$ with probability close
  to $1 - \alpha$ where $\chi^{2}_{1-\alpha}(p)$ is the $(1-\alpha)$
  quantile of the chi-squared distribution with $p$ degrees of
  freedom.

  Similarly, if $\Phi_{1-\alpha}$ is the $(1-\alpha)$ quantile of the
  standard normal distribution and $\hat v_{jj}$ is the $j$-th
  diagonal element of $\hat v_{p} \hat \Gamma_{p}^{-1}$, then for
  large $n$
  \begin{equation}
    \label{eq:51}
    \{ \hat \phi_{pj} \pm \Phi_{1-\frac{\alpha}{2}}
    \frac{1}{n^{\frac{1}{2}}} \hat v_{jj}^{\frac{1}{2}} \} 
  \end{equation} contains $\phi_{pj}$ with probability close to $(1 - \alpha)$.
\end{thm}


\subsection{Estimation for Moving Average Processes Using the Innovations Algorithm}
\label{sec:estim-moving-aver}

Consider estimating
\begin{equation}
  \label{eq:52}
  X_{t} = Z_{t} + \hat \theta_{m1} Z_{t-1} + \cdots + \hat \theta_{mm} Z_{t-m}
\end{equation} with $Z_{t} \sim WN(0, \hat v_{m})$.

\begin{thm}
  \label{defn:estimation_arma:5}
  We can apply the innovation estimates by applying the recursive
  relations
  \begin{align}
    \label{eq:53}
    \hat v_{0} = \hat \gamma(0) \\
    \hat \theta_{m, m - k} = \frac{1}{\hat v_{k}}( \hat \gamma(m-k) -
    \sum_{j=0}^{k-1} \hat \theta_{m, m - j} \hat \theta_{k, k - j}
    \hat v_{j})
  \end{align} for $k = 0, \dots, m - 1$, and
  \begin{equation}
    \label{eq:54}
    \hat v_{m} = \hat \gamma(0) - \sum_{j=0}^{m-1} \hat \theta^{2}_{m,
      m - j} \hat v_{j}.
  \end{equation}
\end{thm}

\begin{thm}
  \label{defn:estimation_arma:6}
  Let $X_{t}$ be the causal invertible ARMA process $\phi(B) X_{t} =
  \theta(B) Z_{t}$ with $Z_{t} \sim WN(0, \sigma^{2})$, $\E{Z_{t}^{4}}
  < \infty$, and let $\psi(z) = \sum_{j=0}^{\infty} \psi_{j} z^{j} =
  \frac{\theta(z)}{\phi(z)}$ for $|z| \leq 1$, and $\psi_{0} = 1$ and
  $\psi_{j} = 0$ for $j < 0$.

  Then for any sequence of positive integers $m_{n}$, such that $m <
  n$, $m \rightarrow \infty$, and $m = o(n^{\frac{1}{3}})$ as $n
  \rightarrow \infty$, we have for each $k$,
  \begin{align}
    \label{eq:55}
    \frac{n^{\frac{1}{2}}}(\hat \theta_{m1} - \psi_{1}, \dots, \hat
    \theta_{mk} - \psi_{k}) \cd N(0, A)
  \end{align} where $A = [a_{ij}]_{i, j = 1}^{k}$ and
  \begin{equation}
    \label{eq:56}
    a_{ij} = \sum_{r=1}^{\min(i, j)} \psi_{i - r} \psi_{j - r}
  \end{equation} and
  \begin{equation}
    \label{eq:57}
    \hat v_{m} \cp \sigma^{2}.
  \end{equation}
\end{thm}

\begin{remark}
  Note that for the AR($p$) process, the Yule-Walker estimator is a
  consistent estimator of $\mathbf{\phi}_{p}$. However, for an MA($q$)
  process, the estimator $\hat{\mathbf{\theta}}_{q}$ is not consistent
  for the true parameter vector as $n \rightarrow \infty$.  For
  consistency, it is necessary to use the estimators with $m$
  satisfying the conditions given in Theorem
  \ref{defn:estimation_arma:6}.
\end{remark}

\begin{thm}[Asymptotic confidence regions for the $\mathbf{\theta}_{q}$]
  \label{defn:estimation_arma:7}
  \begin{equation}
    \label{eq:58}
    \{ \theta \in R | |\theta - \hat \theta_{mj}| \leq \Phi_{1 -
      \frac{\alpha}{2}} \frac{1}{n^{\frac{1}{2}}}
    (\sum_{k=0}^{j-1}\hat \theta_{mk}^{2})^{\frac{1}{2}} \} 
  \end{equation} is an $(1 - \alpha)$ confidence interval for $\theta_{mj}$.
\end{thm}

\subsection{Maximum Likelihood Estimation}
\label{sec:maxim-likel-estim}

Consider $X_{t}$ a gaussian time series with zero mean and
autocovariance function $\kappa(i, j) = \E{X_{i} X_{j}}$.  Let $\hat
X_{j} = P_{j-1} X_{j}$.  Let $\Gamma_{n}$ be the covariance matrix and
assume $\Gamma_{n}$ is nonsingular.  The likelihood of $X_{n}$ is
\begin{equation}
  \label{eq:59}
  L(\Gamma_{n}) = \frac{1}{(2 \pi)^{\frac{n}{2}}} \frac{1}{(\det
    \Gamma_{n})^{\frac{1}{2}}} \exp(-\frac{1}{2} \mathbf{X}'_{n}
  \Gamma_{n}^{-1} \mathbf{X}_{n})
\end{equation}

\begin{thm}
  \label{defn:estimation_arma:8}
  The likelihood of the vector $\mathbf{X}_{n}$ reduces to
  \begin{equation}
    \label{eq:60}
    L(\Gamma_{n}) = \frac{1}{\sqrt{(2\pi)^{n} \prod_{i=0}^{n-1}
        r_{i}}} \exp(-\frac{1}{2} \sum_{j=1}^{n} \frac{(X_{j} - \hat X_{j})^{2}}{r_{j-1}})
  \end{equation}
\end{thm}

\begin{remark}
  Even if $X_{t}$ is not Gaussian, the large sample estimates are the
  same for $Z_{t} \sim IID(0, \sigma^{2})$, regardless of whether or
  not $Z_{t}$ is Gaussian.
\end{remark}

\begin{thm}[Maximum Likelihood Estimators for ARMA processes]
  \label{defn:estimation_arma:9}
  \begin{align}
    \label{eq:61}
    \hat \sigma^{2} = \frac{1}{n} S(\hat{\mathbf{\phi}}, \hat{\mathbf{\theta}}) 
  \end{align} where $\hat{\mathbf{\phi}}, \hat{\mathbf{\theta}}$ are the
  values of $\mathbf{\phi}, \mathbf{\theta}$ that minimize
  \begin{align}
    \label{eq:62}
    \ell(\mathbf{\phi}, \mathbf{\theta}) = \ln (\frac{1}{n}
    S(\mathbf{\theta}, \mathbf{\theta})) + \frac{1}{n} \sum_{j=0}^{n-1}
    \ln r_{j}
  \end{align} and
  \begin{align}
    \label{eq:63}
    S(\hat{\mathbf{\phi}}, \hat{\mathbf{\theta}}) = \sum_{j=1}^{n}
    \frac{(X_{j} - \hat X_{j})^{2}}{r_{j-1}}
  \end{align}
\end{thm}

\begin{thm}[Asyptotic Distribution of Maximum Likelihood Estimators]
  \label{defn:estimation_arma:10}
  For a large sample from an ARMA($p, q$) process,
  \begin{align}
    \label{eq:64}
    \hat{\mathbf{\beta}} = N(\mathbf{\beta}, \frac{1}{n}V{\mathbf{\beta}})
  \end{align}
  where
  \begin{align}
    \label{eq:65}
    V(\mathbf{\beta}) = \sigma^{2}
    \begin{bmatrix}
      \E{U_{t} U'_{t}} & \E{U_{t} V'_{t}} \\
      \E{V_{t} U'_{t}} & \E{V_{t} V'_{t}}
    \end{bmatrix}^{-1}
  \end{align}
  and $U_{t}$ are the autoregressive process $\phi(B) U_{t} = Z_{t}$
  and $\theta(B) V_{t} = Z_{t}$.

  Note that for $p = 0$, $V(\mathbf{\beta}) = \sigma^{2} [\E{V_{t}
    V'_{t}}]^{-1}$, and for $q = 0$, $V(\mathbf{\beta}) = \sigma^{2}
  [\E{U_{t} U'_{t}}]^{-1}$.
\end{thm}

\subsection{Order Selection}
\label{sec:order-selection}

\begin{defn}[Kullback-Leibler divergence]
  \label{defn:estimation_arma:11}
  The Kullback-Leibler (KL) divergence between $f(\cdot; \psi)$ and
  $f(\cdot; \theta)$ is defined as
  \begin{equation}
    \label{eq:66}
    d(\psi | \theta) = \Delta (\psi|\theta) - \Delta(\theta | \theta)
  \end{equation} where
  \begin{align}
    \label{eq:67}
    \Delta(\psi | \theta) = \E{-2 \ln f(X; \psi)}{\theta}
  \end{align} is the Kullback-Leibler index of $f(\cdot; \psi)$
  relative to $f(\cdot; \theta)$.
\end{defn}

\begin{thm}[AICC of ARMA($p, q$) process]
  \label{defn:estimation_arma:12}
  \begin{align}
    \label{eq:68}
    AICC(\mathbf{\beta}) = -2 \ln L_{X}(\mathbf{\beta},
    \frac{S_{X}(\beta)}{n}) + \frac{2(p+q+1)n}{n - p - q - 2}
  \end{align}
\end{thm}

\begin{thm}[AIC of ARMA($p, q$) process]
  \label{defn:estimation_arma:13}
  \begin{align}
    \label{eq:69}
    AIC(\mathbf{\beta}) = -2 \ln L_{X}(\mathbf{\beta}, \frac{S_{X}(\beta)}{n}) + 2(p + q + 1)
  \end{align}
\end{thm}

\begin{thm}[BIC of ARMA($p, q$) process]
  \label{defn:estimation_arma:13}
  \begin{align}
    \label{eq:69}
    BIC(\mathbf{\beta}) = (n - p - q) \ln \frac{n \hat
      \sigma^{2}}{n-p-q} + n(1 + \ln \sqrt{2 \pi}) + (p + q) \ln
    \frac{\sum_{t=1}^{n} X_{t}^{2} - n \hat \sigma^{2}}{p + q}
  \end{align}
  where $\hat \sigma^{2}$ is the MLE estimate of the white noise
  variance.
\end{thm}



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "master"
%%% End: 
