
\chapter{Monte Carlo Inference}
\label{cha:monte-carlo-infer}

\begin{exer}
  What are the expected number of samples obtained from running the
rejection sampling algorithm $N$ times?
\end{exer}


\todo{Bunch of content, straight from the lecture notes}

\section{Quantile Approximation by Monte Carlo Methods}
\label{sec:quant-appr-monte}


\begin{thm}
  \label{defn:monte_carlo:1}
  If $F$ is continuous and strictly increasing, then $\hat c_{\alpha}
  \cas c_{\alpha}$
\end{thm}

\begin{proof}
  Let $\epsilon > 0$.

  Then $\hat F_{B}(c_{\alpha - \epsilon}) = \frac{1}{B}
  \sum_{i=1}^{B} \I{X_{i} \leq c_{\alpha - \epsilon}}$.

  Thus, by the strong law of lare numbers, $\hat F_{B}(c_{\alpha -
    \epsilon}) \cas \alpha - \epsilon$.

  In the same way, $\hat F_{B}(c_{\alpha + \epsilon}) \cas \alpha +
  \epsilon$.

  Thus, for all $\delta > 0$, there exists $\Omega_{1}$ with $P(\Omega)
  \geq 1 - \epsilon$, such that there exists $N_{1}$ with $\forall B > N_{1}$,
  on $\Omega_{1}$,
  \begin{equation}
    |\hat F_{B}(c_{\alpha - \epsilon}) - (\alpha - \epsilon)| \leq \frac{\epsilon}{4} 
  \end{equation}
  and similarly on $\Omega_{2}$,
  \begin{equation}
    |\hat F_{B}(c_{\alpha + \epsilon}) - (\alpha + \epsilon)| \leq \frac{\epsilon}{4} 
  \end{equation}

  On $\Omega_{1} \cap \Omega_{2}$, and for $B > \max \{N_{1}, N_{2}
  \}$,
  \begin{align}
    \label{eq:71}
    \hat F_{B}(c_{\alpha - \epsilon}) \leq \alpha -
    \frac{3\epsilon}{4}
    \hat F_{B}(c_{\alpha + \epsilon}) \geq \alpha + \frac{3\epsilon}{4}
  \end{align}

  We now that by definition, $\hat F_{B}(\hat c_{\alpha}) = \alpha$.

  On $\Omega_{1} \cap \Omega_{2}$,
  \begin{equation}
    \label{eq:71}
    \hat F_{B}(c_{\alpha - \epsilon}) \leq \hat F_{B}(\hat c_{\alpha})
  \end{equation}

  Since $\hat F$ is increasing, $c_{\alpha - \epsilon} \leq \hat
  c_{\alpha} \leq c_{\alpha + \epsilon}$.  Since $F$ is continuous,
  for $\epsilon$ small enough, $|c_{\alpha - \epsilon} - c_{\alpha +
    \epsilon}|$ can be made arbitrarily small.

  Also, $P(\Omega_{1} \cap \Omega_{2}) \geq 1 - 2 \delta$.  So we
  proved that for any $\epsilon_{1} > 0$, there exists $\Omega_{3} =
  \Omega_{1} \cap \Omega_{2}$ such that $P(\Omega_{3}) \geq 1 - 2
  \delta$, and there exists $N_{3} = \max \{ \Omega_{1}, \Omega_{2}
  \}$ such that for all $B \geq n_{3}$,
  \begin{equation}
    \label{eq:71}
    |\hat c_{\alpha} - c_{\alpha}| \leq \epsilon_{1}.
  \end{equation}
  Thus $\hat c_{\alpha} \cas c_{\alpha}$.
\end{proof}

\section{Markov Chain Monte Carlo and Bayesian Inference}
\label{sec:markov-chain-monte}

Given $X_{1}, \dots, X_{n} \iid f$, we want tof find $f$.  Choose a
model class $\mathcal{M} = \{ f_{\theta} \}$.  Assume that there
exists $\theta$ with $f = f_{\theta}$, and seek to find $\theta$.

In a frequentist approach, we have $L(X_{1}, \dots, X_{n}, \theta) =
\prod_{i=1}^{n} f_{\theta}(X_{i})$.

In a Bayesian framework, we have $\theta \tilde p$,
$P(X_{1}, \dots, X_{n}, \theta) = p(\theta) L(X_{1}, \dots, X_{n})$,

\begin{align}
  \label{eq:71}
  \pi(\theta | X_{1}, \dots, X_{n}) = \frac{p(\theta) L(X_{1}, \dots,
    X_{n}, \theta)}{\int p(\theta') L(X_{1}, \dots, X_{n} \theta') d\theta'}
\end{align}

A Bayesian is interested in $\pi$ - in practice, $\pi$ can be
difficult to study, so MC mehtods to approximate quantity related to
$\pi$.


\subsection{Probit Model}
\label{sec:probit-model}

Consider $(X, Y) \in R^{d \times m} \times \R^{m}$, with $y_{i} \in \{
0, 1 \}$.So $Z = X^{T} \theta + \epsilon$, with $\epsilon \sim N(0,
I_{m})$,$Y = \I{Z \geq 0}$.  Alterantively, we can write $Y =
B(F(X_{i}^{T} \theta))$, where $B$ is the Bernouli distribution and
$F$ is the distribution function of the normal distribution.  

Note that we have $L((X, Y), \theta) = \prod_{i=1}^{n} (1-F(X_{i}^{T}
\theta))^{1-Y_{i}} F(X_{i}^{T} \theta)^{Y_{i}}$

The prior placed on $\theta$ is $N(0, I_{d})$.  Thus
\begin{align}
  \label{eq:71}
  \pi(\theta) \propto e^{-\frac{1}{2} \theta^{T} \theta}
  \prod_{i=1}^{n} (1-F(X_{i}^{T} \theta))^{1-Y_{i}} F(X_{i}^{T}
  \theta)^{Y_{i}}
\end{align}

\subsection{Markov Chains}
\label{sec:markov-chains}

Consider the case of a stationary Markov chain - $X_{1}, \dots,
X_{n}$is a chain of data, and the Markov property is that $X_{m+1} |
X_{m}= x_{m} \perp X_{1}, \dots, X_{m-1}$, and $X_{m+1} | X_{m} =
x_{m}$ has distribution $F$, and so is stationary.

Assume $X_{n} \in \{ 1, \dots, n \}$.  Then you can model the chain by
a graph, with transition probabilities $\pi_{ij}$.  Then letting $K =
(\pi_{ij})$, we have that a stationary distribution $\pi$ satisfies
$\pi = K \pi$, and the distribution at time $\pi_{t}$ given $\pi_{0}$
is $K^{t} \pi_{0}$.

ANOTHER CASE...

On this chain, when $m \rightarrow \infty$, $p_{m} \cid p^{\star}$,
statinoary distribution.  In this case, $p^{star}$ is the unique
solution of $Kp = p$.

Note that MCMC techniques allow you to simulate samples which are a
Markov chain and whose stationary distribution is $\pi$.


Let $Y_{1}, \dots, Y_{n}$ be a Markov chain whose stationray
distribution is $\pi$.  Then $\frac{1}{m} \sum_{i=1}^{m} Y_{i} \cas
\E{\theta} \pi$.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "master"
%%% End: 
