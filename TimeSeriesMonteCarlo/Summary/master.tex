\input{../../common/summary.tex}

\title{Time Series and Monte-Carlo Summary}

\begin{document}

\maketitle

\begin{defn}[Mean, Covariance function]
  \label{defn:time_series:7}
  Define the mean function $\mu_{X}(t) = \E{X_{t}}$.

  Define the covariance function $\gamma_{X}(t, s) = \Cov{X_{t}}{X_{s}}
    = \E{(X_{t} - \mu_{X}(t))(X_{s} - \mu_{X}(s))}$.
\end{defn}

\begin{defn}[Weak Stationarity]
  \label{defn:time_series:6}
  A time series $X_{t}$ is stationary if
  \begin{enumerate}
  \item $\E{|X_t|^2} < \infty$ for all $t \in \Z$
  \item $\E{X_{t}} = c$ for all $t \in \Z$
  \item $\gamma_{X}(t, s) = \gamma_{X}(t+h, s+h))$ for all $t, s, h
    \in \Z$
  \end{enumerate}
\end{defn}

\begin{defn}[Strict Stationarity]
  \label{defn:time_series:8}
  A time series $X_{t}$ is said to be strict stationary if the joint
  distributions of $X_{t_{1}, \dots, X_{t_{k}}}$ and $X_{t_{1} + h},
  \dots, X_{t_{k} + h}$ are identical for all $k$ and for all $t_{1},
  \dots, t_{k}, h \in Z$.
\end{defn}

\begin{defn}[Autocovariance function]
  \label{defn:time_series:9}
  For a stationary time series $X_{t}$, define the autocovariance
  function
  \begin{equation}
    \label{eq:2}
    \gamma_{X}(t) = \Cov{X_{t+h}}{X_{t}}.
  \end{equation}
  and the autocorrelation function
  \begin{align}
    \label{eq:3}
    \rho_{X}(h) = \frac{\gamma_{X}(h)}{\gamma_{X}(0)}.
  \end{align}
\end{defn}

\begin{lem}[Properties of the autocovariance function]
  \begin{align}
    \label{eq:13}
    \gamma(0) \geq 0 \\
    |\gamma(h)| \leq \gamma(0) \\
    \gamma(h) = \gamma(-h)
  \end{align} for all $h$.

  Note that these all hold for the autocorrelation function $\rho$,
  with the additional condition that $\rho(0) = 1$.
\end{lem}

\begin{thm}
  \label{defn:time_series:16}
  A real-valued function defined on the integers is the autocovariance
  function of a stationary time series if and only if it is even and
  nonnegative definite.
\end{thm}

\begin{defn}[Sample Autocovariance]
  \label{defn:time_series:12}
  The sample autocovariance function of $\{ x_{1}, \dots, x_{n} \}$ is
  defined by
  \begin{equation}
    \label{eq:7}
    \hat \gamma(h) = \frac{1}{n} \sum_{j=1}^{n-h} (x_{j+h} - \bar
    x)(x_{j} - \bar x), 0 \leq h < n
  \end{equation} and $\hat \gamma(h) = \hat \gamma(-h)$, $-n < h \leq
  0$.

  Note that the divisor is $n$ rather than $n-h$ since this ensures
  that the sample autocovariance matrix
  \begin{equation}
    \label{eq:8}
    \hat \Gamma_{n} = (\hat \gamma(i - j))_{i, j}
  \end{equation} is positive semidefinite.
\end{defn}

\section{Stationary Processes}
\label{sec:stationary-processes-1}

\subsection{Linear Processes}
\label{sec:linear-processes}

\begin{defn}[Wold Decomposition]
  \label{defn:stationary_processes:1}
  If $X_{t}$ is a nondeterministic stationary time series, then
  \begin{equation}
    \label{eq:9}
    X_{t} = \sum_{j=0}^{\infty} \psi_{j} Z_{t-j} + V_{t}
  \end{equation} where
  \begin{enumerate}
  \item $\psi{_0} = 1$ and $\sum_{j=0}^{\infty} \psi_{j}^{2} <
    \infty$,
  \item $Z_{t} \sim WN(0, \sigma^{2})$,
  \item $\Cov{Z_{s}}{V_{t}} = 0$ for all $s, t$,
  \item $Z_{t} = \tilde P_{t} Z_{t}$ for all $t$,
  \item $V_{t} = \tilde P_{s} V_{t}$ for all $s, t$,
  \item $V_{t}$ is deterministic.
  \end{enumerate}
  The sequences $Z_{t}, \psi_{j}, V_{t}$ are unique and can be written
  explicitly as
  \begin{align}
    \label{eq:9}
    Z_{t} = X_{t} - \tilde P_{t-1} X_{t} \\
    \psi_{j} = \frac{\E{X_{t} Z_{t-j}}}{\E{Z_{t}}^{2}} \\
    V_{t} = X_{t} - \sum_{j=0}^{\infty} \psi_{j} Z_{t-j}.
  \end{align}
\end{defn}

\begin{defn}
  \label{defn:stationary_processes:1}
  A times series $\{ X_{t} \}$ is a \textbf{linear process} if it has
  the representation
  \begin{equation}
    \label{eq:9}
    X_{t} = \sum_{j=-\infty}^{\infty} \psi_{j} Z_{t-j}
  \end{equation}
  where $Z_{t} \sim WN(0, \sigma^{2})$ and $\{ \psi_{j} \}$ is a
  sequence of constants with $\sum_{j=-\infty}^{\infty} |\psi_{j}| <
  \infty$.
\end{defn}

A linear process is called a \textbf{moving average} or MA($\infty$)
if $\psi_{j} = 0$ for all $j < 0$, so
\begin{equation}
  \label{eq:9}
  X_{t} = \sum_{j=0}^{\infty} \psi_{j} Z_{t-j}.
\end{equation}

\begin{proposition}
  Let $Y_{t}$ be a stationary time series with mean zero and
  coavariance function $\gamma_{Y}$.  If $\sum_{j=-\infty}^{\infty}
  |\psi_{j}| < \infty$, then the time series
  \begin{equation}
    \label{eq:9}
    X_{t} = \sum_{j=-\infty}^{\infty} \psi_{j} Y_{t-j} = \psi(B) Y_{t}
  \end{equation}
  is stationary with mean zero and autocovariance function
  \begin{equation}
    \gamma_{X}(h) = \sum_{j=-\infty}^{\infty}
    \sum_{k=-\infty}^{\infty} \psi_{j}\psi_{k} \gamma_{Y}(h + k - j).
  \end{equation}

  In the special case where $X_{t}$ is a linear process,
  \begin{equation}
    \gamma_{X}(h) = \sum_{j=-\infty}^{\infty} \psi_{j} \psi_{j + h} \sigma^{2}.
  \end{equation}
\end{proposition}

\subsection{Forecasting Stationary Time Series}
\label{sec:forec-stat-time}

Our goal is to find the linear combination of $1, X_{n}, X_{n-1},
\dots, X_{1}$ that forecasts $X_{n+h}$ with minimum mean squared
error.  The best linear predictor in terms of $1, X_{n}, \dots, X_{1}$
will be deonted by $P_{n} X_{n+h}$ and clearly has the form
\begin{equation}
  P_{n}X_{n+h} = a_{0} + a_{1} X_{n} + \cdots + a_{n} X_{1}.
\end{equation}

To find these equations, we solve the convex problem by setting
derivatives to zero, and obtain the result given below.

\begin{thm}[Properties of $h$-step best linear predictor $P_{n} X_{n+h}$]
  \label{defn:stationary_processes:1}
  \begin{enumerate}
  \item
    \begin{equation}
      P_{n}X_{n+h} = \mu + \sum_{i=1}^{n} a_{i} (X_{n+1-i} - \mu)
    \end{equation} where $\mathbf{a}_{n} = (a_{1}, \dots, a_{n})$ satisfies
    \begin{align}
      \Gamma_{n} \mathbf{a}_{n} = \gamma_{n}(h) \\
      \Gamma_{n} = [\gamma(i - j)]^{n}_{i,j = 1} \\
      \gamma_{n}(h) = (\gamma(h), \gamma(h+1), \dots, \gamma(h + n - 1))
    \end{align}
  \item
    \begin{equation}
      \E{(X_{n+h} - P_{n} X_{n+h})^{2}} = \gamma(0) -
      \IP{\mathbf{a}_{n}, \gamma_{n}(h)}
    \end{equation}
  \item
    \begin{equation}
      \E{X_{n+h} - P_{n} X_{n+h}} = 0
    \end{equation}
  \item
    \begin{equation}
      \E{(X_{n+h} - P_{n} X_{n+h}) X_{j}} = 0
    \end{equation} for $j = 1, \dots, n$.
  \end{enumerate}
\end{thm}

\begin{defn}[Prediction Operator $P(\cdot | \mathbf{W})$]
  \label{defn:stationary_processes:1}
  Suppose that $\E{U^{2}} < \infty$, $\E{V^{2}} < \infty$, $\Gamma =
  \Cov{\mathbf{W}}{\mathbf{W}}$, and $\beta, \alpha_{1}, \dots,
  \alpha_{n}$ are constants.

  \begin{enumerate}
  \item
    \begin{equation}
      P(U | \mathbf{W}) = \E{U} = \mathbf{a}'(\mathbf{W} - \E{\mathbf{W}})
    \end{equation}
    where $\Gamma \mathbf{a} = \Cov{U}{\mathbf{W}}$.
  \item
    \begin{equation}
      \E{(U - P(U | \mathbf{W})) \mathbf{W}} = 0
    \end{equation} and
    \begin{equation}
      \E{U - P(U | \mathbf{W})} = 0
    \end{equation}
  \item
    \begin{equation}
      \E{(U - P(U | \mathbf{W}))^{2}} = \Var{U} - \mathbf{a}' \Cov{U}{\mathbf{W}}
    \end{equation}
  \item
    \begin{equation}
      P{\alpha_{1} + \alpha_{2} V + \beta | \mathbf{W}} = \alpha_{1}
      P(U| \mathbf{W}) + \alpha_{2} P(V | \mathbf{W}) + \beta
    \end{equation}
  \item
    \begin{equation}
      P(\sum_{i=1}^{n} \alpha_{i} W_{i} + \beta | \mathbf{W}) =
      \sum_{i=1}^{n} \alpha_{i} W_{i} + \beta
    \end{equation}
  \item
    \begin{equation}
      \label{eq:15}
      P(U | \mathbf{W}) = E{U}
    \end{equation} if $\Cov{U}{\mathbf{W}} = 0$.
  \end{enumerate}
\end{defn}

\subsection{Innovation Algorithm}
\label{sec:innovation-algorithm}

\begin{thm}
  \label{defn:stationary_processes:2}
  Suppose $X_{t}$ is a zero-mean series with $\E{|X_{t}|^{2}} <
  \infty$ for each $t$ and $\E{X_{i} X_{j}} = \kappa(i, j)$.
  Let $\hat X_{n} = 0$ if $n = 1$, and $P_{n-1} X_{n}$ if $n = 2, 3,
  \dots$, and let $v_{n} = \E{(X_{n+1} - P_{n} X_{n+1})^{2}}$.

  Define the innovations, or one-step prediction errors, as $U_{n} =
  X_{n} - \hat X_{n}$.

  Then we can write
  \begin{align}
    \label{eq:16}
    \hat X_{n+1} =
    \begin{cases}
      0 & n = 0 \\
      \sum_{j=1}^{n} \theta_{nj}(X_{n+1-j} - \hat X_{n+1 - j})
    \end{cases}
  \end{align}
  where the coefficients $\theta_{n1}, \dots, \theta{nn}$ can be
  computed recursively from the equations
  \begin{align}
    \label{eq:17}
    v_{0} = \kappa(1, 1) \\
    \theta_{n, n-k} = \frac{1}{v_{k}}(\kappa(n+1, k+1) -
    \sum_{j=0}^{k-1} \theta_{k, k-j} \theta_{n, n-j} v_{j})
  \end{align}  for $0 \leq k < n$, and
  \begin{equation}
    \label{eq:18}
    v_{n} = \kappa(n+1, n+1) - \sum_{j=0}^{n-1} \theta^{2}_{n, n-j} v_{j}.
  \end{equation}
\end{thm}


\section{ARMA Processes}
\label{sec:arma-processes}

\begin{defn}
  \label{defn:arma_processes:1}
  $X_{t}$ is an ARMA($p, q$) process if $X_{t}$ is stationary and if
  for every $t$,
  \begin{equation}
    \label{eq:14}
    X_{t} - \phi_{1} X_{t-1} - \cdots - \phi_{p} X_{t-p} = Z_{t} +
    \theta_{1} Z_{t-1} + \cdots + \theta_{q} Z_{t-q}
  \end{equation} where $Z_{t} \sim WN(0, \sigma^{2})$  and the
  polynomials $(1 - \phi_{1}z - \cdots - \phi_{p} z^{p})$ and $(1 +
  \theta_{1} z + \cdots + \theta_{q} z^{q})$ have no common factors.

  It can be more convenient to write this in the form
  \begin{equation}
    \label{eq:19}
    \phi(B) X_{t} = \theta(B) Z_{t}
  \end{equation}
  with $B$ the back-shift operator.

  ARMA($0, q$) is a moving average process of order $q$ (MA($q$)).
  ARMA($p, 0$) is an autoregressive process of order $p$ (AR($p$)).
\end{defn}

\begin{thm}
  \label{defn:arma_processes:2}
  A stationary solution of \eqref{eq:14} exists (and is the unique
  stationary solution) if and only if
  \begin{equation}
    \label{eq:20}
    \phi(z) = 1 - \phi_{1} z - \cdots - \phi_{p} z^{p} \neq 0
  \end{equation} for all $|z| = 1$
\end{thm}

\begin{defn}
  \label{defn:arma_processes:3}
  An ARMA($p, q$) process $X_{t}$ is causal (or a causal function of
  $Z_{t}$) if there exists constants $\psi_{j}$ such that
  $\sum_{j=0}^{\infty} |\psi_{j}| < \infty$ and
  \begin{equation}
    \label{eq:21}
    X_{t} = \sum_{j=0}^{\infty} \psi_{j} Z_{t-j}
  \end{equation} for all $t$.
\end{defn}

\begin{thm}
  \label{defn:arma_processes:4}
  An ARMA($p, q$) process is causal if and only if
  \begin{equation}
    \label{eq:22}
    \phi(z) = 1 - \phi_{1} z - \cdots - \phi_{p} z^{p} \neq 0
  \end{equation} for all $|z| \leq 1$.

  Note that the coefficients $\psi_{j}$ are determined by
  \begin{equation}
    \label{eq:25}
    \psi_{j} - \sum_{k=1}^{p} \theta_{k} \psi_{j-k} = \theta_{j}
  \end{equation} for $j = 0, 1, \dots$.
  and $\theta_{0} = 1$, $\theta_{j} = 0$ for $j > q$, and $\psi_{j} =
  0$ for $j < 0$.
\end{thm}

\begin{defn}
  \label{defn:arma_processes:5}
  An ARMA($p, q$) is invertible if there exist constants $\pi_{j}$
  such that $\sum_{j=0}^{\infty} |\pi_{j}| < \infty$ and
  \begin{equation}
    \label{eq:23}
    Z_{t} = \sum_{j=0}^{\infty} \pi_{j} X_{t-j}
  \end{equation} for all $t$.

  The coefficients $\pi_{j}$ are determined by the equations
  \begin{equation}
    \label{eq:26}
    \pi_{j} + \sum_{k=1}^{q} \theta_{k} \pi_{j-k} = -\phi_{j}
  \end{equation}
  where $\phi_{0} = -1$, $\theta_{j} = 0$ for $j > p$, and $\pi_{j} =
  0$ for $j < 0$.
\end{defn}

\begin{thm}
  \label{defn:arma_processes:6}
  Invertibility is equivalent to the condition
  \begin{equation}
    \label{eq:24}
    \theta(z) = 1 + \theta_{1}z + \cdots + \theta_{q} z^{q} \neq 0
  \end{equation} for all $|z| \leq 1$.
\end{thm}

\subsection{ACF and PACF of an ARMA($p, q$) Process}
\label{sec:acf-pacf-an}

\begin{thm}
  \label{defn:arma_processes:7}
  For a causal ARMA($p, q$) process defined by
  \begin{equation}
    \label{eq:28}
    \phi(B) X_{t} = \theta(B) Z_{t}
  \end{equation} we know we can write
  \begin{equation}
    \label{eq:29}
    X_{t} = \sum_{j=0}^{\infty} \psi_{j} Z_{t-j}
  \end{equation} where $\sum_{j=0}^{\infty} \psi_{j} z^{j} = \theta(z)
  / \phi(z)$ for $|z| \leq 1$.

  Thus, the ACVF $\gamma$ is given as
  \begin{equation}
    \label{eq:30}
    \gamma(h) = \E{X_{t+h} X_{t}} = \sigma^{2} \sum_{j=0}^{\infty}
    \psi_{j} \psi_{j+|h|}
  \end{equation}

  A second approach is to multiple each side by $X_{t_k}$ and take
  expectations, and obtain a sequence of $m$ homogenous linear
  difference equations with constant coefficients.  These can be
  solved to obtain the $\gamma(h)$ values.
\end{thm}

\begin{defn}[PACF]
  \label{defn:arma_processes:8}
  The partial autocorrelation function (PACF) of an AMRA process $X$
  is the function $\alpha(\cdot)$ defined by
  \begin{align}
    \label{eq:31}
    \alpha(0) = 1 \\
    \alpha(h) = \phi_{hh}, h \geq 1
  \end{align} where $\phi_{hh}$ is the last component of
  $\mathbf{\phi}_{h} = \Gamma_{h}^{-1} \gamma_{h}$, where $\Gamma_{h}
  = [\gamma(i - j)]^{h}_{i,j = 1}$, and $\gamma_{h} = [\gamma(1),
  \gamma(2), \dots, \gamma(h)]$.
\end{defn}

\begin{thm}
  \label{defn:arma_processes:9}
  For an AR($p$) process, the sample PACF values at lags greater than
  $p$ are approximately independent $N(0, \frac{1}{n})$ random
  variables.  Thus, if we have a sample PACF satisfying
  \begin{equation}
    \label{eq:32}
    |\hat \alpha(h)| > \frac{1.96}{\sqrt{n}}
  \end{equation} for $0 \leq h \leq p$ and
  \begin{equation}
    \label{eq:33}
    |\hat \alpha(h)| < \frac{1.96}{\sqrt{n}}
  \end{equation} for $h > p$, this suggests an AR($p$) model for the data.
\end{thm}

\begin{thm}[PACF summary]
  \label{defn:arma_processes:10}
  For an AR($p$) process $X_{t}$, the PACF $\alpha(\cdot)$ has the
  properties that $\alpha(p) = \phi_{p}$, and $\alpha(h) = 0$ for $h >
  p$.  For $h < p$ we can compute numerically from the expression that
  $\mathbf{\phi}_{h} = \Gamma^{-1}_{h} \mathbf{\gamma}_{h}$.
\end{thm}

\subsection{Forecasting ARMA Processes}
\label{sec:forec-arma-proc}

For the causal ARMA($p, q$) process
\begin{equation}
  \label{eq:35}
  \phi(B) X_{t} = \theta(B) Z_{t}, Z_{t} \sim WN(0, \sigma^{2})
\end{equation} we can avoid using the full innovations algorithm.

If we apply the algorithm to the transformed process $W_{t}$ given by
\begin{equation}
  \label{eq:36}
  W_{t} =
  \begin{cases}
    \frac{1}{\sigma} X_{t} & t = 1, \dots, m \\
    \frac{1}{\sigma} \phi(B) X_{t} & t > m
  \end{cases}
\end{equation} where $m = \max(p, q)$.

For notational convenience, take $\theta_{0} = 1$, $\theta_{j} = 0 $
for $j > q$.

\begin{lem}
  The autocovariances $\kappa(i, j) = \E{W_{i} W_{j}}$ are found from
  \begin{equation}
    \label{eq:37}
    \kappa(i, j) =
    \begin{cases}
      \sigma^{2} \gamma_{X}(i - j) & 1 \leq i, j \leq m \\
      \sigma^{2} (\gamma_{X}(i - j) - \sum_{r=1}^{p} \phi_{r}
      \gamma_{X}(r - |i-j|)) & \min(i, j) \leq m < \max(i, j) \leq 2m
    \\
    \sum_{r=0}^{q}\theta_{r} \theta_{r + |i-j|} & \min(i, j) > m \\
    0 & \text{otherwise}
    \end{cases}
  \end{equation}

  Applying the innovations algorithm to the process $W_{t}$, we obtain
  \begin{equation}
    \label{eq:38}
    \hat W_{n+1} =
    \begin{cases}
      \sum_{j=1}^{n} \theta_{nj} (W_{n+1-j} - \hat W_{n+1-j}) & 1
      \leq n < m \\
      \sum_{j=1}^{q} \theta_{nj} (W_{n+1-j} - \hat W_{n+1-j}) & n \geq m
    \end{cases}
  \end{equation} where the coefficients $\theta_{nj}$ and MSE $r_{n} =
  \E{(W_{n+1} - \hat W_{n+1})^{2}}$ are found recursively using the
  innovations algorithm.

  Since the equations~\eqref{eq:36} allow us to write $X_{n}$ as a
  linear combination of $W_{j}, 1 \leq j \leq n$, and conversely, each
  $W_{n}, n \geq 1$ to be written as a linear combination of $X_{j}, 1
  \leq j \leq n$.  Thus the best linear predictor of the random
  variable $Y$  in terms of $\{ 1, X_{1}, \dots, X_{n} \}$ is the same
  as the best linear predictor of $Y$ in terms of $\{ 1, W_{1}, \dots,
  W_{n}$.  Thus, by linearity of $\hat P_{n}$, we have
  \begin{align}
    \label{eq:39}
    \hat W_{t} =
    \begin{cases}
      \frac{1}{\sigma} \hat X_{t} & t = 1, \dots, m \\
      \frac{1}{\sigma}(\hat X_{t} - \phi_{1} X_{t-1} - \cdots -
      \phi_{p} X_{t-p}) & t > m
    \end{cases}
  \end{align}
  which shows that
  \begin{equation}
    \label{eq:40}
    X_{t} - \hat X_{t} = \sigma(W_{t} - \hat W_{t})
  \end{equation}

  Substituting into \eqref{eq:37} and \eqref{eq:38}, we obtain
  \begin{equation}
    \label{eq:41}
    \hat X_{n+1} =
    \begin{cases}
      \sum_{j=1}^{n} \theta_{nj}(X_{n+1-j} - \hat X_{n+1-j}) & 1 \leq n < m \\
      \phi_{1} X_{n} + \cdots + \phi_{p} X_{n+1-p} + \sum_{j=1}^{q}
      \theta_{nj}(X_{n+1-j} - \hat X_{n+1-j}) & n \geq m
    \end{cases}
  \end{equation}
  and
  \begin{equation}
    \label{eq:42}
    \E{(X_{n+1} - \hat X_{n+1})^{2}} = \sigma^{2} \E{(W_{n+1} - \hat
      W_{n+1})^{2}} = \sigma^{2} r_{n}
  \end{equation}
  where $\theta_{nj}$ and $r_{n}$ are found using the innovation algorithm.
\end{lem}


\section{Estimation of ARMA Processes}
\label{sec:estim-arma-proc}

\subsection{Yule-Walker Equations}
\label{sec:yule-walk-equat}

Consider estimating a causal AR($p$) process.  We can write
\begin{equation}
  \label{eq:27}
  X_{t} = \sum_{j=0}^{\infty} \psi_{j} Z_{t-j}
\end{equation} where $\sum_{j=0}^{\infty} \psi_{j} z^{j} =
\frac{1}{\phi(z)}$ for $z \leq 1$.

Multiplying each side by $Z_{t-j}$, and taking expectations, we obtain
the Yule-Walker equations
\begin{equation}
  \label{eq:34}
  \Gamma_{p} \mathbf{\phi} = \mathbf{\gamma}_{p}
\end{equation} and $\sigma^{2} = \gamma(0) - \IP{\mathbf{\phi},
  \mathbf{\gamma}_{p}}$ where $\Gamma_{p} = [\gamma(i -j)]^{p}_{i, j =
1}$  and $\mathbf{\gamma}_{p} = (\gamma(1), \gamma(2), \dots,
\gamma(p))$.

If we replace the covariances by the sample covariances $\hat
\gamma(j)$, we obtain a set of equations for the so-called Yule-Walker
estimators $\hat{\mathbf{\phi}}$ and $\hat \sigma^{2}$, given by
\begin{equation}
  \label{eq:43}
  \hat \Gamma_{p} \hat{\mathbf{\phi}} = \hat{\mathbf{\gamma}}_{p}
\end{equation}
and $\hat \sigma^{2} = \hat \gamma(0) - \IP{\hat{\mathbf{\phi}}, \hat{\mathbf{\gamma}}_{p}}$

\begin{thm}
  \label{defn:estimation_arma:1}
  If $X_{t}$ is the causal AR($p$) process and $\hat{\mathbf{\phi}}$ is
  the Yule-Walker estimator of $\mathbf{\phi}$, then
  \begin{equation}
    \label{eq:44}
    n^{\frac{1}{2}}(\hat{\mathbf{\phi}} - \mathbf{\phi}) \cd N(0,
    \sigma^{2} \Gamma_{p}^{-1})
  \end{equation}

  Moreover, $\hat \sigma^{2} \cp \sigma^{2}$.
\end{thm}

\begin{thm}
  \label{defn:estimation_arma:2}
  If $X_{t}$ is a causal AR($p$) process and $\hat{\mathbf{\phi}}_{m}$
  is the Yule-Walker estimate of order $m > p$, then
  \begin{equation}
    \label{eq:45}
    n^{\frac{1}{2}}(\hat{\mathbf{\phi}}_{m} - \mathbf{\phi}_{m}) \cd
    N(0, \sigma^{2} \Gamma_{m}^{-1})
  \end{equation} where $\hat{\mathbf{\phi}}_{m}$ is  the coefficient
  vector of the best linear predictor $\IP{\mathbf{\phi}_{m}, \mathbf{X}_{m}}$
  of $X_{m+1}$ based on $X_{m}, \dots, X_{1}$.  So $\mathbf{\phi}_{m}
  = R^{-1}_{m} \mathbf{\rho}_{m}$.  In particular, for $m > p$,
  \begin{equation}
    \label{eq:46}
    n^{\frac{1}{2}} \hat \phi_{mm} \cd N(0, 1)
  \end{equation}
\end{thm}

\begin{thm}[Durbin-Levinson Algorithm for AR models]
  \label{defn:estimation_arma:3}
  Consider fitting an AR($m$) process
  \begin{equation}
    \label{eq:49}
    X_{t} - \hat \theta_{m1} X_{t-1} - \cdots - \hat \theta_{mm}
    X_{t-m} = Z_{t}
  \end{equation} with $Z_{t} \sim WN(0, \hat v_{m})$.

  If $\hat \gamma(0) > 0$, then the fitted autoregressive models for
  $m = 1, 2, \dots, n - 1$ can be determined recursively from the
  relations
  \begin{align}
    \label{eq:48}
    \hat \phi_{11} = \hat \rho(1) \\
    \hat v_{1} = \hat \gamma(0) (1 - \hat \rho^{2})(1) \\
    \hat \phi_{mm} = \frac{\hat \gamma(m) - \sum_{j=1}^{m-1} \hat
      \phi_{m-1, j} \hat \gamma(m - j)}{\hat v_{m-1}} \\
    \begin{Bmatrix}
      \hat \phi_{m1} \\
      \vdots \\
      \hat \phi_{m, m - 1}
    \end{Bmatrix}
    = \hat{\mathbf{\phi}}_{m-1} - \hat \phi_{mm}
    \begin{Bmatrix}
      \hat \phi_{m-1, m-1} \\
      \vdots\ \\
      \hat \phi_{m-1, 1}
    \end{Bmatrix} \\
    \hat v_{m} = \hat v_{m-1}(1 - \hat \phi^{2}_{mm})
  \end{align}
\end{thm}


\begin{thm}[Confidence intervals for AR($p$) estimation]
  \label{defn:estimation_arma:4}
  Under the assumption that the order $p$ of the fitted model is the
  correct value, for large sample-size $n$, the region
  \begin{equation}
    \label{eq:50}
    \{ \mathbf{\phi} \in \R^{p} | (\mathbf{\phi} - \hat \phi_{p})'
    \hat \Gamma_{p} (\mathbf{\phi} - \hat{\mathbf{\phi}}_{p}) \leq
    \frac{1}{n} \hat v_{p} \chi^{2}_{1 - \alpha}(p) \}
  \end{equation} contains $\mathbf{\phi}_{p}$ with probability close
  to $1 - \alpha$ where $\chi^{2}_{1-\alpha}(p)$ is the $(1-\alpha)$
  quantile of the chi-squared distribution with $p$ degrees of
  freedom.

  Similarly, if $\Phi_{1-\alpha}$ is the $(1-\alpha)$ quantile of the
  standard normal distribution and $\hat v_{jj}$ is the $j$-th
  diagonal element of $\hat v_{p} \hat \Gamma_{p}^{-1}$, then for
  large $n$
  \begin{equation}
    \label{eq:51}
    \{ \hat \phi_{pj} \pm \Phi_{1-\frac{\alpha}{2}}
    \frac{1}{n^{\frac{1}{2}}} \hat v_{jj}^{\frac{1}{2}} \}
  \end{equation} contains $\phi_{pj}$ with probability close to $(1 - \alpha)$.
\end{thm}


\subsection{Estimation for Moving Average Processes Using the Innovations Algorithm}
\label{sec:estim-moving-aver}

Consider estimating
\begin{equation}
  \label{eq:52}
  X_{t} = Z_{t} + \hat \theta_{m1} Z_{t-1} + \cdots + \hat \theta_{mm} Z_{t-m}
\end{equation} with $Z_{t} \sim WN(0, \hat v_{m})$.

\begin{thm}
  \label{defn:estimation_arma:5}
  We can apply the innovation estimates by applying the recursive
  relations
  \begin{align}
    \label{eq:53}
    \hat v_{0} = \hat \gamma(0) \\
    \hat \theta_{m, m - k} = \frac{1}{\hat v_{k}}( \hat \gamma(m-k) -
    \sum_{j=0}^{k-1} \hat \theta_{m, m - j} \hat \theta_{k, k - j}
    \hat v_{j})
  \end{align} for $k = 0, \dots, m - 1$, and
  \begin{equation}
    \label{eq:54}
    \hat v_{m} = \hat \gamma(0) - \sum_{j=0}^{m-1} \hat \theta^{2}_{m,
      m - j} \hat v_{j}.
  \end{equation}
\end{thm}

\begin{thm}
  \label{defn:estimation_arma:6}
  Let $X_{t}$ be the causal invertible ARMA process $\phi(B) X_{t} =
  \theta(B) Z_{t}$ with $Z_{t} \sim WN(0, \sigma^{2})$, $\E{Z_{t}^{4}}
  < \infty$, and let $\psi(z) = \sum_{j=0}^{\infty} \psi_{j} z^{j} =
  \frac{\theta(z)}{\phi(z)}$ for $|z| \leq 1$, and $\psi_{0} = 1$ and
  $\psi_{j} = 0$ for $j < 0$.

  Then for any sequence of positive integers $m_{n}$, such that $m <
  n$, $m \rightarrow \infty$, and $m = o(n^{\frac{1}{3}})$ as $n
  \rightarrow \infty$, we have for each $k$,
  \begin{align}
    \label{eq:55}
    \frac{n^{\frac{1}{2}}}(\hat \theta_{m1} - \psi_{1}, \dots, \hat
    \theta_{mk} - \psi_{k}) \cd N(0, A)
  \end{align} where $A = [a_{ij}]_{i, j = 1}^{k}$ and
  \begin{equation}
    \label{eq:56}
    a_{ij} = \sum_{r=1}^{\min(i, j)} \psi_{i - r} \psi_{j - r}
  \end{equation} and
  \begin{equation}
    \label{eq:57}
    \hat v_{m} \cp \sigma^{2}.
  \end{equation}
\end{thm}

\begin{remark}
  Note that for the AR($p$) process, the Yule-Walker estimator is a
  consistent estimator of $\mathbf{\phi}_{p}$. However, for an MA($q$)
  process, the estimator $\hat{\mathbf{\theta}}_{q}$ is not consistent
  for the true parameter vector as $n \rightarrow \infty$.  For
  consistency, it is necessary to use the estimators with $m$
  satisfying the conditions given in Theorem
  \ref{defn:estimation_arma:6}.
\end{remark}

\begin{thm}[Asymptotic confidence regions for the $\mathbf{\theta}_{q}$]
  \label{defn:estimation_arma:7}
  \begin{equation}
    \label{eq:58}
    \{ \theta \in R | |\theta - \hat \theta_{mj}| \leq \Phi_{1 -
      \frac{\alpha}{2}} \frac{1}{n^{\frac{1}{2}}}
    (\sum_{k=0}^{j-1}\hat \theta_{mk}^{2})^{\frac{1}{2}} \}
  \end{equation} is an $(1 - \alpha)$ confidence interval for $\theta_{mj}$.
\end{thm}

\subsection{Maximum Likelihood Estimation}
\label{sec:maxim-likel-estim}

Consider $X_{t}$ a gaussian time series with zero mean and
autocovariance function $\kappa(i, j) = \E{X_{i} X_{j}}$.  Let $\hat
X_{j} = P_{j-1} X_{j}$.  Let $\Gamma_{n}$ be the covariance matrix and
assume $\Gamma_{n}$ is nonsingular.  The likelihood of $X_{n}$ is
\begin{equation}
  \label{eq:59}
  L(\Gamma_{n}) = \frac{1}{(2 \pi)^{\frac{n}{2}}} \frac{1}{(\det
    \Gamma_{n})^{\frac{1}{2}}} \exp(-\frac{1}{2} \mathbf{X}'_{n}
  \Gamma_{n}^{-1} \mathbf{X}_{n})
\end{equation}

\begin{thm}
  \label{defn:estimation_arma:8}
  The likelihood of the vector $\mathbf{X}_{n}$ reduces to
  \begin{equation}
    \label{eq:60}
    L(\Gamma_{n}) = \frac{1}{\sqrt{(2\pi)^{n} \prod_{i=0}^{n-1}
        r_{i}}} \exp(-\frac{1}{2} \sum_{j=1}^{n} \frac{(X_{j} - \hat X_{j})^{2}}{r_{j-1}})
  \end{equation}
\end{thm}

\begin{remark}
  Even if $X_{t}$ is not Gaussian, the large sample estimates are the
  same for $Z_{t} \sim IID(0, \sigma^{2})$, regardless of whether or
  not $Z_{t}$ is Gaussian.
\end{remark}

\begin{thm}[Maximum Likelihood Estimators for ARMA processes]
  \label{defn:estimation_arma:9}
  \begin{align}
    \label{eq:61}
    \hat \sigma^{2} = \frac{1}{n} S(\hat{\mathbf{\phi}}, \hat{\mathbf{\theta}})
  \end{align} where $\hat{\mathbf{\phi}}, \hat{\mathbf{\theta}}$ are the
  values of $\mathbf{\phi}, \mathbf{\theta}$ that minimize
  \begin{align}
    \label{eq:62}
    \ell(\mathbf{\phi}, \mathbf{\theta}) = \ln (\frac{1}{n}
    S(\mathbf{\theta}, \mathbf{\theta})) + \frac{1}{n} \sum_{j=0}^{n-1}
    \ln r_{j}
  \end{align} and
  \begin{align}
    \label{eq:63}
    S(\hat{\mathbf{\phi}}, \hat{\mathbf{\theta}}) = \sum_{j=1}^{n}
    \frac{(X_{j} - \hat X_{j})^{2}}{r_{j-1}}
  \end{align}
\end{thm}

\begin{thm}[Asyptotic Distribution of Maximum Likelihood Estimators]
  \label{defn:estimation_arma:10}
  For a large sample from an ARMA($p, q$) process,
  \begin{align}
    \label{eq:64}
    \hat{\mathbf{\beta}} = N(\mathbf{\beta}, \frac{1}{n}V{\mathbf{\beta}})
  \end{align}
  where
  \begin{align}
    \label{eq:65}
    V(\mathbf{\beta}) = \sigma^{2}
    \begin{bmatrix}
      \E{U_{t} U'_{t}} & \E{U_{t} V'_{t}} \\
      \E{V_{t} U'_{t}} & \E{V_{t} V'_{t}}
    \end{bmatrix}^{-1}
  \end{align}
  and $U_{t}$ are the autoregressive process $\phi(B) U_{t} = Z_{t}$
  and $\theta(B) V_{t} = Z_{t}$.

  Note that for $p = 0$, $V(\mathbf{\beta}) = \sigma^{2} [\E{V_{t}
    V'_{t}}]^{-1}$, and for $q = 0$, $V(\mathbf{\beta}) = \sigma^{2}
  [\E{U_{t} U'_{t}}]^{-1}$.
\end{thm}

\subsection{Order Selection}
\label{sec:order-selection}

\begin{defn}[Kullback-Leibler divergence]
  \label{defn:estimation_arma:11}
  The Kullback-Leibler (KL) divergence between $f(\cdot; \psi)$ and
  $f(\cdot; \theta)$ is defined as
  \begin{equation}
    \label{eq:66}
    d(\psi | \theta) = \Delta (\psi|\theta) - \Delta(\theta | \theta)
  \end{equation} where
  \begin{align}
    \label{eq:67}
    \Delta(\psi | \theta) = \E{-2 \ln f(X; \psi)}{\theta}
  \end{align} is the Kullback-Leibler index of $f(\cdot; \psi)$
  relative to $f(\cdot; \theta)$.
\end{defn}

\begin{thm}[AICC of ARMA($p, q$) process]
  \label{defn:estimation_arma:12}
  \begin{align}
    \label{eq:68}
    AICC(\mathbf{\beta}) = -2 \ln L_{X}(\mathbf{\beta},
    \frac{S_{X}(\beta)}{n}) + \frac{2(p+q+1)n}{n - p - q - 2}
  \end{align}
\end{thm}

\begin{thm}[AIC of ARMA($p, q$) process]
  \label{defn:estimation_arma:13}
  \begin{align}
    \label{eq:69}
    AIC(\mathbf{\beta}) = -2 \ln L_{X}(\mathbf{\beta}, \frac{S_{X}(\beta)}{n}) + 2(p + q + 1)
  \end{align}
\end{thm}

\begin{thm}[BIC of ARMA($p, q$) process]
  \label{defn:estimation_arma:13}
  \begin{align}
    \label{eq:69}
    BIC(\mathbf{\beta}) = (n - p - q) \ln \frac{n \hat
      \sigma^{2}}{n-p-q} + n(1 + \ln \sqrt{2 \pi}) + (p + q) \ln
    \frac{\sum_{t=1}^{n} X_{t}^{2} - n \hat \sigma^{2}}{p + q}
  \end{align}
  where $\hat \sigma^{2}$ is the MLE estimate of the white noise
  variance.
\end{thm}

\section{Spectral Analysis}
\label{sec:spectral-analysis}

Let $X_{t}$ be a zero-mean stationary time series with autocovariance
function $\gamma(\cdot)$ satisfying $\sum_{h=-\infty}^{\infty}
|\gamma(h)| < \infty$.

\begin{defn}
  \label{defn:spectral_analysis:1}
  The spectral density of $X_{t}$ is the function $f(\cdot)$ defined
  by
  \begin{align}
    \label{eq:47}
    f(\lambda) = \frac{1}{2 \pi} \sum_{h=-\infty}^{\infty}
    e^{-ih\lambda} y(h)
  \end{align}  The summability implies that the series converges
  absolutely.
\end{defn}

\begin{thm}
  \label{defn:spectral_analysis:2}
  \begin{enumerate}
  \item $f$ is even
  \item $f(\lambda) \geq 0$ for all $\lambda \in (-\pi, \pi]$.
  \item $\gamma(k) = \int_{-\pi}^{\pi} e^{-k\lambda} f(\lambda) d
    \lambda = \int_{-\pi}^{\pi} \cos (k \lambda) f(\lambda) d\lambda$.
  \end{enumerate}
\end{thm}

\begin{defn}
  \label{defn:spectral_analysis:3}
  A function $f$ is the \textbf{spectral density} of a stationary time
  series $X_{t}$ with autocovariance function $\gamma(\cdot)$ if
  \begin{enumerate}
  \item $f(\lambda) \geq 0$ for all $\lambda \in (0, \pi]$,
  \item $\gamma(h) = \int_{-\pi}^{\pi} e^{ih\lambda} f(\lambda)
    d\lambda$ for all integers $h$.
  \end{enumerate}
\end{defn}

\begin{lem}
  If $f$ and $g$ are two spectral density corresponding to the
  autocovariance function $\gamma$, then $f$ and $g$ have the same
  Fourier coefficients and hence are equal.
\end{lem}

\begin{thm}
  \label{defn:spectral_analysis:4}
  A real-valued function $f$ defined on $(-\pi, \pi]$ is the spectral
  density of a stationary process if and only if
  \begin{enumerate}
  \item $f(\lambda) = f(-\lambda)$,
  \item $f(\lambda) \geq 0$
  \item $\int_{-\pi}^{\pi} f(\lambda) d\lambda < \infty$.
  \end{enumerate}
\end{thm}

\begin{thm}
  \label{defn:spectral_analysis:5}
  An absolutely summable function $\gamma(\cdot)$ is the
  autocovariance function of a stationary time series if and only if
  it is even and
  \begin{equation}
    \label{eq:70}
    f(\lambda) = \frac{1}{2 \pi} \sum_{h=-\infty}^{\infty}
    e^{-ih\lambda} \gamma(h) \geq 0
  \end{equation} for all $\lambda \in (-\pi, \pi]$, in which case
  $f(\cdot)$ is the spectral density of $\gamma(\cdot)$.
\end{thm}

\begin{thm}[Spectral Representation of the ACVF]
  \label{defn:spectral_analysis:6}
  A function $\gamma(\cdot)$ defined on the integers is the ACVF of a
  stationary time series if and only if there exists a
  right-continuous, nondecreasing, bounded function $F$ on $[-\pi,
  \pi]$ with $F(-\pi) = 0$ such that
  \begin{equation}
    \label{eq:72}
    \gamma(h) = \int_{-\pi}^{\pi} e^{ih\lambda} dF(\lambda)
  \end{equation} for all integers $h$.
\end{thm}

\begin{remark}
  The function $F$ is a \textbf{generalized distribution function} on
  $[-\pi, \pi]$ in the sense that $G(\lambda) =
  \frac{F(\lambda)}{F(\pi)}$ is a probability distribution function on
  $[-\pi, \pi]$. Note that since $F(\pi) = \gamma(0) = \Var{X_{1}}$,
  the ACF of $X_{t}$ has the spectral representation function
  \begin{equation}
    \label{eq:73}
    \rho(h) = \int_{-\pi}^{\pi} e^{ih \lambda} dG(\lambda)
  \end{equation}

  The function $F$ is called the spectral distribution function of
  $\gamma(\cdot)$. If $F(\lambda)$ can be expressed as $F(\lambda) =
  \int_{-\pi}^{\lambda} f(y) dy$ for all $\lambda \in [-\pi, \pi]$,
  then $f$ is the spectral density function and the time series is
  said to have a continuous spectrum. If $F$ is a discrete function,
  then the time series is said to have a discrete spectrum.
\end{remark}

\begin{thm}
  \label{sec:spectral-analysis-1}
  A complex valued function $\gamma(\cdot)$ is the autocovariance
  function of a stationary process $X_{t}$ if and only if either
  \begin{enumerate}
  \item $\gamma(h) = \int_{-\pi}^{\pi} e^{-ihv} dF(v) $ for all $h =
    0, \pm 1, \dots$ where $F$ is a right-continuous, non-decreasing,
    bounded function on $[-\pi, \pi]$ with $F(-\pi) = 0$, or
  \item $\sum_{i, j =1}^{n} a_{i} \gamma(i - j) \overline a_{j} \geq
    0$ for all positive integers $n$ and all $a = (a_{1}, \dots, a_{n}
    \in \mathbb{C}^{n})$.
  \end{enumerate}
\end{thm}

\subsection{The Spectral Density of an ARMA Process}
\label{sec:spectral-density-an}

\begin{thm}
  \label{sec:spectral-density-an-1}
  If $Y_{t}$ is any zero-mean, possibly complex-valued stationary
  process with spectral distribution function $F_{Y}(\cdot)$ and
  $X_{t}$ is the process $X_{t} = \sum_{j = -\infty}^{\infty} \psi_{j}
  Y_{t-j}$ where $\sum_{j=-\infty}^{\infty} |\psi_{j}| < \infty$, then
  $X_{t}$ is stationary with spectral distribution function
  $F_{X}(\lambda) = \int_{-\pi, \lambda} | \sum_{j=-\infty}^{\infty}
  \psi_{j} e^{-ijv}|^{2} dF_{Y}(v)$ for $-\pi \leq \lambda \leq \pi$.

  If $Y_{t}$ has a spectral density $f_{Y}(\cdot)$, then $X_{t}$ has a
  spectral density $f_{X}(\cdot)$ given by $f_{X}(\lambda) =
  |\Psi(e^{-i\lambda})|^{2} f_{Y}(\lambda)$ where $\Psi(e^{-i\lambda})
  = \sum_{j=-\infty}^{\infty} \psi_{j} e^{-ij\lambda}$.
\end{thm}

\begin{thm}
  \label{sec:spectral-density-an-3}
  Let $X_{t}$ be an ARMA($p, q$) process, not necessarily causal or
  invertible satisfying $\phi(B)X_{t} = \theta(B) Z_{t}$, $Z_{t} \sim
  WN(0, \sigma^{2})$ where $\phi(z) = 1 - \phi_{1} z - \dots -
  \phi_{p} z^{p}$ and $\theta(z) = 1 + \theta_{1} z + \dots +
  \theta_{q} z^{q}$ have no common zeroes and $\phi(z)$ has no zeroes
  on the unit circle.  Then $X_{t}$ has spectral density
  \begin{equation}
    \label{eq:71}
    f_{X}(\lambda) = \frac{\sigma^{2}}{2 \pi} \frac{|\theta(e^{-i\lambda})|^{2}}{||\phi(e^{-i\lambda})|^{2}}
  \end{equation} for $-\pi \leq \lambda \leq \pi$.
\end{thm}

\begin{thm}
  \label{sec:spectral-density-an-4}
  The spectral density of the white noise process is constant,
  $f(\lambda) = \frac{\sigma^{2}}{2 \pi}$.
\end{thm}


\subsection{The Periodogram}
\label{sec:periodogram}

\begin{defn}
  \label{sec:periodogram-1}
  The periodogram of $(x_{1}, \dots, x_{n})$ is the function
  \begin{equation}
    \label{eq:74}
    I_{n}(\lambda) = \frac{1}{n} | \sum_{t=1}^{n} x_{t} e^{-it\lambda}|^{2}
  \end{equation}
\end{defn}

\begin{thm}
  \label{sec:periodogram-2}
  If $x_{1}, \dots, x_{n}$ are any real numbers and $\omega_{k}$ is
  any of the nonzero Fourier Frequencies $\frac{2 \pi k}{n}$ in
  $(-\pi, \pi]$, then $I_{n}(\omega_{k}) = \sum_{|h| < n}^{} \hat
  \gamma(h) e^{-ih \omega_{k}}$ where $\hat \gamma(h)$ is the sample
  ACVF of $x_{1}, \dots, x_{n}$.
\end{thm}

\begin{thm}
  \label{sec:periodogram-4}
  Let $X_{t}$ be the linear process $X_{t} = \sum_{j=-\infty}^{\infty}
  \psi_{j} Z_{t-j}$, $Z_{t} \sim IID(0, \sigma^{2})$, with
  $\sum_{j=-\infty}^{\infty} |\psi_{j}| < \infty$. Let
  $I_{n}(\lambda)$ be the periodogram of $X_{1}, \dots, X_{n}$, and
  let $f(\lambda)$ be the spectral density of $X_{t}$.
  \begin{enumerate}
  \item If $f(\lambda) > 0$for all $\lambda \in [-\pi, \pi]$ and if $0
    < \lambda_{1} < \dots < \lambda_{m} < \pi$, then the random vector
    $(I_{n}(\lambda_{1}), \dots, I_{n}(\lambda_{m}))$ converges in
    distribution to a vector of independent and exponentially
    distributed random variables, the $i$-th component which has mean
    $2\pi f(\lambda_{i})$, $i = 1 \dots, m $.
  \item If $\sum_{j=-\infty}^{\infty} |\psi_{j}| |j|^{\frac{1}{2}} <
    \infty$, $\E{Z^{4}_{1}} = \nu \sigma^{4} < \infty$, $\omega_{j} =
    \frac{2 \pi j}{n} \geq 0$, and $\omega_{k} = \frac{2 \pi k}{n}
    \geq 0$, then
    \begin{align}
      \label{eq:75}
      \Cov{I_{n}(\omega_{j}), I_{n}(\omega_{k})} =
      \begin{cases}
        2(2 \pi)^{2} f^{2}(\omega_{j}) + O(n^{-\frac{1}{2} }) &
        \omega_{j} = \omega_{k} = \{ 0, \pi \} \\
        (2 \pi)^{2} f^{2}(\omega_{j})+ O(n^{-\frac{1}{2} }) & 0 <
        \omega_{j} = \omega_{k} < \pi \\
        O(n^{-1}) & \omega_{j} \neq \omega_{k}
      \end{cases}
    \end{align}
  \end{enumerate}

\end{thm}

\begin{defn}
  \label{sec:periodogram-3}
  The estimator $\hat f(\omega) = \hat f(g(n, \omega))$ with $\hat
  f(\omega_{j})$ defined by $\frac{1}{2 \pi} \sum_{|k| \leq m_{n}}^{}
  W_{n}(k) I_{n}(w_{j+k})$ with $m \rightarrow \infty$, $\frac{m}{n}
  \rightarrow 0$, $W_{n}(k) = W_{n}(-k)$, $W_{n}(k) \geq 0$ for all
  $k$, and $\sum_{|k| \leq m}^{} W_{n}(k) = 1$, and $\sum_{|k|}^{}
  W_{n}^{2}(k) \rightarrow 0$ as $n \rightarrow \infty$ is called a
  \textbf{discrete spectral average estimator} of $f(w)$.
\end{defn}

\begin{thm}
  \label{sec:periodogram-5}
  Let $X_{t}$ be the linear process $X_{t} = \sum_{j=-\infty}^{\infty}
  \psi_{j} Z_{t-j}$, $Z_{t} \sim IID(0, \sigma^{2})$, with
  $\sum_{j=-\infty}^{\infty} |\psi_{j}| |j|^{\frac{1}{2}} < \infty$
  and $\E{Z_{1}^{4}} < \infty$.  If $\hat f$is a discrete spectral
  average estimator of the spectral density $f$, then for $\lambda,
  \omega \in [0, \pi]$,
  \begin{enumerate}
  \item $\lim_{n \rightarrow \infty} \E{\hat f(\omega)} = f(\omega)$
  \item
    \begin{align}
      \label{eq:76}
      \lim_{n \rightarrow \infty} \frac{1}{\sum_{|j| \leq m}^{}
        W_{n}^{2}(j)} \Cov{\hat f(\omega), \hat f(\lambda)} =
      \begin{cases}
        2 f^{2}(\omega) & w = \lambda = \{ 0, \pi \} \\
        f^{2}(\omega) & 0 < \omega = \lambda < \pi \\ \\
        0 & \omega \neq \lambda.
      \end{cases}
    \end{align}
  \end{enumerate}
\end{thm}

\section{ARIMA Processes}
\label{sec:arima-processes}

\begin{defn}
  \label{sec:arima-processes-1}
  If $d$ is a non-negative integer, $X_{t}$ is said to be an
  ARIMA($p,d,q$) process if $Y_{t} = (1 - B)^{d} X_{t}$ is a causal
  ARMA($p, q$) process
\end{defn}

\begin{thm}
  \label{sec:arima-processes-2}
  ARIMA models should be used when there is a slowly decaying positive
  sample autocorrelation function.

  If there is a slowly decaying oscillatory  sample ACF, applying
  the operator $(1 - B + B^{2})$ can applied to produce a series with
  a more rapidly autocorrelation function.
\end{thm}

\subsection{The Box-Cox Transformation}
\label{sec:box-cox-transf}

\begin{defn}
  \label{sec:arima-processes-3}
  The Box-Cox transformation is a\textbf{variance-stabilizing
    transformation} - and should be used whenever the standard
  deviation increases \textbf{linearly} with the mean. The equation is
  \begin{equation}
    \label{eq:77}
    f_{\lambda}(U_{t}) =
    \begin{cases}
      \frac{(U_{t} - 1)}{\lambda}  & U_{t} \geq 0, \lambda > 0 \\
      \ln U_{t} & U_{t} > 0, \lambda = 0.
    \end{cases}
  \end{equation}
\end{defn}

\subsection{Unit Roots Test}
\label{sec:unit-roots-test}

\begin{thm}
  \label{sec:unit-roots-test}
  Let $X_{1}, \dots, X_{n}$ be observations from the AR($1$) model
  $X_{t} - \mu = \phi_{1}(X_{t -1} - \mu) + Z_{t}$ for $Z_{t} \sim
  WN(0, \sigma^{2})$ where $|\phi_{1} < 1$ and $\E{X_{t}} = \mu$.  For
  large $n$, the maximum likelihood estimator of $\hat \phi_{1}$ is
  approximately $N(\phi_{1}, \frac{1 - \phi_{1}^{2}}{n} )$.  In the
  unit root case, the normal approximation is no longer applicable,
  which precludes its use for testing the unit root hypothesis $H_{0}:
  \phi_{1} = 1$ vs $H_{1}: \phi_{1} < 1$.

  To construct a test of $H_{0}$, write the model as $\Delta X_{t} =
  X_{t} - X_{t-1} = \phi^{\star}_{0} + \phi^{\star}_{1} X_{t-1} + Z_{t}$,
  $Z_{t} \sim WN(0, \sigma^{2})$ where $\phi^{\star}_{0} = \mu(1 -
  \phi_{1})$ and $\phi^{\star}_{1} = \phi_{1} - 1$.  Now, let $\hat
  \phi_{1}^{\star}$ be the OLS estimator of $\phi_{1}^{\star}$ found
  by regression $\delta X_{t}$ on $1$ and $X_{t-1}$.  The estimated
  standard error of $\phi_{1}^{\star}$ is
  \begin{equation}
    \label{eq:77}
    \hat{SE} (\hat \phi_{1}^{\star}) = S(\sum_{t=1}^{n} (X_{t-1} -
    \overline X)^{2})^{-\frac{1}{2}}
  \end{equation} where $S^{2} = \sum_{t=2}^{n} (\delta X_{t} - \hat
  \phi_{0}^{\star} - \hat \phi_{1}^{\star} X_{t-1})^{2} / (n - 3)$ and
  $\overline X$ is the sample mean of $X_{1}, \dots, X_{n-1}$.

  Then the limit distribution of the $t$-ration $\hat \tau_{\mu}=
  \frac{\hat \phi_{1}^{\star}}{\hat{SE}(\hat \phi_{1}^{\star})}$ under
  the unit root assumption can be derived.
\end{thm}

\begin{thm}
  \label{sec:unit-roots-test}
  Let $X_{t}$ be a causal and invertible ARMA($p, q$) process
  satisfying $\phi(B)X_{t} = \theta(B) Z_{t}$, $Z_{t} \sim WN(0,
  \sigma^{2})$.  Then the differenced series $Y_{t} = \Delta X_{t}$ is
  a non-invertible ARMA($p, q+1$) process with moving average
  polynomial $\theta(z)(1-z)$.  Thus, testing for a unit root in a MA
  polynomial is equivalent to testing that the time series has not been
  over-differenced.

  Let $X_{1}, \dots, X_{n}$ be observations from the MA(1) model $X_{t}
  = Z_{t} + \theta Z_{t-1}$, $Z_{t} \iid(0, \sigma^{2})$.  Then under
  the assumption $\theta = -1$, $n(\hat \theta + 1)$ where $\hat
  \theta$ is the MLE converges in distribution.  A test of $H_{0}:
  \theta = -1$ vs $H_{1}: \theta > -1$ can be fashioned on this
  limiting result by rejecting $H_{0}$ when $\hat \theta > -1 +
  \frac{c_{\alpha}}{n}$ where $c_{\alpha}$ is the $(1 - \alpha)$
  quantile of the limit distribution of $n(\hat \theta + 1)$.
\end{thm}


\section{Random Variate Generation}
\label{sec:monte-carlo}

\begin{thm}
  \label{sec:monte-carlo-2}
  Consider a discrete random variable $X$ with $M$ mass points $\{
  m_{1}, \dots, m_{M} \}$ with probability $p_{1}, \dots, p_{M}$, such
  that $p_{} \geq 0$, $\sum p_{i} = 1$.  Then the CDF of such a random
  variable is $F(x) = \Prob{X \leq x} = \sum_{i=1}^{\lfloor x \rfloor
    x} p_{i}$.  Write $F(m_{i}) = F_{i}$, and note that $p_{i} = F_{i}
  - F_{i-1}$.  Then we have that to draw from $F$, we simulate $U \sim
  \mathcal{U}_{[0, 1]}$, and set $X = m_{i}$ if $F_{i-1} \leq U \leq F_{i}$.
\end{thm}

\begin{thm}
  \label{sec:monte-carlo-3}
  Let $X \sim F$ be a scalar random variable on $X \subset \R$.
  Assume $F$ is strictly increasing and continuous.  Then $U = F(X)
  \sim \mathcal{U}_{[0, 1]}$.
\end{thm}

\begin{proof}
  $\Prob{U \leq u} = \Prob{F(x) \leq u} = \Prob{X \leq F^{-1}(u)} =
  F(F^{-1}(u)) = u$.

  Thus $X = F^{-1}(U) \sim F$, as $\Prob{X \leq x} = \Prob{F^{-1}(U)
    \leq x} = \Prob{U \leq F(x)} = F(x)$.
\end{proof}

\begin{defn}
  \label{sec:monte-carlo-4}
  Assume that we can generate a uniform random variable $U$ on some
  domain $\mathcal{U}$.  Then let $\mu$ the density of this uniform
  random variable.  Assume we now wish to simulate a random variable
  $X$ uniformly on some domain $\mathcal{X} \subset \mathcal{U}$ ($\mu
  (\cdot | \mathcal{X})$).  Then do to so, simply simulate $U$
  uniformly on $\mathcal{U}$, and if $U \in \mathcal{X}$, set $X = U$,
  otherwise reject $U$ and repeat.  Then if $\mu(X) = p > 0$, $X$ is
  distributed uniformly on $\mathcal{X}$.
\end{defn}

\begin{thm}
  \label{sec:monte-carlo-5}
  Suppose we have two density's $f, g$ and $g$ defined on $X \subset
  \R$. Suppose that we can samply according to $g$ and there exists $M
  \in [1, \infty)$ such that $f(x) \leq M g(x)$ for all $x \in
  \mathcal{X}$. Then $g$ is an envelope density that dominates $f$. If
  we then simulate $Y \sim g$ , and some $U \sim \mathcal{U}_{[0,
    1]}$, and if $U \leq \frac{f(Y)}{Mg(Y)}$, set $X = Y$, otherwise
  re-sample.

  Then the output $X$ has density $f$.
\end{thm}

\begin{thm}
  \label{sec:monte-carlo-6}
  To generate a Gaussian random variable, we can use the
  \textbf{Box-Muller} method.  This proceeds by generating $U \sim
  \mathcal{U}_{[0, 1]}$, and setting $\theta = 2 \pi U$, and then
  simulating $V \sim \mathcal{U}_{[0, 1]}$, and setting $R^{2} - 2
  \log V$.  We can then set $X_{1}, X_{2} = R \sin \theta, R \cos
  \theta$.

  Alternatively, we can generate $U, V \in \mathcal{U}_{[0, 1]}$
  uniformly on the unit disk, and set $W = U^{2} + V^{2}$ (by
  rejecting sampling), and set $X_{1} = U \sqrt{-2 \frac{\log W}{W}
  }$, $X_{2} = V \sqrt{-2 \frac{\log W}{W}}$.
\end{thm}

\section{Monte-Carlo Method and Non-Parametric Inference}
\label{sec:monte-carlo-method}

\begin{defn}
  \label{sec:monte-carlo-method-1}
  The plug-in estimator is defined as
  \begin{equation}
    \hat F_{n}(x) = \frac{1}{n} \sum_{i=1}^{n} \I{X_{i} \leq x}
  \end{equation}
\end{defn}

\begin{thm}
  \label{sec:monte-carlo-method-2}
  Consider estimating $\theta(F) = \E_{F} \phi(X) =
  \int_{\mathcal{X}}^{} \phi(x) f(x) dx$, with plug-in estimator
  $\theta(\hat F_{n}) = \E_{\hat F_{n}} \phi(X) = \frac{1}{n}
  \sum_{i=1}^{n} f(X_{i})$.

  Assume that $\int_{\mathcal{X}} \phi(x)^{2} f(x) dx < \infty$.  Then
  $\theta(\hat F_{n})$ is such that
  \begin{align}
    \label{eq:1}
    \E_{F^{n}}(\theta(\hat F_{n})) = \theta(F), \\
    \label{eq:2}
    \V_{F^{n}}(\theta(\hat F_{n})) = \frac{1}{n} (\int_{\mathcal{X}}
    (\phi(x) - \theta(F))^{2} f(x) dx)
  \end{align}
\end{thm}

\begin{thm}
  \label{sec:monte-carlo-method-3}
  Assume that $\int_{\mathcal{X}} \phi(x)^{2} f(x) dx < \infty$.  Then
  the following statements hold:
  \begin{enumerate}
  \item $\theta(\hat F_{n})$ converges almost surely to $\theta(F)$
  \item $\sqrt{n} (\theta(\hat F_{n}) - \theta(F))$ converges in
    distribution to $Normal(0, \int_{\mathcal{X}} (\phi(x) -
    \theta(F))^{2} f(x) dx)$
  \end{enumerate}
\end{thm}

\begin{thm}
  \label{sec:monte-carlo-method-4}
  Let $g$ be a density that is strictly positive whenever $f \phi$ is
  non-zero.  Let $X_{1}, \dots, X_{n}$ be IID samples from $g$, and
  define the following estimate of  $\theta(F)$ as $\hat \theta_{g} =
  \frac{1}{n} \sum_{i=1}^{n} w_{i} \phi(X_{i})$ where $w_{i} =
  \frac{f(X_{i})}{g(X_{i})}$.

  Then $\hat \theta_{g}$ is such that
  \begin{align}
    \label{eq:3}
    \E_{G^{n}}(\hat \theta_{g}) = \theta(F), \\
    \label{eq:4}
    \V_{G^{n}}(\hat \theta_{g}) = \frac{1}{n} (\int_{\mathcal{X}}
    \frac{\phi(x)^{2} f(x)^{2}}{g(x)} dx - \theta(F)^{2})
  \end{align}
\end{thm}

\begin{thm}
  \label{sec:monte-carlo-method-5}
  Note that the variance of this estimate depends crucially on the
  function $\psi(x) = \frac{\phi(x)^{2} f(x)^{2}}{g(x)}$, and the
  density that minimizes the variance of $\hat \theta_{g}$ is
  $g^{\star} = \frac{f |\phi|}{\int_{\mathcal{X}} f |\phi|}$.
\end{thm}

\begin{defn}
  \label{sec:monte-carlo-method-6}
  If we sample randomly on our space $\mathcal{X}$ according to $f$,
  we create a source of randomness and thus of error.  By using
  \textbf{stratified sampling} we reduce the error by reducing the
  amount of randomness in the picking of the points.

  \begin{enumerate}
  \item Divide the domain into $K$ strata $\Omega_{i}$ that are
    measurable according-to $f$ and form a partition of the domain, and
    we know exactly $w_{i} = \Prob_{f}(\Omega_{i})$.
  \item Sample exactly $T_{i}$ in each stratum $\Omega_{i}$ according
    to $f | \Omega_{i}$.  The numbers $T_{i}$ are deterministic with
    $\sum_{}^{} T_{i}$.  Writing the conditional empirical mean in
    stratum $\Omega_{i}$ as $\mu_{i} = \frac{1}{T_{i}} \sum_{j=1}^{n}
    X_{j} \I{X_{j} \in \Omega_{i}}$, and return the weighted estimate
    of the integral $\hat \mu = \sum_{i=1}^{K} w_{i} \hat \mu_{i}$.
  \end{enumerate}
\end{defn}

\begin{thm}
  \label{sec:monte-carlo-method-7}
  $\E{\hat \mu} = \mu$, and $\V \hat \mu = \sum_{i=1}^{K}
  \frac{w_{i}^{2} \sigma^{2}_{i}}{T_{i}} $ where $\mu_{i} =
  \frac{1}{w_{i}} \int_{\Omega_{i}} \phi(x) f(x) dx$ is the
  conditional mean, and $\sigma^{2}_{i} = \frac{1}{w_{i}}
  \int_{\Omega_{i}}^{} (\phi(x) - \mu_{i})^{2} f(x) dx $ is the
  conditional variance in stratum $\Omega_{i}$.
\end{thm}

\begin{defn}
  \label{sec:monte-carlo-method-8}
  Uniform stratified sampling takes $T_{i}^{u} = w_{i} n$.  This
  \textbf{consolidates} the random sampling while preserving the shape
  of the density $f$.  With this choice of $T_{i}$, we have $\E{\hat
    \mu_{u}} = \mu$, and $\V \hat \mu_{u} = \sum_{i=1}^{K} \frac{w_{i}
  \sigma_{i}^{2}}{n}$.
\end{defn}

\begin{thm}
  \label{sec:monte-carlo-method-9}
  If we solve the minimization problem
  \begin{align}
    \label{eq:5}
    \min_{(T_{i})_{i}} \V \hat \mu = \sum_{i=1}^{K} \frac{w_{i}^{2}
      \sigma_{i}^{2}}{T_{i}}
  \end{align} such that $T_{i} \geq 0$, $\sum_{i}^{} T_{i} = n$
  we obtain the unique solution
  \begin{align}
    \label{eq:6}
    T_{i}^{\star} = \frac{w_{i} \sigma_{i}}{\sum_{j}^{} w_{j}
      \sigma_{j}} n,
  \end{align} known as \textbf{oracle stratified sampling}.  In this
  case, $\E{\hat \mu^{\star}} = \mu$, and $\V \hat \mu^{\star}_{n} =
  \frac{1}{n} (\sum_{i=1}^{K} w_{i} \sigma_{i})^{2}$.
\end{thm}

\begin{thm}
  \label{sec:monte-carlo-method-10}
  If we have Lipschitz function $\phi$ that we are integrating uniformly
  over $[0, 1]$, we have there exists $C > 0$ such that for any $u,
  v$, $|\phi(u) - \phi(v)| \leq C |u - v$.  If we uniformly divide
  $[0, 1]$ into $K$ sections, we have $\Omega_{i} = [\frac{i}{K},
  \frac{i+1}{K}]$, with $w_{i} = \frac{1}{K}$.  Then we have $\V \hat
  \mu_{u} \leq \sum_{i=1}^{n} \frac{1}{n} \int_{\Omega_{i}}^{}
  \frac{C^{2}}{n^{2}} dx = \frac{C^{2}}{n^{3}}$, which is a significant
  gain over the Monte-Carlo estimate of $\frac{1}{n}$.
\end{thm}


\begin{defn}
  \label{sec:monte-carlo-method-11}
  To estimate the $c_{\alpha}$ quantile with tail probability $\alpha$
  of a distribution $F$, we can do this with Monte-Carlo by
  \begin{enumerate}
  \item Choose $B \in \N$.
  \item Restrict $\alpha \in k = \{ 1, \dots, B \} $, with $\alpha =
    \frac{k}{B+1} $.
  \item Simulate $T_{1}, \dots, T_{B}$ according to $F$.
  \item Let $\hat c_{\alpha} = T_{(k)}$ where $T_{(k)}$ is the $k$-th
    order statistic of $T_{1}, \dots, T_{B}$.
  \end{enumerate}
\end{defn}

\begin{thm}
  \label{sec:monte-carlo-method-12}
  If $F$ corresponds to a density $f$, then $\E{F(\hat c_{\alpha})} = \alpha$.
\end{thm}

\begin{defn}
  \label{sec:monte-carlo-method-13}
  Suppose we are interested in the distribution $K_{n}(F)$ of a root
  or pivot $R_{n}(X, F)$ where $X  = (X_{1}, \dots, X_{n})$ (e.g. the
  distribution of the statistic $T(X_{1}, \dots, X_{n})$ in hypothesis
  tests).  The boostrapboostrap estimator of $K_{n}(F)$is $K_{n}(\hat F)$.
\end{defn}

\begin{defn}
  \label{sec:monte-carlo-method-14}
  To approximate the Boostrap estimator by Monte-Carlo, we compute the
  following:
  \begin{enumerate}
  \item Draw $B$ independent boostrap samples $X^{\star}_{b} = (X_{b,
      1}^{\star}, \dots, X_{b, n}^{\star})$ from $\hat F^{n}$.
  \item Approximate $K_{n}(\hat F)$ by the empirical distribution
    function of $(R_{n}(X^{\star}_{b}, \hat F))_{b}$.
  \end{enumerate}
\end{defn}

\section{Bayesian Inference and Associated Methods}
\label{sec:bayes-infer-assoc}

\begin{defn}
  \label{sec:bayes-infer-assoc-1}
  Our objective is to generate samples according to the posterior
  $\pi(\theta | \overline X) = L(X, \theta) p(\theta)$.

  The solution is the generate a Markov chain $(\theta^{(1)},
  \theta^{(2)}, \dots, \theta^{(t)}, \dots)$ such that $\pi$ is the
  stationary distribution.
\end{defn}

\begin{defn}
  \label{sec:bayes-infer-assoc-2}
  The Gibbs sampler proceeds as follows. Let $\overline \theta =
  (\theta_{1}, \dots, \theta_{p})$ be the parameter of interest and
  $\pi(\theta_{i} | \overline \theta_{(-i)}) = \pi_{i}(\overline
  \theta_{(-i)})$ be the conditional posterior distributions. The
  Gibbs sampler works as follows:
  \begin{enumerate}
  \item Set the initial vector $\theta^{(0)}$,
  \item At time $t+1$:
    \begin{enumerate}
    \item Set $\theta_{1}^{(t+1)} \sim \pi_{1}(\theta_{2}^{(t)},
      \theta_{3}^{(t)}, \dots) = \pi_{1}(\overline \theta^{(t)}_{(-1)})$
    \item Set $\theta_{2}(t+1) = \pi_{2}(\theta_{1}^{t+1},
      \theta_{3}^{(t)}, \dots, \theta_{p}^{(t)}) =  \pi_{2}(\overline
      \theta^{(t)}_{(-2)})$
    \item \dots
    \item Set $\theta_{p}^{(t+1)} \sim \pi_{p}(\theta^{(t+1)}_{(-p)})$
    \end{enumerate}
  \item Collect $T$ samples using this iteration.
  \item Throw away the first $b$ samples and consider only the last samples.
  \end{enumerate}

  This method generates a Markov chain whose stationary distribution
  is the posterior under not-too-strong assumptions.

  Note that the samples $\overline \theta^{(t)}$ are \textbf{not independent}.
\end{defn}

\begin{proof}
  The probability of transition from $a$ to $b$, $K(b, a)$ is given as
  $K(b, a) = \min \{ \frac{\pi(b)}{\pi(a)}, 1 \} + (1 -
  \sum_{b'=1}^{M} \min \{  \frac{\pi(b)}{\pi(a)}, 1 \}) \I{a = b}$.
  Then
  \begin{align}
    \label{eq:10}
    (K \pi)(b) &= \sum_{a=1}^{M} K(b, a) \pi(a) \\
    &= \sum_{a=1}^{M} (\min \{ \frac{\pi(b)}{\pi(a)} , 1 \} + (1 -
    \sum_{b'=1}^{M} \min \{ \frac{\pi(b')}{\pi(a)}, 1 \} ) \I{a=b})
    \pi(a) \\
    &= \pi(b)
  \end{align} so $K \pi = \pi$ , and so $\pi$ is the invariant point
  of $K$ and thus the stationary measure.
\end{proof}

\begin{defn}
  \label{sec:bayes-infer-assoc-3}
  The Metropolis Hastings algorithm is a sequential form of rejection
  sampling.  It is an Markov Chain Monte-Carlo method to construct a
  Markov chain whose stationary distribution is $\pi$.

  Consider a distribution $\pi$ defined on a domain $\mathcal{X}$.
  Assume that $\pi$ is such that for any atom $x \in \mathcal{X}$, $\{
  x \} $ is measurable according to $\pi$.  For any $x \in
  \mathcal{X}$, define a transition measure $\mu(\cdot | x)$ on
  $\mathcal{X}$.
  The method proceeds as follows:
  \begin{enumerate}
  \item Set the initial vector $\theta^{(0)}$.
  \item At time $t+1$,
    \begin{enumerate}
    \item Simulate $X \sim \mu(\cdot | \theta^{(t)})$ and $U \sim
      \mathcal{U}_{[0, 1]}$
    \item If $U \leq \frac{\pi(X) \mu(\theta^{(t)} |
        X)}{\pi(\theta^{(t)}) \mu(X | \theta^{(t)})}$, then
      $\theta^{(t+1)} = X$, otherwise $\theta^{(t+1)} = \theta^{(t)}$.
    \end{enumerate}
  \item Collect $T$ samples like that.
  \item Throw away the first $b$ samples, consider the last samples
    (and possibly sub-sample to reduce correlations.)
  \end{enumerate}

  Note that the initial state is important (particular for unbounded
  distributions), the transition probability is important (for fast
  convergence), and the number of samples discarded is
  problem-dependent.
\end{defn}

\begin{defn}
  \label{sec:bayes-infer-assoc-4}
  Given MCMC methods produce a correlated chain of samples - so $t$
  samples do not provide the sample information as $t$ IID samples.  A
  common notion for measuring this is the \textbf{effective sample
    size}.  For any $l \geq 0$, define $\gamma(l)$ as the correlation
  between two samples of lag $l$.  Defining $\rho(l) =
  \frac{\gamma(l)}{\gamma(1)}$, the effective sample size $\tilde T$of
  a chain of length $t$ is
  \begin{equation}
    \label{eq:8}
    \tilde T = \frac{t}{1 + 2 \sum_{l=1}^{T-1} \rho(l)}.
  \end{equation}
\end{defn}

\bibliographystyle{plainnat}
\bibliography{../../common/bibliography}
\end{document}
