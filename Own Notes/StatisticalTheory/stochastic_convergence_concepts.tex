\chapter{Stochastic Convergence Concepts}
\label{cha:stoch-conv-conc}

\begin{defn}[Random Variable]
  A random variable is a (measurable) mapping $$X: (\Omega, \mathcal{A},
  \mu) \rightarrow \mathbb{R}.$$

  The \textbf{distribution function} is defined by
  \begin{equation}
    \label{eq:7}
    F(t) = \Prob{X \leq t} = \mu(w \in \Omega | X(w) \leq t), t \in
  \mathbb{R}
  \end{equation}
  where $\Prob = \mu \circ X^{-1}$ is the law of $X$.

  A random vector $\mathbf{X}$ is a vector of random variables with joint
  distribution
  \begin{equation}
    \label{eq:8}
F(\mathbf{t}) = \Prob{\mathbf{X} \leq \mathbf{t}} = \Prob{\mathbf{X}_{i} \leq \mathbf{t}_{i}, 1 \leq i \leq n}
  \end{equation}
\end{defn}

\begin{defn}[Convergence almost surely]
A sequence $X_{n}, n \in \mathbb{N}$ of random variables converges
almost surely to a random variable $X$ if
\begin{equation}
  \label{eq:4}
  \Prob{X_{n} \rightarrow X} = \mu(\omega \in \Omega | X_{n}(\omega)
  \rightarrow X(\omega)) = 1
\end{equation}

We say that $X_{n} \cas X$.
\end{defn}

\begin{defn}[Convergence in probability]
$X_{n} \cp X$ (in probability) if for all $\epsilon > 0$,
\begin{equation}
  \label{eq:5}
  \Prob{|X_{n} - X} > \epsilon) \rightarrow 0
\end{equation} as $n \rightarrow \infty$.
For random vectors, we define analogously with taking the norm in
$\mathbb{R}^{n}$. 
\end{defn}

\begin{defn}[Convergence in distribution]
$X_{n} \cd X$ or $X_{n}$ converges to $X$ in distribution if
\begin{equation}
  \label{eq:6}
  \Prob{X_{n} \leq t} \rightarrow \Prob{X \leq t}
\end{equation} whenever $t \mapsto \Prob{X \leq t}$ is continuous.
\end{defn}

\begin{proposition}
  Let $(X_{n}, n \in \N)$, $X$ taking values in $\mathcal{X} \subseteq
  \R^{d}$.
  \begin{enumerate}
  \item
    \begin{equation}
      \label{eq:9}
      X_{n} \cas X \Rightarrow X_{n} \cp X \Rightarrow X_{n} \cd X
    \end{equation}
  \item If $X_{n} \rightarrow X$ in any mode, and if $g: \mathcal{X}
    \rightarrow \R^{d}$ is continuous, then $g(X_{n}) \rightarrow
    g(X)$in the same mode.
  \item \textbf{Slutsky's lemma} If $X_{n} \cd X$ and $Y_{n} \cd c$ (a
    constant).  Then
    \begin{enumerate}
    \item
      \begin{equation}
        \label{eq:10}
        Y_{n} \cp c
      \end{equation}
    \item
      \begin{equation}
        \label{eq:11}
        X_{n} + Y_{n}\cd X + c
      \end{equation}

    \item
      \begin{equation}
        \label{eq:12}
        X_{n} Y_{n} \cd cX
      \end{equation} where $Y_{n} \in \R$.
    \item
      \begin{equation}
        \label{eq:13}
        X_{n} Y_{n}^{{-1}} \cd c^{-1} X
      \end{equation} where $Y_{n} \in \R, c \neq 0$.
    \end{enumerate}
  \item If $(A_{n}, n \in \N)$ are random matrices with
    $(A_{n})_{{ij}} \cp A_{ij}$ for all $i, j$ and $X_{n} \cd X$, then
    $A_{n} X_{n} \cd AX$, and if $A$ is invertible, $A_{n}^{-1} X_{n}
    \cd A^{-1}X$, where $A = (A_{ij})$.
  \end{enumerate}
\end{proposition}

\begin{proof}
  Exercise.
\end{proof}

Some key results from probability theory are statements about
\begin{equation}
  \label{eq:15}
  \frac{1}{n} \sum_{i=1}^{n} X_{i}
\end{equation} where the $X_{1}, \dots, X_{n}$ form an
\textbf{infinite} sequence of IID copies of a fixed random variable $X
\sim \Prob$.  The $(X_{1}, X_{2}, \dots)$ can be accommodated as the
coordinate projections of the product probability space
\begin{equation}
  \label{eq:16}
  (\R^{\N}, \mathbb{B}^{\N}, \Prob^{\N})
\end{equation} or, if the $X_{i}$'s are random vectors in $R^{d}$,
then
\begin{equation}
  \label{eq:17}
  ((\R^{d})^{\N}, (\mathbb{B}^{d})^{\N}, \Prob^{\N})
\end{equation} where $Pr = \Prob^{\N}$ is the product space measure
associated to the sequence $(X_{1}, \dots, X_{n}, \dots)$.

\begin{thm}[Law of Large Numbers]
  \label{defn:stochastic_convergence_concepts:1}
  let $X_{1}, \dots, X_n$ be IID copies of $X ~ \Prob$ such that
  $\E{|X_{i}|} < \infty$, then
  \begin{equation}
    \label{eq:18}
    \frac{1}{n} \sum_{i=1}^{n} X_{i} \cas \E{X}
  \end{equation}
\end{thm}

\begin{thm}[Central limit theorem]
  \label{defn:stochastic_convergence_concepts:2}
  Let $X_{1}, \dots, X_{n}$ be IID copies of $X \sim \Prob$ on $\R$
  with $\Var{X} = \sigma^{2} < \infty$.  Then
  \begin{equation}
    \label{eq:19}
    \sqrt{n}\left(\frac{1}{n} \sum_{i=1}^{n} X_{i} - \E{X}\right) \cd N(0, \sigma^{2})
  \end{equation}

  In the multivariate case, where $X \sim \Prob$ on $\R^{d}$ with the
  covariance of $X$ as $\Sigma$, then
  \begin{equation}
    \label{eq:21}
    \sqrt{n} \left(\frac{1}{n} \sum_{i=1}^{n} X_{i} - \E{X}\right) \cd N(0, \Sigma)
  \end{equation}
\end{thm}

Assuming that the random variables $X_{i}$ are bounded, say $|X_{i}|
\leq 1$, the central limit theorem is in fact a non-asymptotic
phenomena (at least for tail events), since by Hoeffding's inequality,
for all $n \in \N$ and $u > 0$,
\begin{equation}
  \label{eq:22}
  \Prob{\sqrt{n} \left| \frac{1}{n} \sum_{i=1}^{n}X_{i} - \E{X}
    \right| > u} \leq 2 e^{-\frac{u^{2}}{2}}
\end{equation}

which compares ``well'' to the Gaussian tail $\Phi(u) = \frac{1}{n}e^{-\frac{u^{2}}{2}}$.


\section{Uniform Laws of Large Numbers}
\label{sec:uniform-laws-large}

Consider $X_{1}, X_{2}, \dots, X_{n}$ IID from law $\Prob$ on $T$
(e.g. $\R^{d}$), and let $h: T \rightarrow R$ such that $\E{|h(X)|} <
\infty$. Then the $h(X_{i})$'s are also IID, so by the law of large
numbers,
\begin{equation}
  \label{eq:23}
  \frac{1}{n} \sum_{i=1}^{n} h(X_{i}) - \E{h(X)} \cas 0
\end{equation}

For finitely many $h$'s, say $h_{i}$, the exceptional set $A_{m}$ such
that the $m$'th LLN $\frac{1}{n} \sum_{i=1}^{n} h_{m}(X_{i}) -
\E{h_{m}(X)} \cas 0$ fails has probability zero, and clearly by the
union bound
\begin{equation}
  \label{eq:24}
  \Prob{\cup_{m=1}^{M} A_{m}} \leq \sum_{m=1}^{M} \Prob{A_{m}} = 0
\end{equation} and so clearly
\begin{equation}
  \label{eq:25}
  \max_{m=1, \dots, M} \left| \sum_{1}{n} \sum_{i=1}^{n} h_{m}(X_{i})
    - \E{h_{m}(X)} \right| \cas 0
\end{equation} as $n \rightarrow \infty$.

For a general class $\mathcal{H}$ of measurable functions $T
\rightarrow \R$, we say that the brackets $\lfloor h_{j}, 
h_{j} \rceil, j = 1, \dots, N$ \textbf{cover} $\mathcal{H}$ if for all
$h \in \mathcal{H}$, there exists some $j$ such that $\lfloor h_{j}(x)
\leq h(x) \leq h_{j}(x) \rceil$ for all $x \in T$.

\begin{proposition}
  Suppose $\mathcal{H}$ is (for all $\epsilon > 0$), covered by
  brackets $\lfloor h_{j}, h_{j} \rceil, i = 1, \dots, N_{\epsilon}$
  such that
  \begin{equation}
    \label{eq:26}
    \E{|h_{j}(X)\rceil|} < \infty, \E{|\lfloor h_{j}(X)|} < \infty,
  \end{equation} and
  \begin{equation}
    \label{eq:27}
    \E{|\lfloor h_{j}(X) - h_{j}(X)\rceil|} < \epsilon.
  \end{equation}  Then
  \begin{equation}
    \label{eq:28}
    \sup_{h \in \mathcal{H}} \left| \frac{1}{n} \sum_{i=1}^{n}
      h(X_{i}) - \E{h(X)} \right| \cas 0
  \end{equation} as $n \rightarrow \infty$.
\end{proposition}

\begin{proof}
  Let $\epsilon > 0$ be given.  By the law of large numbers for
  finitely many $h_{i}$'s, we have that for all $n \geq N(\epsilon,
  \omega)$,
  \begin{equation}
    \label{eq:29}
  \end{equation}
\end{proof}





%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "master"
%%% End: 
