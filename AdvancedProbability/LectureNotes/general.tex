\chapter{Conditional Expectation}
\label{cha:cond-expect}

Let $(\Omega, \mathcal{F}, \Prob)$ be a probability space.
$\Omega$ is a set, $\mathcal{F}$ is a $\sigma$-algebra on $\Omega$,
and $\mathbb{P}$ is a probability measure on ($\Omega, \mathcal{F}$).

\begin{defn}
  \label{defn:1}
  $\mathcal{F}$ is a $\sigma$-algebra on $\Omega$ if it satisfies
  \begin{itemize}
  \item $\emptyset, \Omega \in \mathcal{F}$
  \item $A \in \mathcal{F} ==> A^{c} \in \mathcal{F}$
  \item $(A_n)_{n \geq 0}$ is a collection of sets in $\mathcal{F}$
    then $\cup_{n} A_{n} \in \mathcal{F}$.
  \end{itemize}
\end{defn}

\begin{defn}
  \label{defn:2}
  $\Prob$ is a probability measure on $(\Omega, \mathcal{F})$ if
  \begin{itemize}
  \item $\Prob: \mathcal{F} \rightarrow [0, 1]$ is a set function.
  \item $\Prob{\emptyset} = 0$, $\Prob{\Omega} = 1$,
  \item $(A_{n})_{n \geq 0}$ is a collection of pairwise disjoint
      sets in $\mathcal{F}$, then $\Prob{\cup_{n} A_{n}} =
      \sum_{n} \Prob{A_{n}}$.
  \end{itemize}
\end{defn}

\begin{defn}
  \label{defn:3}
  The Borel $\sigma$-algebra $\mathbb{B}(\mathbb{R})$ is the
  $\sigma$-algebra generated by the open sets of $\mathbb{R}$.  Call
  $\mathcal{O}$ the collection of open subsets of $\mathbb{R}$, then
  \begin{equation}
    \label{eq:1}
    \mathbb{B}(\mathbb{R}) = \cap \{ \xi: \xi \text{ is a sigma
      algebra containing $\mathcal{O}$} \}
  \end{equation}
\end{defn}

\begin{defn}
  \label{defn:4}
  $\mathcal{A}$ a collection of subsets of $\Omega$, then we write
  $\sigma(\mathcal{A}) = \cap \{ \xi: \xi \text{ a sigma algebra
    containing $\mathcal{A}$} \}$
\end{defn}

\begin{defn}
  \label{defn:5}
  $X$ is a random variable on $(\Omega, \mathcal{F})$  if $X: \Omega
  -> \mathbb{R}$ is a function with the property that $X^{-1}(V) \in
  \mathcal{F}$ for all $V$ open sets in $\mathbb{R}$.
\end{defn}

\begin{exer}
  If $X$ is a random variable then $\{ B \subseteq \mathbb{R}, X^{-1}(B)
  \in \mathcal{F} \}$ is a $\sigma$-algebra and contains
  $\mathbb{B}(\mathbb{R})$.
\end{exer}

If $(X_{i}, i \in I)$ is a collection of random variables, then we
write $\sigma(X_{i}, i \in I) = \sigma(\{ \omega \in \Omega:
X_{i(\omega) \in \mathcal{B} \}, i \in I, B \in
  \mathbb{B}(\mathbb{R}))) }$
and it is the smallest $\sigma$-algebra that makes all the $X_{i}$'s
measurable.

\begin{defn}
  \label{defn:6}
  First we define it for the positive simple random variables.

  \begin{equation}
    \label{eq:2}
    \E{\sum_{i=1}^{n} c_{i} \mathbf{1}(A_{i})} =
    \sum_{i=1}^{n} \Prob{A_{i}}.
  \end{equation}
  with $c_{i}$ positive constants, $(A_{i}) \in \mathcal{F}$.

  We can extend this to any positive random variable $X \geq 0$ by
  approximation $X$ as the limit of piecewise constant functions.

  For a general $X$, we write $X = X^{+} - X^{-}$ with $X^{+}= \max(X,
  0), X^{-} = \max(-X, 0)$.
\end{defn}

If at least one of $\E{X^{+}}$ or $\E{X^{-}}$ is
finite, then we define $\E{X} = \E{X^{+}} +
\E{X^{-}}$.

We call $X$ integrable if $\E{|X|} < \infty$.

\begin{defn}
  \label{defn:7}
  Let $A, B \in \mathcal{F}, \Prob{B} > 0$. Then
  \begin{align*}
    \Prob{A | B} = \frac{\Prob{A \cap B}}{\Prob{B}} \\
    \mathbb{E}[X | B] = \frac{\E{X \mathbb{1}(B)}}{\Prob{B}}
  \end{align*}
\end{defn}

Goal - we want to define $\E{X | \mathcal{G}}$ that is a
random variable measurable with respect to the $\sigma$-algebra
$\mathcal{G}$.

\section{Discrete Case}
\label{sec:discrete-case}

Suppose $\mathcal{G}$ is a $\sigma$-algebra countably generated
$(B_{i)_{i \in \mathbb{N}}}$ is a collection of pairwise disjoint sets
in $\mathcal{F}$ with $\cup B_{i} = \Omega$. Let $\mathcal{G} =
\sigma(B_{i}, i \in \mathbb{N})$.

It is easy to check that $\mathcal{G} = \{ \cup_{j \in J} B_{j}, J
\subseteq {N} \}$.

Let $X$ be an integrable random variable.  Then
\begin{align*}
  X' = \E{X | \mathcal{G}} = \sum_{i \in \mathbb{N}}
  \E{X | \mathcal{B}_{i}} \I{B_{i}}
\end{align*}

\begin{enumerate}
\item $X'$ is $\mathcal{G}$-measurable (check).
\item
  \begin{equation}
    \label{eq:3}
    \E{|X'|} \leq \E{|X|}
  \end{equation} and so $X'$ is integrable.
\item $\forall G \in \mathcal{G}$, then
  \begin{equation}
    \label{eq:4}
    \E{X \I{G} = \E{X' \I{G}}}
  \end{equation} (check).
\end{enumerate}

\section{Existence and Uniqueness}
\label{sec:existence-uniqueness}

\begin{defn}
  \label{defn:8}
  $A \in \mathcal{F}$, $A$ happens almost surely (a.s.) if
  $\Prob{A} = 1$.
\end{defn}

\begin{thm}[Monotone Convergence Theorem]
  If $X_{n} \geq 0$ is a sequence of random variables and $X_{n}
  \uparrow X$ as $n \rightarrow \infty$ a.s, then
  \begin{equation}
    \label{eq:5}
    \E{X_{n}} \uparrow \E{X}
  \end{equation} almost surely as $n \rightarrow \infty$.
\end{thm}

\begin{thm}[Dominated Convergence Theorem]
  If $(X_{n})$ is a sequence of random variables such that $|X_{n}|
  \leq Y$ for $Y$ an integrable random variable, then if $X_{n} \cas
  X$ then $\E{X_{n}} \cas \E{X}$. 
\end{thm}

\begin{defn}
  \label{defn:9}
  For $p \in [1, \infty)$, $f$ measurable functions, then
  \begin{align}
    \| f \|_{p} = E[|f|^{p}]^{\frac{1}{p}} \\
    \| f \|_{\infty} = \inf \{ \lambda : |f| \leq \lambda a.e. \}
  \end{align}
\end{defn}

\begin{defn}
  \label{defn:10}
  \begin{align*}
    L^{p} = L^{p}(\Omega, \mathcal{F}, \mathbb{P}) = \{ f : \| f
    \|_{p} < \infty \} \\
  \end{align*}
  Formally, $L^{p}$ is the collection of equivalence classes where two
  functions are equivalent if they are equal a.e.  We will just
  represent an element of $L^{p}$ by a function, but remember that
  equality is a.e.
\end{defn}

\begin{thm}
  The space $(L^{2}, \| \cdot \|_{2})$ is a Hilbert space with
  $\langle U, V \rangle> = \E{UV}$.

  Suppose $\mathcal{H}$ is a closed subspace, then $\forall f \in
  L^{2}$ there exists a unique $g \in \mathcal{H}$ such that $\| f - g
  \|_{2} = \inf \{ \| f- h \|_{2}, h \in \mathcal{H}$ and $\langle f
  -g, h \rangle = 0$ for all $h \in \mathcal{H}$.

  We call $g$ the orthogonal projection of $f$ onto $\mathcal{H}$.
\end{thm}


\begin{thm}
  Let $(\Omega, \mathcal{F}, \P)$ be an underlying probability space,
  and let $X$ be an integrable random variable, and let $\mathcal{G}
  \subset \mathcal{F}$ sub $\sigma$-algebra.  Then there exists a
  random variable $Y$ such that
  \begin{enumerate}
  \item $Y$ is $\mathcal{G}$-measurable
  \item If $A \in \mathcal{G}$,
    \begin{equation}
      \label{exmp:general:1}
      \E{X \I{A} = \E{Y \I{A}}}
    \end{equation} and $Y$ is integrable.
  \end{enumerate}
  Moreover, if $Y'$ also satisfies the above properties, then $Y = Y'$ a.s.
\end{thm}
\begin{remark}
  $Y$ is called a version of the conditional expectation of $X$ given
  $\mathcal{G}$ and we write $\mathcal{G} = \sigma(Z)$ as $Y = \E{X |
    \mathcal{G}}$.
\end{remark}


\begin{remark}
  (b) could be replaced by the following condition: for all $Z$
  $\mathcal{G}$-measurable, bounded random variables,
  \begin{equation}
    \label{exmp:general:2}
    \E{XZ} = \E{YZ}
  \end{equation}
\end{remark}

\begin{proof}

  \textbf{Uniqueness} - let $Y'$ satisfy (a) and (b).  If we consider
  $\{Y' - Y > 0 \} = A$, $A$ is $\mathcal{G}$ measurable.
  From (b),
  \begin{align*}
    \E{(Y' - Y)\I{A} = \E{X\I{A}}} - \E{X
      \I{A}} = 0
  \end{align*} and hence $\Prob{Y' - Y > 0)} = 0$ which implies that $Y'
    \leq Y$ a.s.  Similarly, $Y' \geq Y$ a.s.

  \textbf{Existence} - Complete the following three steps:
  \begin{enumerate}
  \item $X \in L^2(\Omega, \mathcal{F}, \P)$ is a Hilbert space with
    $\langle U, V \rangle = \E{UV}$.
    The space $L^{2}(\Omega, \mathcal{G}, \P)$ is a closed
    subspace.
    \begin{equation}
      \label{exmp:general:3}
      X_{n} \rightarrow X (L^{2}) => X_{n} \cp X => \exists subseq
      X_{n_{k}} \cas X => X' = \limsup X_{n_{k}}
    \end{equation}

  We can write
  \begin{align*}
    L^{2}(\Omega, \mathcal{F}, \P) = L^{2}(\Omega, \mathcal{G}, \P) +
    L^{2}(\Omega, \mathcal{G}, \P)^{\perp} \\
    X = Y + Z
  \end{align*}  Set $Y = \E{X | \mathcal{G}}$, $Y$ is
  $\mathcal{G}$-measurable, $A \in \mathcal{G}$.
  
  \begin{align*}
    \E{X \I{A}} = E{Y \I{A}} + \underbrace{E{Z \I{A}}}_{=0}
  \end{align*}
  
\item   If $X \geq 0$ then $Y \geq 0$ a.s. Consider $A = \{ Y < 0 \}$, then
  \begin{equation}
    \label{exmp:general:4}
    0 \leq \E{X \I{A}} = \E{Y \I{A}} \leq 0
  \end{equation}
  Thus $\Prob{A} = 0 \Rightarrow Y \geq 0$ a.s.

  Let $X \geq 0$, Set $0 \leq X_{n} = max(X, n) \leq n$, so $X_{n} \in
  L^{2}$ for all $n$. Write $Y_{n} = \E{X_{n} | \mathcal{G}}$, then
  $Y_{n} \geq 0$ a.s., $Y_{n}$ is increasing a.s..  Set $Y = \limsup
  Y_{n}$.  So $Y$ is $\mathcal{G}$-measurable.  We will show $Y = \E{X
    | \mathcal{G}}$ a.s.  For all $A \in \mathcal{G}$, we need to
  check $\E{X \I{A}} = \E{Y \I{A}}.$ We know that
  $\E{X_{n} \I{A}} = \E{Y_{n} \I{A}}$, and $Y_{n}
  \uparrow Y$ a.s.  Thus, by monotone convergence theorem, $\E{X
    \I{A}} = \E{Y\I{A}}$.
  
  If $X$ is integrable, setting $A = \Omega$, we have $Y$ is
  integrable.
\item $X$ is a general random variable, not necessarily in $L^{2}$ or
  $\geq 0$.  Then we have that $X = X^{+} + X^{-}$.  We define $\E{X |
    \mathcal{G}} = \E{X^{+} | \mathcal{G}} - \E{X^{-} | \mathcal{G}}$.
  This satisfies (a), (b).
\end{enumerate}
\end{proof}

\begin{remark}
  If $X \geq 0$, we can always define $Y = \E{X | \mathcal{G}}$ a.s.
  The integrability condition of $Y$ may not be satisfied.
\end{remark}

\begin{defn}
  \label{defn:general:1}
  Let $\mathcal{G}_{0}, \mathcal{G}_{1}, \dots$ be sub $\sigma$-algebras of
  $\mathcal{F}$.  Then they are called independent if for all $i, j
  \in \mathbb{N}$,
  \begin{equation}
    \label{exmp:general:5}
    \Prob{G_i \cap \dots \cap G_j} = \Pi_{i = 1}^{n} \Prob{G_{i}}
  \end{equation}
\end{defn}

\begin{thm}
  \begin{enumerate}
  \item If $X \geq 0$ then $\E{X | \mathcal{G}} \geq 0$
  \item $\E{\E{X | \mathcal{G}}} = \E{X}$ ($A = \Omega$)
  \item $X$ is $\mathcal{G}$-measurable implies $\E{X | \mathcal{G}} = X$ a.s.
  \item $X$ is independent of $\mathcal{G}$, then $\E{X|\mathcal{G}} = \E{X}$.
  \end{enumerate}
\end{thm}

\begin{thm}[Fatau's lemma] $X_{n} \geq 0$, then for all $n$,
  \begin{equation}
    \label{exmp:general:6}
    \E{\liminf X_{n}} \leq \liminf \E{X_{n}}
  \end{equation}
\end{thm}

\begin{thm}[Conditional Monotone Convergence]  Let $X_{n} \geq 0$,
  $X_{n} \uparrow X$ a.s.  Then
  \begin{equation}
    \label{exmp:general:7}
    \E{X_{n} | \mathcal{G}} \uparrow \E{X | \mathcal{G}} a.s.
  \end{equation} 
\end{thm}

\begin{proof}
  Set $Y_{n} = \E{X_{n} | \mathcal{G}}$.  Then $Y_{n} \geq 0$ and
    $Y_{n}$ is increasing.  Set $Y  = \limsup Y_{n}$.  Then $Y$ is
    $\mathcal{G}$-measurable.  
\end{proof}

\begin{thm}[Conditional Fatau's Lemma]  $X_{n} \geq 0$, then
  \begin{equation}
    \label{exmp:general:8}
    \E{\liminf X_{n} | \mathcal{G}} \leq \liminf \E{X_{n} |
      \mathcal{G}} a.s.
  \end{equation}
\end{thm}

\begin{proof}
  Let $X$ denote the limit inferior of the $X_{n}$. For every natural number
  $k$ define pointwise the random variable $Y_k= \inf_{n \geq k} X_n$.
  Then the sequence $Y_{1}, Y_{2}, \dots$ is increasing and converges pointwise
  to $X$. For $k \leq n$, we have $Y_{k} \leq X_{n}$, so that 
  \begin{equation}
    \label{eq:6}
    \E{Y_k | \mathcal{G}} \leq \E{X_n | \mathcal{G}} a.s
  \end{equation}

  by the monotonicity of conditional expectation, hence
  \begin{equation}
    \label{eq:7}
    \E{Y_k|\mathcal{G}} \leq \inf_{n \geq k} \E{X_n | \mathcal{G}} a.s.
  \end{equation}

  because the countable union of the exceptional sets of probability
  zero is again a null set. Using the definition of X, its
  representation as pointwise limit of the $Y_k$, the monotone convergence
  theorem for conditional expectations, the last inequality, and the
  definition of the limit inferior, it follows that almost surely

  \begin{align}
  \E{\liminf_{n \rightarrow \infty} X_{n} | \mathcal{G}} 
 & = \E{X |
    \mathcal{G}}                                         \\
 & = \E{\lim_{k \rightarrow \infty} Y_{k} | \mathcal{G}} \\
 & = \lim_{k \rightarrow \infty} \E{Y_{k} | \mathcal{G}} \\
 & \leq \lim_{k \rightarrow \infty} \inf_{n \geq k} \E{X_{n} |
    \mathcal{G}}                                         \\
 & = \liminf_{n \rightarrow \infty} \E{X_{n} | \mathcal{G}}
 \end{align}
\end{proof}


\begin{thm}[Conditional dominated convergence] TODO
  
\end{thm}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "master"
%%% End: 
