\input{../../common/summary.tex}

\title{Convex Optimization Summary}

\begin{document}

\maketitle

\section{Existence}
\label{sec:existence}

\begin{defn}
  \label{sec:existence-1}
  For $C \subseteq \R^{n}$, define $\delta_{C}$ as
  \begin{equation}
    \label{eq:1} \delta_{C}(x) =
    \begin{cases}
      0 & x \in C \\
      \infty & x \notin C
    \end{cases}
  \end{equation}

  Note $x'$ minimizes $f$ over $C$ if and only if $x'$ minimizes $f +
  \delta_{C}$ over $\R^{n}$.
\end{defn}

\begin{defn}
  \label{sec:existence-2}
  \begin{enumerate}
  \item $\dom f = \{ x \in \R^{n} | f(x) < \infty \}$,
  \item \begin{equation}
      \argmin f =
      \begin{cases}
        \emptyset & f \equiv \infty \\
        \{ x \in \R^{n} | f(x) = \inf f \} & f < \infty
      \end{cases}
    \end{equation}
  \item $f$ is \textbf{proper} if and only if $\dom f \neq \emptyset$
    and $f(x) > -\infty$ for all $x \in \R^{n}$.
  \end{enumerate}
\end{defn}

\begin{defn}
  \label{sec:existence-3}
  For $f: \R^{n} \rightarrow \overline \R$, denote
  \begin{equation}
    \label{eq:2}
    \epi f = \{ (x, \alpha) \in \R^{n} \times \R | f(x) \leq \alpha \}
  \end{equation}
\end{defn}

\begin{thm}
  \label{sec:existence-4}
  A set $C$ is an epigraph if and only if for every $x$ there is an
  $\alpha \in \overline \R$ such that $C \cap (x \times \R) = [\alpha,
  \infty]$ - so all vertical one-dimensional sections must be closed
  upper half-lines. If $f$ is proper then $\epi f $ is not empty and
  does not include a complete vertical line.
\end{thm}

\begin{defn}
  \label{sec:existence-5}
  For $f: \R^{n} \rightarrow \overline \R$, define
  \begin{equation}
    \label{eq:3}
    \liminf_{x \rightarrow x'} f(x) = \lim_{\delta \downarrow 0}
    \inf_{\| x - x'\|_{2} \leq \delta} f(x) = \lim_{k \rightarrow
      \infty} \inf_{\| x - x'\|_{2} \leq \frac{1}{k}} f(x)
  \end{equation}
  $f$ is \textbf{lower semicontinuous} at $x'$ if and only if $f(x')
  \leq \liminf_{x \rightarrow x'} f(x)$.
  $f$ is \textbf{lower semicontinous} if $f$ is lower semicontinous at
  every $x' \in \R^{n}$.
\end{defn}

\begin{thm}
  \label{sec:existence-7}
  \begin{equation}
    \label{eq:4}
    \liminf_{x \rightarrow x'} f(x) = \min \{ \alpha \in \overline \R
    | \exists (x^{k}) \rightarrow x' : f(x^{k}) \rightarrow \alpha \}
  \end{equation}
  In particular, $f$ is lower semi-continuous at $x'$ if and only if
  $f(x') \leq \liminf_{k \rightarrow \infty} f(x^{k})$ for all
  convergence sequences $x^{k} \rightarrow x'$.
\end{thm}

\begin{thm}
  \label{sec:existence-8}
  Let $f: \R^{n} \rightarrow \overline \R$.  Then the following are
  equivalent:
  \begin{enumerate}
  \item $f$ is lsc on $\R^{n}$
  \item $\epi f$ is closed in $\R^{n} \times \R$
  \item The sub level sets $\lev_{\leq \alpha} f = \{ x \in \R^{n} |
    f(x) \leq \alpha \} $ are closed in $\R^{n}$ for all $\alpha \in
    \overline \R$.
  \end{enumerate}
\end{thm}

\begin{proof}
  $\epi f$ can only be not closed along vertical lines.
\end{proof}

\begin{defn}
  \label{sec:existence-9}
  $f: \R^{n} \rightarrow \overline \R$ is level bounded if and only if
  $\lev_{\leq \alpha} f$ is bounded for all $\alpha \in \R$.
\end{defn}

\begin{thm}
  \label{sec:existence-10}
  $f: \R^{n} \rightarrow \overline \R$ is level-bounded if and only if
  $f(x^{k}) \rightarrow \infty$ for all sequences $(x^{k})$ satisfying
  $\| x^{k} \|_{2} \rightarrow \infty$.
\end{thm}

\begin{proof}
  $K(\alpha)$ such that $x^{k} \notin \lev_{\leq \alpha} f$  for $k
  \geq K(\alpha)$ by boundedness. In reverse, find $\alpha$ with $\lev
 _{\leq \alpha} f $ unbounded and choose $x^{k}$ in this set with
 $f(x^{k}) \leq \alpha$.
\end{proof}

\begin{thm}
  \label{sec:existence-11}
  Assume $f: \R^{n} \rightarrow \overline \R$ is lsc, level-bounded,
  and proper.  Then $\inf f(x) \in (-\infty, +\infty)$ and $\argmin f$
  is nonempty and compact.
\end{thm}

\begin{proof}
  Consider $\cap_{\alpha \in \R, a > \in f} = \argmin f$, then this is
  countable interesction of nonempty, compact sets, so intersection is
  nonempty.

  Finiteness of inf is shown as for any $x \in \argmin f \neq
  \emptyset$, must have $f(x) = -\infty$ contradicting properness.
\end{proof}

\begin{thm}
  \label{sec:existence-12}
  \begin{enumerate}
  \item $f, g$ lsc, proper implies $f + g$ is lsc.
  \item $f$ lsc, $\lambda \geq 0$ implies $\lambda f$ is lsc
  \item $f: \R^{n} \rightarrow \overline \R$ is lsc and $g: \R^{m}
    \rightarrow \R^{n}$ is continuous implies $f \circ g$ is lsc.
  \end{enumerate}
\end{thm}

\section{Convexity}
\label{sec:convexity}

\begin{defn}
  \label{sec:convexity-1}
  $f: \R^{n} \rightarrow \overline \R$ is convex if and only if
  \begin{equation}
    \label{eq:5}
    f((1 - \tau)x + \tau y) \leq (1 - \tau) f(x) + \tau f(y)
  \end{equation} for all $x, y \in \R^{n}, \tau \in (0, 1)$.

  A set $C \subseteq \R^{n}$ is convex if and only if $\delta_{C}$ is
  convex if and only if $(1 - \tau) x + \tau y \in C$ for all $x, y
  \in C, \tau \in (0, 1)$.

  $f: \R^{n} \rightarrow \overline \R$ is strictly convex if and only
  if $f$ is convex and the inequality is strict for all $x \neq y$ and
  $\tau \in (0, 1)$.
\end{defn}

\begin{defn}
  \label{sec:convexity-2}
  Let $x_{0}, \dots, x_{m} \in \R^{n}$  and $\lambda_{0}, \dots,
  \lambda_{m} \geq 0$, $\sum_{i=0}^{m} \lambda_{i} = 1$.  The linear
  combination $\sum_{i=0}^{m} \lambda_{i} x_{i}$ is a \textbf{convex
  combination} of the points $x_{0}, \dots, x_{m}$.
\end{defn}

\begin{thm}
  \label{sec:convexity-3}
  \begin{enumerate}
  \item $f: \R^{n} \rightarrow \overline \R$ is convex if and only if
    \begin{equation}
      \label{eq:6}
      f(\sum_{i=0}^{m} \lambda_{i} x_{i}) \leq \sum_{i=0}^{m}
      \lambda_{i} f(x_{i})
    \end{equation} for all $m \geq 0, x_{i} \in \R^{n}, \lambda_{i}
    \geq 0, \sum_{i=0}^{m} \lambda_{i} = 1$.
  \item $C \subseteq \R^{n}$ is convex if and only if $C$ contains all
    convex combinations of its elements.
  \end{enumerate}
\end{thm}

\begin{thm}
  \label{sec:convexity-4}
  $f: \R^{n} \rightarrow \overline \R$ is convex implies $\dom f$ is convex.
\end{thm}

\begin{thm}
  \label{sec:convexity-5}
  \begin{enumerate}
  \item $f: \R^{n} \rightarrow \overline \R$ is convex if and only if
    $\epi f$ is convex in $\R^{n} \times \R$.
  \item $f: \R^{n} \rightarrow \overline \R$ is strictly convex if and only if
    $\{ (x, \alpha) \in \R^{n} \times \R | f(x) < \alpha \} $ is
    convex in $\R^{n} \times \R$.
  \item $f: \R^{n} \rightarrow \overline \R$ is convex implies
    $\lev_{\leq \alpha} f$ is convex for all $\alpha \in \overline \R$.
  \end{enumerate}
\end{thm}

\begin{thm}
  \label{sec:convexity-7}
  Assume $f: \R^{n} \rightarrow \overline \R$ is convex. Then
  \begin{enumerate}
  \item $\argmin f$ is convex.
  \item $x$ is a local minimizer of $f$ implies $x$ is a global
    minimize of $f$.
  \item $f$ is strictly convex and proper impiles $f$ has at most one
    global minimizer.
  \end{enumerate}
\end{thm}

\begin{thm}
  \label{sec:convexity-8}
  Let $I$ be an arbitrary index set. Then
  \begin{enumerate}
  \item $f_{i}, i \in I$ convex implies $\sup_{i \in I} f_{i}(x)$ is
    convex.
  \item $f_{i}, i \in I$ strictly convex, $I$ finite implies $\sup_{i
      \in I} f_{i}(x)$ is strictly convex.
  \item $C_{i}, i \in I$ is convex implies $\cap_{i \in I} C_{i}$ is convex.
  \item $f_{k}, k \in N$ is convex implies $\limsup_{k \rightarrow
      \infty} f_{k}(x)$ is convex.
  \end{enumerate}
\end{thm}

\begin{thm}
  \label{sec:convexity-9}
  Assume $C \subseteq \R^{n}$ is open and convex, and $f: C
  \rightarrow \R$ is differentiable. Then the following are
  equivalent:
  \begin{enumerate}
  \item $f$ is [strictly] convex
  \item $\IP{y-x, \grad f(y) - \grad f(x)} \geq 0$ for all $x, y \in
    C$ [and $> 0$ if $x \neq y$]
  \item $f(x) + \IP{y- x, \grad f(x)} \leq f(y)$ for all $x, y \in C$
    [and $< f(y)$ if $x \neq y$]
  \item If $f$ is additionally twice differentiable, then $\grad^{2}
    f(x)$ is positive semidefinite for all $x \in C$.
  \end{enumerate}
\end{thm}

\begin{proof}
  Reduce to one-dimensional sections $g(t) = f(x + t(x-y))$.  Take
  $g(x+h)$, use convexity to show $g'(x) \leq \frac{g(y) -
    g(x)}{y-x}$.

  Use $g(x) = \sup_{y \in \dom g} g(y) + (y-x) g'(y)$ to show the
  supremum over convex functions is convex.
\end{proof}

\begin{thm}
  \label{sec:convexity-10}
  \begin{enumerate}
  \item Assume $f_{1}, \dots, f_{m}: \R^{n} \rightarrow \overline \R$
    are convex, $\lambda_{1}, \dots, \lambda_{m} \geq 0$.  Then $f =
    \sum_{i=1}^{m} \lambda_{i} f_{i}$ is convex.  If at least one of
    the $f_{i}$ with $\lambda_{i} > 0$ is strictly convex, then $f$ is
    strictly convex.
  \item Asssume $f_{i}: \R^{n_{i}} \rightarrow \overline \R$ are
    convex.  Then
    \begin{align}
      \label{eq:7}
      f: \R^{n_{1}} \times \dots R^{n_{m}} \rightarrow \overline \R
      f(x_{1}, \dots, x_{m}) = \sum_{i=1}^{m} f_{i}(x_{i})
    \end{align} is convex. If all $f_{i}$ are strictly convex, then
    $f$ is strictly convex.
  \item If $f: \R^{m} \rightarrow \overline \R$ is convex, $A \in
    \R^{m \times n}, b \in \R^{m}$.  Then $g(x) = f(Ax + b)$ is convex.
  \end{enumerate}
\end{thm}

\begin{thm}
  \label{sec:convexity-11}
  \begin{enumerate}
  \item $C_{1}, \dots, C_{m}$ convex implies $C_{1} \times \dots
    \times C_{m}$ is convex.
  \item $C \subseteq \R^{n}$ convex, $A \in \R^{m \times n}$, $b \in
    \R^{m}$, $L(x) = Ax + b$ implies $L(C)$ is convex.
  \item $C \subseteq \R^{m}$ convex, $A \in \R^{m \times n}$, $b \in
    \R^{m}$, $L(x) = Ax + b$ implies $L^{-1}(C)$ is convex.
  \item $C_{1}, C_{2}$ is convex implies $C_{1} + C_{2}$ is convex.
  \item $C$ convex, $\lambda \in \R$ implies $\lambda C$ is convex.
  \end{enumerate}
\end{thm}

\begin{defn}
  \label{sec:convexity-12}
  For a set $S \subseteq \R^{n}$ and $x \in \R^{n}$, define the
  projection of $x$ onto $S$ as
  \begin{equation}
    \label{eq:9}
    \proj_{S}(y) = \argmin_{x \in S} \|x - y\|_{2}.
  \end{equation}
\end{defn}

\begin{thm}
  \label{sec:convexity-13}
  Assume $C \subseteq \R^{n}$ is convex, closed, and $C \neq
  \emptyset$.  Then $\proj_{C}$ is single-valued - that is, the
  projection of $x$ onto $C$ is unique for every $x \in \R^{n}$.
\end{thm}

\begin{proof}
  $f$ is lsc, level-bounded, and proper, so $\argmin f \neq
  \emptyset$.  $f$ is strictly convex so has at most one minimizer.
\end{proof}

\begin{defn}
  \label{sec:convexity-14}
  For an arbitary set $S \subseteq \R^{n}$,
  \begin{equation}
    \label{eq:10}
    \con S = \bigcap_{\text{$C$ convex, $S \subseteq C$}} C
  \end{equation}
  is the convex hull of $S$.  It is the smallest convex set that
  contains $S$.
\end{defn}

\begin{thm}
  \label{sec:convexity-15}
  $\con S = \{ \sum_{i=0}^{p} \lambda_{i} x_{i} | x_{i} \in S,
  \lambda_{i} \geq 0, \sum_{i=0}^{p} \lambda_{i} = 1, p \geq 0  \} = D$
\end{thm}

\begin{proof}
  $D \subseteq \con S$ as a convex set contains all convex
  combinations of points in $S$, thus $D \subseteq \con S$.

  $\con S \subseteq D$ as taking linear combination of $x, y \in D$,
  we have a convex combination of elements in $D$, which is in $D$, so
  $\con S \subseteq D$.

\end{proof}

\begin{thm}
  \label{sec:convexity-16}
  \begin{enumerate}
  \item $\cl C = \{ x \in \R^{n} | \text{for all open neighbourhoods
      $N$ of $x$, $N \cap C \neq \emptyset$} \}$
  \item $\int C = \{ x \in \R^{n} | \text{there exists an open
      neighbourhood $N$ of $x$ such that $N \subseteq C$} \} $
  \item $\bnd C = \cl C \backslash \int C$.
  \end{enumerate}
\end{thm}

\begin{thm}
  \label{sec:convexity-17}
  $\cl C = \bigcap_{\text{$S$ closed, $C \subseteq S$}} S$.
\end{thm}

\section{Cones and Generalized Inequalities}
\label{sec:cones-gener-ineq}

\begin{defn}
  \label{sec:cones-gener-ineq-1}
  $K \subseteq \R^{n}$ is a cone if and only if $0 \in K$ and $\lambda
  x \in K$ for all $x \in K, \lambda \geq 0$.

  A cone $K$ is pointed if and only if $\sum_{0=1}^{m} x_{i} = 0$,
  $x_{i} \in K$ implies $x_{i} = 0$ for all $i$.
\end{defn}

\begin{thm}
  \label{sec:cones-gener-ineq-2}
  Let $K \subseteq \R^{n}$ be arbitrary. Then the following are equivalent:
  \begin{enumerate}
  \item $K$ is a convex cone.
  \item $K$ is a cone and $K + K \subseteq K$.
  \item $K \neq \emptyset$ and $\sum_{i=0}^{m} \alpha_{i} x_{i} \in K$
    for all $x_{i} \in K$ and $\alpha_{i} \geq 0$ (not necessarily
    summing to 1).
  \end{enumerate}
\end{thm}

\begin{proof}
  $x = \sum_{i=0}^{m} \alpha_{i} x_{i} \in K$, $a_{i} \geq 0 \iff
  \sum_{i=0}^{m} \frac{a_{i}}{\sum_{j}^{} \alpha_{j}} x \in K$, which
  is a convex combination.
\end{proof}

\begin{thm}
  \label{sec:cones-gener-ineq-4}
  Assume $K$ is a convex cone.  Then $K$ is pointed if and only if $K
  \cap -K = \{ 0 \} $.
\end{thm}

\begin{proof}
  $x_{1} + x_{2} + \dots + x_{m} \in K$, $x_{1} \neq 0$, $x_{2} +
  \dots + x_{m} = -x_{1}$, $x_{2} + \dots + x_{m} \in K$, so $x_{1}
  \in K \cap - K$.
\end{proof}

\begin{thm}
  \label{sec:cones-gener-ineq-5}
  For a closed convex cone $K \subseteq \R^{n}$ we define the
  generalized inequality
  \begin{equation}
    \label{eq:11}
    x \leq_{K} y \iff x - y \in K
  \end{equation}

  Then
  \begin{enumerate}
  \item $x \leq_{K} x$
  \item $x \leq_{K} y, y \leq_{K} z \Rightarrow x \leq_{K} z$
  \item $x \leq_{K} y \Rightarrow -y \leq_{K} -x$
  \item $x \leq_{K} y, \lambda \geq 0 \Rightarrow \lambda x \geq_{K}
    \lambda y$
  \item $x \geq_{K} y, x' \geq_{K} y' \Rightarrow x + x' \geq_{K} y + y'$
  \item $x^{k} \rightarrow x$, $y^{k} \rightarrow y$ with $x^{k}
    \geq_{K} y^{K}$ for all $k \in \N$, then $x \geq_{K} y$.
  \item $x \geq_{K} y, y \geq_{K} \Rightarrow x = y$ (antisymmetry)
    holds if and only if $K$ is pointed.
  \end{enumerate}
\end{thm}

\begin{defn}[$K_{n}^{LP}$]
  \label{sec:cones-gener-ineq-6}
  For any pointed, closed, convex cone $K \subseteq \R^{m}$, a matrix
  $A \in \R^{m \times n}$, and vectors $c \in \R^{n}, b \in \R^{m}$,
  define the conic problem $\inf_{x} c^{T} x s.t. Ax \geq_{K} b$.
\end{defn}

\begin{defn}[$K_{n}^{SOCP}$]
  \begin{equation}
    \label{eq:55}
    K_{n}^{SOCP} = \{ x \in \R^{n} \}  | x_{n} \geq \sqrt{x_{1}^{2} +
      \dots + x_{n-1}^{2}}.
  \end{equation}
\end{defn}

\section{Subgradients}
\label{sec:subgradients}

\begin{defn}
  \label{sec:subgradients-1}
  For any $f: \R^{n} \rightarrow \overline \R$ and $x \in \R^{n}$,
  \begin{equation}
    \label{eq:12}
    \partial f(x) = \{ v \in \R^{n} | f(x) + \IP{v, y -x} \leq f(y)
    \forall y \in \R^{n} \}
  \end{equation} is the set of subgradients of $f$ at $x$.
\end{defn}

\begin{thm}
  \label{sec:subgradients-3}
  Assume $f, g: \R^{n} \rightarrow \overline \R$ are convex.  Then
  \begin{enumerate}
  \item If $f$ is differentiable at $x$, then $\partial f(x) = \{
    \grad f(x) \} $
  \item If $f$ is differentiable at $x$ and $g(x) \in \R$, then
    $\partial (f +g)(x) = \partial g(x) + \grad f(x)$.
  \end{enumerate}
\end{thm}

\begin{thm}
  \label{sec:subgradients-4}
  Assume $f: \R^{n} \rightarrow \overline \R$ is proper.  Then $x \in
  \argmin f \iff 0 \in \partial f(x)$.
\end{thm}

\begin{proof}
  \begin{equation}
    \label{eq:57}
    0 \in \partial f(x) \iff f(x) \leq f(y)\forall y \iff x \in
    \argmin f
  \end{equation} where properness was required since $\argmin +\infty
  = \emptyset$ by definition.
\end{proof}

\begin{defn}
  \label{sec:subgradients-5}
  For a convex set $C \subseteq \R^{n}$ and $x \in C$, the normal cone
  $N_{C}(x)$ at $x$ is
  \begin{equation}
    \label{eq:13}
    N_{C}(x = \{ v \in \R^{n} | \IP{v, y -x } \leq 0 \forall y \in C \} )
  \end{equation}  $N_{C}(x) = \emptyset$ for $x \notin C$.

  Note that $N_{C}(x)$ is a cone if $x \in C$.
\end{defn}

\begin{thm}
  \label{sec:subgradients-6}
  Assume $C \subseteq \R^{n}$ is convex with $C \neq \emptyset$.  Then
  $\partial \delta_{C}(x) = N_{C}(x)$.
\end{thm}

\begin{proof}
  For $x \in C$, this follows easily, and for $x \notin C$, choosing
  $y \in C$ shows that $\partial \delta_{C}(x) = \emptyset$.
\end{proof}

\begin{thm}
  \label{sec:subgradients-7}
  Assume $C \subseteq \R^{n}$ is closed and convex with $C \neq
  \emptyset$ and $x \in \R^{n}$.  Then $y \in \proj_{C}(x) \iff x - y
  \in N_{C}(y)$.
\end{thm}

\begin{proof}
  Follows from $y$ being the unique minimizer of $f(y') = \frac{1}{2}
  \| y' - x \|_{2}^{2} + \delta_{C}(y')$.
\end{proof}

\begin{thm}
  \label{sec:subgradients-8}
  Assume $f: \R^{n} \rightarrow \overline \R$ is proper and convex.
  Then
  \begin{equation}
    \label{eq:14}
    \partial f(x) =
    \begin{cases}
      \emptyset & x \notin \dom f \\
      \{ v \in \R^{n} | (v, -1) \in N_{\epi f}(x, f(x)) \} & x \in
      \dom f
    \end{cases}
  \end{equation}

  If $x \in \dom f$ then $N_{\dom f}(x) = \{ v \in \R^{n} | (v, 0) \in
  N_{\epi f}(x, f(x)) \} $.
\end{thm}

\begin{proof}
  $x \notin \dom f$: choose $f(y) < \infty$, then $\partial f(x) =
  \emptyset$.

  $x \in \dom f$: $v^{T}(y-x) + (-1)(- f(x)) \leq 0 \forall (y,
  \alpha) \in \epi f \iff (v, -1) \in N_{\epi f}(x, f(x))$.
\end{proof}

\begin{defn}
  \label{sec:subgradients-10}
  For any set $C \subseteq \R^{n}$, define the affine hull and relative
  interior by
  \begin{align}
    \label{eq:15}
    \aff C = \cap_{\text{$A$ affine, $C \subseteq A$}} A \\
    \rint C = \{ x \in \R^{n} | \text{there exists an open
      neighbourhood $N$ of $x$ with $N \cap \aff C \subseteq C$} \}.
    \label{eq:16}
  \end{align}
\end{defn}

\begin{thm}
  \label{sec:subgradients-11}
  \begin{enumerate}
  \item Assume $f: \R^{n} \rightarrow \overline \R$  is convex. Then
    \begin{align}
      \label{eq:17}
      g(x) = f(x+y) \Rightarrow \partial g(x) = \partial f(x+y) \\
      g(x) = f(\lambda x) \Rightarrow \partial g(x) = \lambda \partial
      f(\lambda x), \lambda \neq 0 \\
      \label{eq:19}
      g(x) = \lambda f(x) \Rightarrow \partial g(x) = \lambda \partial
      f(x), \lambda > 0 \\
    \end{align}
  \item Assume $f: \R^{n} \rightarrow \overline \R$ is proper and
    convex, and $A \in \R^{n \times m}$ is such that
    \begin{equation}
      \label{eq:21}
      \{ Ay | y \in \R^{m} \}  \cap \rint \dom f
    \end{equation}
    If $x \in \dom (f \circ A) = \{ y \in \R^{m} | Ay \in \dom f \} $,
    then $\partial (f \circ A)(x) = A^{T} \partial f(Ax)$.
  \item Assume $f_{0}, \dots, f_{m}: \R^{n} \rightarrow \overline \R$
    are proper and convex, and $\rint \dom f_{0} \cap \dots \cap \rint
    \dom f_{m} \neq \emptyset$.  If $x \in \dom f$, then
    $\partial(\sum_{i=0}^{m} f_{i})(x) = \sum_{i=0}^{m} \partial f_{i}(x)$.
  \end{enumerate}
\end{thm}

\section{Conjugate Functions}
\label{sec:conjugate-functions}

\begin{defn}
  \label{sec:conjugate-functions-1}
  For $f: \R^{n} \rightarrow \overline \R$, $\con f(x) =\sup_{\text{$g
      \leq f$, $g$ convex}}$ is the convex hull of $f$.
  $\con f$ is the greatest convex function majorized by $f$.
\end{defn}

\begin{defn}
  \label{sec:conjugate-functions-2}
  For $f: \R^{n} \rightarrow \overline \R$, the \textbf{lower closure}
  $\cl f$ is defined as $(\cl f)(x) = \liminf_{y \rightarrow x} f(y)$.
  Alternatively, $(\cl f)(x) = \sup_{\text{$g \leq f$, $g$ lsc}} g(x)$.
\end{defn}

\begin{thm}
  \label{sec:conjugate-functions-3}
  For $f: \R^{n} \rightarrow \overline \R$, we have $\epi (\cl f) =
  \cl (\epi f)$.  Moreover, if $f$ is convex then $\cl f$ is convex.
\end{thm}

\begin{proof}
  $(x, \alpha) \in \cl(\epi f) \iff \exists x^{k} \rightarrow x,
  \alpha^{k} \rightarrow \alpha, f(x^{k} \leq \alpha^{k}) \iff
  \liminf_{y \rightarrow x} f(y) \leq \alpha \iff (x, \alpha) \in \epi
  (\cl f)$.
\end{proof}

\begin{thm}
  \label{sec:conjugate-functions-4}
  Assume $C \subseteq \R^{n}$ is closed and convex.  Then $C =
  \bigcap_{(b, \beta), C \subseteq H_{b, \beta}} H_{b, \beta}$ where
  $B_{b, \beta} = \{ x \in \R^{n} | \IP{x, b} - \beta \leq 0 \} $
\end{thm}

\begin{proof}
  Separating hyperplane theorem - $x \in C$ then $x$ is the
  intersection. If $x \notin C$ consider the separating hyperplane of
  $C$ and $\{ x \} $.
\end{proof}

\begin{thm}
  \label{sec:conjugate-functions-5}
  Assume $f: \R^{n} \rightarrow \overline \R$ is proper, lsc, and
  convex.  Then $f(x) = \sup_{\text{$g$ affine, $f \leq f$}} g(x)$.
\end{thm}

\begin{proof}
  $f$ lsc implies $\epi f$ is closed, $f$ is convex implies $\epi f$ is
  convex, so $\epi $ f is the intersection of all half spaces
  containing it.  Show $g$ affine $\iff$ there exists $(b, c), \beta$
  with $c < -$, $\epi g = H_{(b, c), \beta}$.

  Then as $\epi (\sup_{g \leq f} g(x)) = \cap_{g \leq f} \epi g$
  where $g$ is affine and $g \leq f \iff \epi f \subseteq \epi g$, we
  need only show $I_{1} = \cap_{(b, c), \beta \in S} H_{(b, c), \beta}
  = \cap_{(b, c), \beta \in S, c < 0} H_{(b, c), \beta} = I_{2}$.
\end{proof}

\begin{defn}
  \label{sec:conjugate-functions-6}
  Let $f: \R^{n} \rightarrow \overline \R$, then
  \begin{align}
    \label{eq:8}
    f^{\star}: \R^{n} \rightarrow \overline \R \\
    f^{\star}(v) = \sup_{x \in \R^{n}} \IP{v, x} - f(x)
  \end{align} is the \textbf{conjugate to $f$}.  The mapping $f
  \mapsto f^{\star}$ is the Legendre-Fenchel transform.
\end{defn}

\begin{thm}
  \label{sec:conjugate-functions-7}
  Assume $f: \R^{n} \rightarrow \overline \R$.  Then $f^{\star} =
  (\con f)^{\star} = (\cl f)^{\star} = (\cl \con f)^{\star}$ and
  $f^{\star \star} = (f^{\star})^{\star} \leq f$ .  If $\con f$ is
  proper, then $f^{\star}$ and $f^{\star \star}$ are proper, lsc, and
  convex, and $f^{\star \star} = \cl \con f$.  If $f: \R^{n}
  \rightarrow \overline \R$ is proper, lsc, and convex, then $f^{\star
  \star} = f$.
\end{thm}

\begin{proof}
  $(v, \beta) \in \epi f^{\star} \iff (v, x) - \beta \leq f(x) \forall
  x$ so $(v, \beta) \in \epi f\star$ define all affine functions
  majorized by $f$.  Thus for every affine function $h$, $h \leq f
  \iff h \leq \cl f \iff h \leq \con f \iff h \leq \cl \con f$.  Which
  gives our result.

  For the inquality, expand $f^{\star \star}(y) = \sup_{v} \IP{v, y} +
  \inf_{x}( f(x) - \IP{v, x}) \leq f(y)$ at $y = x$.

  As $\con f$ is proper, then $\cl \con f$ is proper, lsc, and
  convex, so $\cl \con f = \sup_{g \leq \cl \con f} g(x)$ where $g$ is
  affine,which equalis $f^{\star \star}$ by definition.

  Finally, if $f$ is convex, then $\con f = f$, so $\con f$is proper,
  and $\con f = f$ is lsc, so $f^{\star \star} = \cl \con f = \cl f = f$.
\end{proof}

\begin{thm}
  \label{sec:conjugate-functions-8}
  Assume $f: \R^{n} \rightarrow \overline \R$.  Then
  \begin{enumerate}
  \item $\con f$ is not proper implies $f^{\star} \equiv +\infty$ or
    $f^{\star} \equiv -\infty$.
  \item In particular, $f^{\star}$ proper implies $\con f$ is proper.
  \end{enumerate}
\end{thm}

\begin{proof}
  If $\con f = \infty$ , then $f^{\star} (v) = -\infty$.  If $\con
  f(x') = -\infty$, then $f^{\star}(v) = (\con f)^{\star}(v) \geq +\infty$.
\end{proof}

\begin{thm}
  \label{sec:conjugate-functions-9}
  Assume $f: \R^{n} \rightarrow \overline \R$ is proper, lsc, and
  convex.  Then $\partial f^{\star} = (\partial f)^{-1}$,
  specifically,
  \begin{equation}
    \label{eq:20}
    v \in \partial f(x) \iff f(x) + f^{\star}(v) = \IP{v, x} \iff x
    \in \partial f^{\star}(v).
  \end{equation}
  Moreover,
  \begin{align}
    \label{eq:22}
    \partial f(x) = \argmax_{v'} \IP{v', x} - f^{\star}(v')
    \partial f^{\star}(x) = \argmax_{x'} \IP{v, x'} - f(x)
  \end{align}
\end{thm}

\begin{proof}
  $f(x) + f^{\star}(v) = \IP{v, x}$ iff $x \in \argmax_{x'} \IP{v, x'}
  - f(x') \iff v \in \partial f(x)$.
\end{proof}

\begin{thm}
  \label{sec:conjugate-functions-10}
  For a proper, lsc, convex $f: \R^{n} \rightarrow \overline \R$, we
  have
  \begin{align}
    \label{eq:23}
    (f(\cdot) - \IP{a, \cdot})^{\star} = f^{\star}(\cdot + a) \\
    \label{eq:24}
    (f(\cdot + b))^{\star} = f^{\star}(\cdot) - \IP{\cdot, b}, \\
    \label{eq:25}
    (f(\cdot) + c)^{\star} = f^{\star}(\cdot) - c \\
    \label{eq:26}
    (\lambda f(\cdot))^{\star} = \lambda
    f^{star}(\frac{\cdot}{\lambda}), \lambda > 0 \\
    \label{eq:27}
    (\lambda f(\frac{\cdot}{\lambda} ))^{\star} = \lambda
    f^{\star}(\cdot), \lambda > 0
  \end{align}
\end{thm}

\begin{thm}
  \label{sec:conjugate-functions-11}
  Let $f_{i}: \R^{n_{i}} \rightarrow \overline \R$, $i = 0, \dots, m$
  be proper and $f(x_{0}, \dots, x_{m}) = \sum_{i=0}^{m}
  f_{i}(x_{i})$.  Then $f^{\star}(v_{1}, \dots, v_{m}) = \sum_{i=0}^{m} f^{\star}_{i}(v_{i})$.
\end{thm}

\begin{proof}
  For support functions, $f^{\star}(x) = \sigma_{C}(x)$, so $C$
  nonempty closed convex implies $f^{\star}$ is proper lsc convex with
  $f^{\star}(0) = 0$ and $f^{\star}(\lambda v) = \lambda
  f^{\star}(v)$, so $f^{\star} = \sigma_{C}$ is positively homogenous.

  If $g$ is positively homogenous lower semincontinuous, $g^{\star}(x)
  \in \{ 0, \infty \} $, so $g^{\star}$ is an indicatior function,
  which must be convex and nonempty by properness lsc convexity of
  $g^{\star}$.

  One to one follows from $\delta^{\star \star}_{C} = \delta_{C}$.

  For cones, show $g$ is positively homogenous lsc convex proper
  indicator function $\iff$ $g = \delta_{K}$ for $K$a convex closed
  cone.

  Taking any such indicator function $\delta_{K}$, then
  $\delta_{K}^{\star}$ is an indicator function, with
  $\delta_{K}^{\star}(v) < \infty \iff v \in K^{\star}$.
\end{proof}

\begin{defn}
  \label{sec:conjugate-functions-12}
  For any set $S \subseteq \R^{n}$ define the \textbf{support
    function} $\supp_{S}(v) = \sup_{x \in S} \IP{v, x} = (\delta^{\star}_{S})(v)$
\end{defn}

\begin{defn}
  \label{sec:conjugate-functions-13}
  A function $f: \R^{n} \rightarrow \overline \R$ is said to be
  positively homogenous if and only if $0 \in \dom f$ and $f(\lambda
  x) = \lambda f(x)$ for all $x \in \R^{n}$ and $\lambda > 0$.
\end{defn}

\begin{thm}
  \label{sec:conjugate-functions-14}
  The set of positively homogenous proper lsc convex functions and the
  set of closed convex nonempty sets are in one-to-one correspondence
  through the Legendre-Fenchel transform:
  \begin{align}
    \label{eq:28}
    \delta_{C} \leftrightarrow_{\star} \supp_{C} \\
    \label{eq:29}
    x \in \partial \supp_{C}(v) \iff x \in C \\
    \label{eq:30}
    \supp_{C}(v) = \IP{v, x} \iff v \in N_{C}(x).
  \end{align}

  In particular, the set of closed convex cones is in one-to-one
  correspondence with itself - for any cone $K$ define the
  \textbf{polar cone} or \textbf{dual cone} as $K^{\star} = \{ v \in
  \R^{d} | \IP{v, x} \leq 0 \forall x \in K \} $.  Then
  \begin{align}
    \label{eq:31}
    \delta_{K} \leftrightarrow_{\star} \delta_{K^{\star}} \\
    \label{eq:32}
    x \in N_{K^{\star}}(v) \iff v \in N_{K}(x) % \iff x \in K, v \in
    % K^{\star}, \IP{x, v} = 0 \iff 0 \leq_{K} x \perp v
    % \geq_{K^{\star}} 0
  \end{align}
  TODO: fill next condition/implication in.
\end{thm}

\section{Duality in Optimization}
\label{sec:duality-optimization}

\begin{defn}
  \label{sec:duality-optimization-1}
  Assume $f: \R^{n} \times \R^{m} \rightarrow \overline \R$ is proper,
  lsc, and convex.  Define the \textbf{primal problem} as
  \begin{equation}
    \label{eq:33}
    \inf_{x \in \R^{n}} \phi(x), \phi(x) = f(x, 0),
  \end{equation} the \textbf{dual problem} as
  \begin{equation}
    \label{eq:34}
    \sup_{y \in \R^{n}} \psi(y), \psi(y) = -f^{\star}(0, y)
  \end{equation}
  and the \textbf{inf-projections}
  \begin{align}
    \label{eq:35}
    p(u) = \inf_{x} f(x, u) \\
    \label{eq:36}
    q(v) = \inf_{y} f^{\star}(v, y) = -\sup_{y} (-f^{\star}(v, y))
  \end{align}
  $f$ is sometimes called a \textbf{peturbation function} for $\psi$,
  and $p$ the associated \textbf{marginal function}.
\end{defn}

\begin{thm}
  \label{sec:duality-optimization-2}
  Assume $f: \R^{n} \times \R^{m} \rightarrow \overline \R$ is proper,
  lsc, and convex.  Then
  \begin{enumerate}
  \item $\phi$ and $-\psi$ are lsc and convex.
  \item $p, q$ are convex.
  \item $p(0)$ and $p^{\star \star}(0)$ are the optimal values of the
    primal and dual problems -
    \begin{align}
      \label{eq:37}
      p(0)  = \inf_{x} \phi(x), p^{\star \star}(0) = \sup_{y} \psi(y).
    \end{align}
  \item The primal and dual problems are feasible if and only if their
    associated marginal function contains 0:
    \begin{align}
      \label{eq:38}
      \inf_{x} \phi(x) < \infty \iff 0 \in \dom p \\
      \label{eq:39}
      \sup_{y} \psi(y) > -\infty \iff 0 \in \dom q
    \end{align}
  \end{enumerate}
\end{thm}

\begin{proof}
  $f$ proper lsc convex implies $f^{\star}$ is proper lsc convex
  implies $\pi, \psi$ lsc convex.

  $p, q$ are convex from the strict epigraph set, with $E = \{ (u, a)
  \in \R^{m } \times \R | p(u) = \inf_{x \in \R^{n}} f(x, u) < \alpha
  \} = A(E')$, where $A$ is the lnear projection mapping $A(x, u,
  \alpha) = (u, \alpha)$, and $E'$ is the strict epigraph of $f$ and
  thus convex, so $A(E')$ is convex.

  $p^{\star}(y) = -\psi(y)$, so $p^{\star \star}(0) = \sup_{y} \psi(y)$ .

  $0 \in \dom p \iff p(0) < \infty \iff \inf_{x} f(x, 0) < \infty \iff
  \inf \psi < \infty$.
\end{proof}

\begin{thm}
  \label{sec:duality-optimization-4}
  Assume $f: \R^{n} \times \R^{m} \rightarrow \overline \R$ is proper,
  lsc, and convex. Then \textbf{weak duality} always holds,
  \begin{equation}
    \label{eq:40}
    \inf_{x} \phi(x) \geq \sup_{y} \psi(y),
  \end{equation} and under certain conditions the infimum and supremum
  are equal and finite - \textbf{strong duality}
  \begin{equation}
    \label{eq:41}
    p(0) \in \R, \text{$p$ lsc in $0$} \iff \inf_{x} \phi(x) =
    \sup_{y} \psi(y) \in \R.
  \end{equation}
  The difference $\inf \phi - \sup \psi$ is the \textbf{duality gap}.
\end{thm}

\begin{proof}
  $\inf_{x} \phi(x) = p(0) \geq p^{\star \star}(0) = \sup_{y} \psi(y)$
  so must show $p(0) \in \R$ and $p$ lsc in $0$ if and only if $p(0) =
  p^{\star \star}(0) \in \R$.

  $(\Rightarrow)$ follows as $p^{\star \star}(0) \leq \cl p(0) \leq
  p(0)$, so $\liminf_{y \rightarrow 0} p(y) = \cl p(0) = p(0) \in \R$,
  so $p$ is lsc in $0$.

  $(\Leftarrow)$ follows from the claim thatif it holds then $\cl p$
  is proper lsc convex.  Convexity, lsc is clear, and an improper
  convex lsc function is always constant $\infty$ or $-\infty$,
  contradiction $p(0) \in \R$. So $(p^{\star})^{\star}(0) = ((\cl
  p)^{\star})^{\star}(0) = \cl p(0) = p(0)$.
\end{proof}

\begin{thm}
  \label{sec:duality-optimization-5}
  Assume $f: \R^{n} \times \R^{m} \rightarrow \overline \R$ is proper,
  lsc, and convex.  Then we have the \textbf{primal-dual optimality
    conditions},
  \begin{align}
    \label{eq:42}
    (0, y') \in \partial f(x', 0) \iff \{ x' \in \argmin_{x} \phi(x),
    y' \in \argmax_{y} \psi(y), \inf_{x} \psi(x) = \sup_{y} \psi(y) \}
    \iff (x', 0) \in \partial f^{\star}(0, y').
  \end{align}

  The set of \textbf{primal-dual optimal points} $(x', y')$ satisfying
  this equation is either empty or equal to $(\argmin \phi) \times
  (\argmax \psi)$.
\end{thm}

\begin{proof}
  Follows from invertibility of sugbradient in terms of conjugate
  functions, showing $f(x', 0) = -f^{\star}(0, y') \iff \phi(x') =
  \psi(y') \in \R$, and equality with infinite value is explicitly
  excluded by definition of $\argmin$.
\end{proof}

\begin{thm}
  \label{sec:duality-optimization-6}
  Assume $f: \R^{n} \times \R^{m} \rightarrow \overline \R$ is proper,
  lsc, and convex. Then
  \begin{enumerate}
  \item\label{item:1} $0 \in \interior \dom p$ or $0 \in \interior \dom q$ implies
    $\inf_{x} \phi(x) = \sup_{y} \psi(y)$. $(S')$
  \item\label{item:2} $0 \in \interior \dom p$ and $0 \int \interior \dom q$ implies
    $\inf_{x} \phi(x) = \sup_{y} \psi(y) \in \R$. $(S)$
  \item\label{item:3} $0 \in \int \dom p$ and $\inf_{x} \phi(x) \in \R$ if and only
    if $\argmax_{y} \psi(y)$ is nonempty and bounded. $(P)$
  \item\label{item:4} $0 \in \int \dom q$ and $\sup_{y} \psi(y) \in \R$ if and only
    if $\argmin_{x} \phi(x)$ is nonempty and bounded. $(D)$
  \end{enumerate}
  In particular, if any of $S, P, D$ hold, then strong duality holds -
  $\inf \phi = \sup \psi \in \R$. If $S$, or ($P$ and $D$) hold, then
  there exists $x', y'$ satisfying the primal-dual optimality
  conditions. Also, $P$ implies $\partial p(0) = \argmax_{y}
  \psi(y)$, and $D$ implies $\partial q(0) = \argmin_{x}
  \phi(x)$.
\end{thm}

\begin{proof}
  \ref{item:1}: If $p(0) = -\infty$, then $p^{\star \star}(0) \leq
  p(0) = -\infty$ Use $\cl p = p$ on $\int \dom p$, so $\sup \psi =
  \inf \psi = -\infty$.  Otherwise, $p(0) \in \R$ so as $\cl p = p$ on
  $\int \dom p$, we have $p$ is lsc in 0 and so $p^{\star \star}(0) =
  p(0) \in \R$.

  This follows symmetrically on $f'(x, y) = f^{\star}(y, x)$.

  If both $0 \in \int \dom p,  0 \in \int \dom q$, then $+\infty >
  p(0) \geq p^{\star \star}(0) = \sup \psi = -q(0) > -\infty$, which
  is finite.

  Nonemptyness and boundedness follows from $0 \in \int \dom p$ if and
  only if $\psi $ is proper lsc convex and level bounded.

  Subdifferential: if \ref{item:3} then $p(0) \in \R$, so $\cl p(0) =
  p(0) \in \R$, $\cl p$ is then proper and lsc convex, so $\partial
  (\cl p)(0) = \arg \max \psi$, but $\partial p(0) = \partial (\cl
  p)(0)$.
\end{proof}

\begin{thm}
  \label{sec:duality-optimization-7}
  Assume $k: \R^{n} \rightarrow \overline \R$ and $h: \R^{m}
  \rightarrow \overline \R$ are both proper, lsc, convex, and $A \in
  \R^{m \times n}, b \in \R^{m}, c \in \R^{n}$. For $f(x, u) = \IP{c,
    x} + k(x) + h(Ax - b + u)$, the primal and dual problems are of
  the form
\begin{align}
  \label{eq:43}
  \inf \phi(x), \phi(x) = \IP{c, x} + k(x) + h(Ax - b) \\
  \label{eq:44}
  \sup_{y} \psi(y), \psi(y) = -\IP{b, y} - h^{\star}(y) -
  k^{\star}(-A^{T}y - c)
\end{align} with
\begin{align}
  \label{eq:45}
  \int \dom p = \int (\dom h - A \dom k) + b \\
  \label{eq:46}
  \int \dom q = \int(\dom k^{\star} - (-A^{T}) \dom h^{\star}) + c
\end{align}
and optimality conditoins
\begin{align}
  \label{eq:47}
  \{ -A^{T} y' - c \in \partial k(x'), y' \in \partial h(Ax' - b) \}
  \\
  \iff \{ x' \in \argmin_{x} \phi(x), y' \in \argmax_{y} \psi(y),
  \inf_{x} \phi(x) = \sup_{y} \psi(y) \} \\ \iff \{ Ax' - b \in \partial
  h^{\star}(y'), x' \in \partial k^{\star}(-A^{T} y' - c) \}
\end{align}
\end{thm}

\begin{thm}
  \label{sec:duality-optimization-8}
  Assume $f: \R^{n} \times \R^{m} \rightarrow \overline \R$ is proper,
  lsc, convex.  Define the associated \textbf{Lagrangian} as $l(x, y)
  = -f(x, \cdot)^{\star}(y)$, so $l(x, y) = \inf_{u}(f(x, u) - \IP{y,
    u})$.  Then $l(\cdot, y)$ is convex for every $y$, $-l(x, \cdot)$
  is lsc and convex for every $x, y$ and $f(x, \cdot) = (-l(x,
  \cdot))^{\star}$, and $(v, y) \in \partial f(x, u) \iff v
  \in \partial_{x} l(x, y) \text{ and } u \in \partial_{y}(-l)(x, y)$.
\end{thm}

\begin{proof}
  Consider $g(x, y, u) = f(x, u) - \IP{y, u}$, which is proper lsc
  convex. Then $l(\cdot, y) = \inf_{u} g(\cdot, y, u)$ is convex.Then
  $f_{x}(y) = f(x, y)$, then $-l(x, \cdot) = f^{\star}_{x}(\cdot)$ so
  $f_{x}$ is either $+\infty$ or proper lsc convex, and $-l(x, \cdot)$
  is eiather $-\infty$ or proper lsc convex, but always lsc convex.

  By subgradient definitino, $(v, y) \in \partial f(x, u) \iff
  \inf_{u'} f(x', u') - \IP{y, u'} \geq f(x, u) - \IP{y, u} + \IP{v,
    x' - x} \forall x'$, which evaluated at $x' = x$ impilies
  $\inf_{u'} f(x, u') - \IP{y, u'} = f(x, u) - \IP{y, u}$.

  Continuing, we obtain $(v, y) \in \partial f(x, u) \iff y
  \in \partial_{u} f(x, u),  \in \partial_{x} l(x, y)$, and the first
  condition is equlivanet to $u \in \partial f^{\star}_{x}(y)$, or $u
  \in \partial (- l(x, \cdot))^{\star \star} = \partial_{y}(-l)(x, y)$
  as required.
\end{proof}

\begin{defn}
  \label{sec:duality-optimization-9}
  For any function $l: \R^{n} \times \R^{m} \rightarrow \overline \R$
  we say that $(x', y')$ is a \textbf{saddle point} of $l$ if $l(x,
  y') \geq l(x', y') \geq l(x', y)$ for all $x, y$. The set of all
  saddle points is denoted by $\saddlepoint l$.

  Evquivalently, $(x', y') \in \saddlepoint l$ if $\inf_{x} l(x, y') = l(x',
  y') = \sup_{y} l(x', y)$.
\end{defn}

\begin{thm}
  \label{sec:duality-optimization-10}
  Assume $f$ is proper, lsc, and convex with associated Lagrangian $l$.
  Then $\phi(x) = \sup_{y} l(x, y)$, and $\psi(y) = \inf_{x} l(x, y)$,
  and the primal problem is $\inf_{x} \phi(x) = \inf_{x} \sup_{y} l(x,
  y)$, and the dual problem is $\sup_{y} \psi(y) = \sup_{y} \inf_{x}
  l(x, y)$.  Moreove, the optimality condition
  \begin{align}
    \label{eq:49}
    \{ x' \in \argmin_{x} \phi(x), y' \in \argmax_{y} \psi(y),
    \inf_{x} \phi(x) = \sup_{y} \psi(y) \}
    \\ \iff (x', y') \in \saddlepoint l
    \\ \iff \{ 0 \in \partial_{x} l(x', y'), 0 \in \partial_{y}(-l)(x', y') \}
  \end{align}
\end{thm}

\begin{thm}
  \label{sec:duality-optimization-11}
  Assume $X \subseteq \R^{n}$, $Y \subseteq \R^{m}$ are nonempty,
  closed, convex, and $L: X \times Y \rightarrow \R$ is a continuous
  function with $L(\cdot, y)$ is convex for every $y$ and $-L(x,
  \cdot)$ convex for every $x$.  Then $l(x, y) = L(x, y) +
  \delta_{X}(x) - \delta_{Y}(y)$ with the convention $+\infty - \infty
  = +\infty$ on the right, is the Lagrangian to $f(x, u) = \sup_{y}
  l(x, y) + \IP{u, y} = (-l(x, \cdot))^{\star}(u)$.

  $f$ is proper, lsc, and convex, so the previous result applies with
  promal and dual problems $\phi(x) = \delta_{X}(x) + \sup_{y \in Y}
  L(x, y)$, $\psi(y) = -\delta_{Y}(y) + \int_{x \in X} L(x, y)$.
  Moreover, if $X$ and $Y$ are bounded, then $\saddlepoint l$ is nonempty and bounded.
\end{thm}

\begin{proof}
  For a fixed $y$, $\inf_{x} l(x, y) = -f^{\star}(0, y) = \psi(y)$,
  and for a fixed $x$, $\sup_{y} l(x, y) = f(x, 0) = f^{\star
    \star}_{x}(0) = f(x, 0) = \phi(x)$ as $f_{x}$ is either proper lsc
  convex or $+\infty$.

  The optimality condition is equivalent to $(0, y') \in f(x', 0) \iff
  0 \in \partial_{x} l(x', y'), 0 \in \partial_{y}(-l)(x', y')$, which
  is the saddle point condition.
\end{proof}

\section{Numerical Optimality}
\label{sec:numerical-optimality}

\begin{defn}
  \label{sec:numerical-optimality-1}
  For $\phi: \R^{n} \rightarrow \overline \R$, a point $x$ is an
  $\epsilon$-optimal solution if $\phi(x) - \inf \phi \leq \epsilon$.
\end{defn}

\begin{defn}
  \label{sec:numerical-optimality-2}
  Assume $(x^{k}, y^{k})$ is a primal-dual feasible pair - so $x^{k}
  \in \dom \phi$ and $y^{k} \in \dom \psi$.  Then $\phi(x^{k}) \geq
  \psi(y^{k})$, and $0 \leq \phi(x^{k}) - \inf \phi \leq \phi(x^{k}) -
  \psi^{y^{k}} = \gamma(x^{k}, y^{k}) := \gamma$.  $\gamma$ is the
  \textbf{numerical primal-dual gap}.  If $\gamma < \epsilon$ then
  $x^{k}$ is an $\epsilon$-optimal solution with \textbf{optimality
    certificate} $y^{k}$.

  The normalized gap is $\overline \gamma = \overline \gamma(x^{k},
  y^{k}) = \frac{\phi(x^{k}) - \psi(y^{k})}{\psi(y^{k})}$.
\end{defn}

\begin{defn}
  \label{sec:numerical-optimality-3}
  Assume $\phi(x) = \phi_{0}(x) = \sum_{i=1}^{n_{p}} \delta_{g_{i}(x)
    \leq 0}$, $\psi(y) = \psi_{0}(y) - \sum_{i=1}^{n_{d}}
  \delta_{h_{i}(x) \leq 0}$ where $\dom \phi_{0} = \dom \psi_{0} =
  \R^{n}$ and $g_{i}: \R^{n} \rightarrow \R$, $h_{i}: \R^{m}
  \rightarrow \R$ are suitable continuous real-valued convex
  functions, so the primal and dual constraints are of the form
  $g_{i}(x) \leq 0, h_{i}(y) \leq 0$. Then the primal and dual
  infeasibilities are defined as $\eta_{p} = \max \{ 0, g_{1}(x^{k}),
  \dots, g_{n_{p}}(x^{k}) \} $ and $\eta_{d} = \max \{ 0,
  h_{1}(y^{k}), \dots, h_{n_{d}}(y^{k}) \} $.
\end{defn}

\section{First-Order Methods}
\label{sec:first-order-methods}

\begin{defn}
  \label{sec:first-order-methods-1}
  For $f: \R^{n} \rightarrow \overline \R$, we define
  \begin{enumerate}
  \item The \textbf{forward step}, $F_{\tau_{k} f}(x^{k}) = (I -
    \tau_{k} \partial f) x^{k}$
  \item The \textbf{backward step}, $B_{\tau_{k}f}(x^{k}) = (I +
    \tau_{k} \partial f)^{-1} x^{k}$.
  \end{enumerate}
\end{defn}

\begin{thm}
  \label{sec:first-order-methods-2}
  If $f: \R^{n} \rightarrow \overline \R$ is proper lsc convex with
  $\tau > 0$, then the backward step is $B_{\tau f}(x) = \argmin_{y}
  \{ \frac{1}{2} \| y - x \|_{2}^{2} + \tau f(y) \} $ and is therefore unique.
\end{thm}

\begin{proof}
  $y \in B_{\tau f}(x) \iff 0 \in y - x + \tau \partial f(y) \iff y
  \in \argmin_{y'} \{ \frac{1}{2} \| y' - x \|_{2}^{2} + \tau f(y') \}$
\end{proof}

\begin{thm}
  \label{sec:first-order-methods-3}
  Assume $f$ is proper lsc convex and $\argmin f \neq \emptyset$. The
  \textbf{forward step} is $x^{k+1} \in F_{\tau_{k} f}(x^{k})$. The
  sequence is not unique, can get stuck if $x^{k}$ is infeasible.

  The \textbf{backward step} is $x^{k+1} = B_{\tau_{k} f}(x^{k})$ -
  which is a \textbf{unique} sequence, and cannot get stuck.  Substeps
  are as hard as the original problem (but strictly convex).
\end{thm}

\begin{enumerate}
\item Forward stepping: $x^{k+1} \in F_{\tau_{k} f}(x^{k})$:
\item Backward stepping: $x^{k+1} = B_{\tau_{k} f}(x^{k})$.
\end{enumerate}

If $f = g + h, \partial f = \partial g + \partial h$ with $f, g, h$
proper lsc convex, $\argmin f \neq \emptyset$, we can do:

\begin{enumerate}
\item Backward-Backward stepping: $x^{k+1} = B_{\tau_{k} h}
  B_{\tau_{k} g}(x^{k})$.
\item Forward-Backward stepping: $x^{k+1} \in B_{\tau_{k} h}
  F_{\tau_{k} g}(x^{k})$.  If $f(x) = g(x) + \delta_{C}(x)$, $g$
  differentiable, $C \neq \emptyset$ closed and convex, then $x^{k+1}
  \in \argmin_{x} \{ \frac{1}{2}  \| y - (x^{k} - \tau_{k} \Delta
  g(x^{k})) \|_{2}^{2} + \delta_{C}(x) \} = \proj_{C}(x^{k} - \tau_{k}
  \Delta g(x^{k}))$, which is a gradient projection.
\end{enumerate}

\section{Interior-Point Methods}
\label{sec:inter-point-meth}

\begin{defn}
  \label{sec:inter-point-meth-1}
  For a cone $K$ we define the \textbf{canonical barriers} $F = F_{K}$
  and associated parameters $\theta_{F}$.
  \begin{enumerate}
  \item $K = K^{LP}_{n} = \{ x \in \R^{n} | x_{1}, \dots, x_{n} \geq 0
    \} $, $F(x) = \sum_{i=1}^{n} -\log x_{i}$, $\theta_{F} = n$.
  \item $K = K_{n}^{SOCP} = \{ x \in \R^{n} | x_{n} \geq
    \sqrt{x_{1}^{2} + \cdots + x_{n-1}^{2}}\}$, $F(x) =
    -\log(x_{n}^{2} - x_{1}^{2} - \dots - x_{n-1}^{2})$, $\theta_{F} =
    2$.
  \item $K = K_{n}^{SDP} = \{ X \in \R^{n \times n} | \text{$X$
      symmetric positive semidefinite} \}$, $F(x) = - \log \det X$,
    $\theta_{F} = n$.
  \item $K = K^{1} \times K^{2}$, then $F_{K}(x^{1}, x^{2}) =
    F_{K^{1}}(x^{1}) + F_{K^{2}}(x^{2})$, with $\theta_{F} =
    \theta_{F^{1}} + \theta_{F^{2}}$.
  \end{enumerate}
\end{defn}

\begin{thm}
  \label{sec:inter-point-meth-2}
  If $F$ is a canonical barrier for $K$, then $F$ is smooth on $\dom F
  = \int K$ and strictly convex, $F(tx) = F(x) - \theta_{F} \log t$
  for all  $x \in \dom F$, and for $x \in \dom F$, we have
  \begin{enumerate}
  \item $- \grad F(x) \in \dom F$
  \item $\IP{\grad F(x), x} = - \theta_{F}$,
  \item $-\grad F(-\grad F(x)) = x$,
  \item $-\grad F(tx) = -\frac{1}{t} \grad F(x)$
  \end{enumerate}
\end{thm}

\begin{proof}
  Differentiate with respect to $t$ and let $t = 1$.
\end{proof}

\begin{thm}
  \label{sec:inter-point-meth-3}
  The \textbf{primal central path} is the mapping $t \mapsto x(t) =
  \argmin \{ -t \IP{b, y} + F(y) + \delta_{A^{T}y = c} \} $

  The \textbf{dual central path} is the mapping $t \mapsto y(t) =
  \argmin \{ -t \IP{b, y} + F(y) + \delta_{A^{T}y = c} \} $

  The \textbf{primal-dual central path} is the mapping $t \mapsto z(t)
  = (x(t), y(t))$ for some $t > 0$, if and only if
  \begin{align}
    \label{eq:52}
    Ax - b \in \dom F \\
    \label{eq:53}
    A^{T} y = c \\
    \label{eq:54}
    ty + \grad F(Ax - b) = 0
  \end{align}
\end{thm}

\begin{proof}
  $y = -\frac{1}{y} \Delta f(Ax - b) \in \dom F$ as $Ax - b \in \dom
  F$ and $\dom F$ is a cone.  We also have that if $x$ is on the primal path,
  then $A^{T} y = c$, so $y$ is feasible.

  For dual optimality, we need $0 \in -tb + \grad F(y) + N_{A^{T} y =
    c}$. As $-tb + \grad F(y) = -tAx \in \range A$, $y$ is the unique
  dual solution.  Multiplying the dual optimality result by $A^{T}$
  gives the optimality condition for the primal central point.
\end{proof}

\begin{thm}
  \label{sec:inter-point-meth-4}
  For feasible $x, y$ (so $Ax - b \in K$, $A^{T}y = c$, $y \in K$),
  the duality gap is $\phi(x) - \psi(y) = \IP{y, Ax - b}$.  Moreover,
  for points $(x(t), y(t))$ on the central path, the duality gap is
  $\phi(x(t)) - \psi(y(t)) = \frac{\theta_{F}}{t}$.
\end{thm}

\begin{proof}
  $\phi(x) - \psi(y) = \IP{c, x} - \IP{b, y} = \IP{y, Ax - b} =
  \IP{-\frac{1}{t} \grad F(Ax(t) - b), Ax(t) - b} = \frac{\theta_{F}}{t}$.
\end{proof}

\begin{thm}
  \label{sec:inter-point-meth-5}
  We define $\| v \|_{x}^{\star} = (v^{T} \grad^{2} F(Ax - b)^{-1}
  v)^{\frac{1}{2}}$, $z = (x, y)$, so $z(t)$ is the primal-dual
  central path, and $\dist(z, z(t)) = \| ty + \grad F(Ax -
  b)\|_{x}^{\star}$.
  Then for $Ax - b \in \dom F$, $y \in \dom F$, $A^{T}y = c$, we have
  $\dist(z, z(t)) \leq 1$ implies $\phi(x) - \psi(y) \leq 2(\phi(x(t))
  - \psi(y(t))) = 2 \frac{\theta_{F}}{t}$.
\end{thm}

\begin{proof}
  If we linearize $\grad F(Ax^{k+1} - b) = \grad F(Ax^{k} - b + A
  \Delta x) \approx \grad F(A x^{k} - b) + \grad^{2} F(Ax^{k} - b) A
  \Delta x$, and solve the constraints $A^{T}(y) = c, ty + \grad F(Ax
  - b) = 0$.
\end{proof}

\begin{thm}
  \label{sec:inter-point-meth-6}
  Assume $0 < \rho \leq \kappa < \frac{1}{10}$, $t^{k} > 0$ fixed, and
  $z^{k} = (x^{k}, y^{k})$ stirctly feasible, so $Ax^{k} - b \in \dom
  F$, $y^{k} \in \dom f$, such that $\dist(z^{k}, z(t^{k})) < \kappa$.

  If we apply a full Netwton step with $\tau_{k} = 1$ and $t^{k+1} = (1
  + \frac{\rho}{\sqrt{\theta_{F}}}) t^{k}$ to generate $z^{k+1}$, then
  $x^{k+1}, y^{k+1}$ are strictly primal and dual feasible, and
  $\dist(z^{k+1}, z(t^{k+1})) > \kappa$ as well.
\end{thm}

\section{Support Vector Machines}
\label{sec:supp-vect-mach}

\begin{defn}
  \label{sec:supp-vect-mach-1}
  The primal formulation of an SVM is
  \begin{align}
    \label{eq:18}
    \inf_{w, b} \frac{1}{2} \| w \|_{2}^{2}
  \end{align} such that $1 \leq y^{i} (\IP{w, x^{i}} + b)$ for all $1
  \leq i \leq n$.

  The dual formulation is
  \begin{align}
    \label{eq:48}
    \inf_{z \in \R^{n}} \frac{1}{2} \| \sum_{i=1}^{n} y^{i} x^{i}
    z_{i} \|_{2}^{2} + e^{T} z
  \end{align} such that $z_{i} \leq 0$, $\sum_{i=1}^{n} y^{i} z_{i} = 0$.
\end{defn}

\section{Total Variation and Applications}
\label{sec:total-vari-appl}

\begin{defn}
  \label{sec:total-vari-appl-1}
  For $u \in L^{1}(\Omega, \R^{m})$, the \textbf{total variation} of
  $u$ is defined as
  \begin{equation}
    \label{eq:50}
    TV(u)= \sup_{v \in C_{c}^{1}(\Omega, \R^{m \times n}), \| v
      \|_{\infty} \leq 1} \int_{\Omega} \IP{u, \divergence v} dx
  \end{equation}
\end{defn}

\begin{thm}
  \label{sec:total-vari-appl-2}
  Assume $A \subseteq \Omega$ is a set so that its boundary is $C^{1}$
  and satisfies $\mathcal{H}^{n-1}(\Omega \cap \partial A) < \infty$.
  Define
  \begin{equation}
    \label{eq:51}
    \I{A}(x) =
    \begin{cases}
      1 & x \in A \\
      0 & x \notin A
    \end{cases}
  \end{equation}
  then $TV(\I{A}) = \mathcal{H}^{n-1}(\Omega \cap \partial A)$.
\end{thm}

\begin{proof}
  The lower bound follows from $TV(1_{A}) = \sup_{v \in
    C_{c}^{1}(\Omega, \R^{n}), \| v \|_{\infty} \leq 1} \int_{\partial
    A} \IP{v, n} ds$ by Gauss's theorem.
\end{proof}

\begin{thm}[Coarea formula]
  \label{sec:total-vari-appl-3}
  If $u \in BV(\Omega)$, then $TV(\I{x|u(x) > t}) < \infty$ for
  $\mathcal{L}^{1}$-a.e. $t \in \R$, and $TV(u) = \int_{\R} TV(\I{u >
    t}) dt$.
\end{thm}

\begin{defn}
  \label{sec:total-vari-appl-4}
  For $\Omega \subseteq \R^{d}$ and $k \geq 1$, define the space
  $BV^{k}(\Omega)$ as $BV^{k} = \{ u \in W^{k-1, 1} | \grad^{k-1}u \in
BV(\Omega, R^{d^{k-1}}) \}$ and the higer-ordre total variation as
\begin{align}
  \label{eq:56}
  TV^{k}(u) = \sup_{v \in C_{c}^{k}(\Omega, \sym^{k}(\R^{d})), \| v
    \|_{\infty} \leq 1} \int_{\Omega} u \divergence^{k} v dx = TV(\grad^{k-1}
  u)
\end{align}
\end{defn}

\section{Relaxation}
\label{sec:relaxation}

\begin{defn}
  Consider the \textbf{Chan-Vese} model
  \begin{equation}
    \label{eq:58}
    f_{CV}(C, c_{1}, c_{2}) = \int_{C} (g - c_{1}^{2} dx +
    \int_{\Omega \backslash C} = (g - c_{2})^{2} dx + \lambda \mathcal{H}^{d-1}(C)).
  \end{equation}
\end{defn}

\begin{thm}
  Let $c_{1}, c_{2}$ be fixed, and consider $\inf_{u: \Omega
    \rightarrow [0, 1], u \in BV(\Omega)} f(u), f(u) = \IP{u,
    s}_{L^{1}} + \lambda TV(u)$.  Then if $u$ is a minimizer of $f$,
  and $u(x) \in \{ 0, 1 \} a.e.$, then $C$ is a minimzer of
  $f_{CV(\cdot, c_{1}, c_{2})}$.
\end{thm}

\begin{proof}
  Follows by definitions - must have $u = 1_{C}$.
\end{proof}

\begin{defn}
  Let $\mathcal{C} = BV(\Omega, [0, 1]) = \{ u \in BV(\Omega) | u(x)
  \in [0, 1] a.e. \}$.  Then for $u \in \mathcal{C}$, $\alpha \in [0,
  1]$.  Define $\overline u_{\alpha} = 1_{\{ u > \alpha \} }$. Then
  $f: \mathcal{C} \rightarrow \R$ satisfies the \textbf{generalized
    coarea condition} if and only if
  \begin{equation}
    \label{eq:59}
    f(u) = \int_{0}^{1} f(\overline u_{\alpha}) d\alpha
  \end{equation} for all $u \in \mathcal{C}$.
\end{defn}

\begin{thm}
  Let $f*u$ = TV(u), the condition is the cooarea formula.  As the
  condition is additivie, we need only show $\int_{\Omega} s(x) u(x)
  = \int_{0}^{1} \int_{\Omega} s(x) 1_{\{ u(x) > \alpha \}} d\alpha$,
  where we use Fubini due to $s \in L^{\infty}(\Omega)$.
\end{thm}

\begin{thm}
  Assume $f: \mathcal{C} \rightarrow \R$ satisfies the generalized
  coarea condition, and $u^{\star}$ satisfies $u^{\star} \in
  \argmin_{u \in \mathcal{C}} f(u)$. Then for almost every $\alpha \in
  [0, 1]$, the thresholded function statisfies $\overline
  u_{\alpha}^{\star} \in \argmin_{u \in BV(\Omega, \{ 0, 1 \})} f(u)$.
\end{thm}

\begin{proof}
  Follows by considering the set $S_{\epsilon} = \{ \alpha \in [0, 1]
  | f(u^{\star}) \leq f(u^{\star}_{\alpha}) - \epsilon \} $ for some
  $\epsilon > 0$, and showing that this implies $f(u^{\star} \leq
  \int_{0}^{1} f(\overline u_{\alpha}^{\star})) d\alpha - \epsilon
  L^{1}(S_{\epsilon})$ which contradicts the generalized coarea formula.
\end{proof}

\bibliographystyle{plainnat}
\bibliography{../../common/bibliography}
\end{document}