
\chapter{Parametric Statistical Models}
\label{cha:param-stat-models}

Let $Y_{1}, \dots, Y_{n}$ observations.

\begin{exmp}
  \label{defn:parametric_statistical_models:3}
  $Y_{1} = (Z_{i}, X_{i})$ where the $Z_{i}$'s are response variables,
  and the covariates $X_{i}$ are related to $Z_{i}$ by the regression
  relationship $Z_{i} = g(X_{i}, \theta) + \epsilon_{i}$ for $\theta
  \in \Theta \subseteq \R^{p}$, $\epsilon_{i}$ IID with
  $\E{\epsilon_{i}} = 0$, and $g: X \times \Theta \rightarrow \R$.

  A regression function (possibly non-linear) and known --- for example,
  \begin{equation}
    \label{eq:14}
    g(X_{i}, \theta) = X_{i}^{T} \theta,
  \end{equation} a linear model.
\end{exmp}


A natural way to estimate $\theta$ is by nonlinear least squares (NLS)
which finds $\hat \theta$ that minimizes
\begin{equation}
  \label{eq:20}
  Q_{n}(\theta) = \frac{1}{n} \sum_{i=1}^{n}(Z_{i}- g(X_{i}, \theta))^{2}
\end{equation}

\begin{exmp}
  \label{defn:parametric_statistical_models:2}
  We are given a model of PDF/PMF's $\{ f(\cdot, \theta): \theta \in
  \Theta \}, \Theta \subseteq \R^{p}$ for the distribution of a random
  variable $Y$.  We view $Y_{1}, \dots, Y_{n}$ as IID copies of $Y$.

  The likelihood function of the model  is defined as
  \begin{equation}
    \label{eq:30}
    L_{n}(\theta) = \Pi_{i=1}^{n} f(Y_{i}, \theta)
  \end{equation}

  The log-likelihood function $l_{n}(\theta) = \log L_{n}(\theta)$.  A
  \textbf{maximum likelihood estimator} (MLE) is any value $\hat
  \theta = \hat \theta_{MLE} \in \Theta$ that maximizes
  $L_{n}(\theta)$ over $\Theta$.  Equivalently, we minimize
  \begin{equation}
    \label{eq:31}
    Q_{n}(\theta) = -\frac{1}{n} l_{n}(\theta) = - \frac{1}{n}
    \sum_{i=1}^{n} \log f(Y_{i}, \theta)
  \end{equation}
\end{exmp}

\section{Consistency of M-Estimators}
\label{sec:cons-m-estim}

In both the examples, $\hat \theta_{n}$ is found by minimizing a
random criterion function $Q_{n}(\theta)$ over $\Theta$, and proved a
``limiting function'' $Q(\theta)$ exists, we expect these minimizers
to converge to the minimizers of $Q$.

\begin{thm}
  \label{defn:parametric_statistical_models:1}
  Let $\Theta \subseteq \R^{p}$ be compact.  Let $Q: \Theta
  \rightarrow \R$ be a continuous, non-random function that has a
  unique minimizer $\theta_{0} \in \Theta$.

  Let $Q_{n}: \Theta \rightarrow \R$ be any sequence of random
  functions such that
  \begin{equation}
    \label{eq:32}
    \sup_{\theta \in \Theta} |Q_{n}(\theta) - Q(\theta)| \cp 0
  \end{equation} as $n \rightarrow \infty$.

  If $\theta_{n}$ is \textbf{any} sequence of minimizers of $Q_{n}$,
  then $\hat \theta_{n} \cp \theta_{0}$ as $n \rightarrow \infty$.
\end{thm}

\begin{proof}
  Let $\epsilon > 0$ be arbitrary. The set $\Theta_{\epsilon} = \{
  \theta \in \Theta : \| \theta - \theta_{0} \| \geq \epsilon \}$ is
  compact and $Q$ is continuous on $\Theta_{\epsilon}$, so $Q$ attains
  its infimum
  \begin{equation}
    \label{eq:33}
    c(\epsilon) = \inf_{\theta \in \Theta_{\epsilon}} Q(\theta) =
    Q(\bar \theta_{\epsilon}) \in \Theta_{\epsilon} > Q(\theta_{0})
  \end{equation} as $\theta_{0}$ is the minimizer.

  Pick $0 < \delta(\epsilon) < \frac{c(\epsilon) - Q(\theta_{0})}{2}$,
  which implies
  \begin{equation}
    \label{eq:34}
    c(\epsilon) - \delta(\epsilon) > Q(\theta_{0}) + \delta(\epsilon)
  \end{equation}

  Define the event
  \begin{equation}
    \label{eq:35}
    A_{n}(\epsilon) = \{ \sup_{\theta \in \Theta} |Q_{n}(\theta) -
    Q(\theta)| < \delta(\epsilon) \}.
  \end{equation}  On this event we have
  \begin{align*}
    \inf_{\theta \in \Theta_{\epsilon}} Q_{n}(\theta) & = \inf_{\theta
      \in \Theta_{\epsilon}} [Q_{n}(\theta) - Q(\theta) + Q(\theta)]
                                                                                              \\
                                                      & \geq \inf_{\theta \in \Theta_{\epsilon}} Q(\theta) - \sup_{\theta
      \in \Theta} |Q_{n}(\theta) - Q(\theta)|                                                 \\
                                                      & \geq C(\epsilon) - \delta(\epsilon)   \\
                                                      & \geq Q(\theta_{0}) + \delta(\epsilon) \\
                                                      & \geq Q(\theta_{0}) + \delta(\epsilon) - |Q_{n}(\theta_{0}) -
    Q(\theta_{0})|                                                                            \\
                                                      & \geq Q_{n}(\theta_{0})
  \end{align*} since on $A_{n}(\epsilon)$, in particular
  $|Q_{n}(\theta_{0}) - Q(\theta_{0})| < \delta(\epsilon)$.

  We conclude
  \begin{equation}
    \label{eq:36}
    \inf_{\theta: \|\theta - \theta_{0} \| \geq \epsilon}
    Q_{n}(\theta) > Q_{n}(\theta_{0})
  \end{equation}

  Now suppose $\hat \theta_{n} \in \Theta_{\epsilon}$, then
  $Q_{n}(\hat \theta_{n}) \geq \inf_{\theta \in \Theta_{\epsilon}}
  Q_{n}(\theta) > Q_{n}(\theta_{0})$.

  Hence, on $A_{n}(\epsilon)$, we have $\| \hat \theta_{n} -
  \theta_{0}\| < \epsilon$, $A_{n}(\epsilon) \subseteq \{ \| \hat
  \theta_{n} - \theta_{0} \| < \epsilon \}$, so since by hypothesis
  $\Prob{A_{n}(\epsilon)} \rightarrow 1$ for all $\epsilon > 0$, we
  see $\Prob{\| \hat \theta_{n} - \theta_{0} \| < \epsilon}
  \rightarrow 1$, as $\Prob{\| \hat \theta_{n} - \theta_{0} \| \geq
    \epsilon} \rightarrow 0$ as $n \rightarrow \infty$. Since
  $\epsilon > 0$ was arbitrary, the result follows.
\end{proof}

\begin{remark}
  Uniform convergence of $Q_{n} \rightarrow Q$ is necessary. In fact,
  none of the conditions can be relaxed.
\end{remark}

\begin{exer}
  \begin{enumerate}
  \item What is $Q$ in Examples
    \ref{defn:parametric_statistical_models:3}, \ref{defn:parametric_statistical_models:2}?
  \item What is $\Theta_{0}$?
  \item When does uniform convergence occur?
  \end{enumerate}
\end{exer}

\begin{exmp}
  \label{defn:parametric_statistical_models:4}
  Let $Y = (Z, X)$ such that $Z = g(X, \theta_{0}) + \epsilon$, where
  $\E{\epsilon | X} = 0$, $\theta_{0}$ is the ``true value'', and
  based on \iid observations $Y_{1}, \dots, Y_{n}$, we minimize
  \begin{equation}
    \label{eq:38}
    Q_{n}(\theta)= \frac{1}{n} \sum_{i=1}^{n} (Z_{i} - g(X_{i}, \theta))^{2}
  \end{equation} over $\Theta$.  We expect
  \begin{equation}
    \label{eq:40}
    Q(\theta) = \E{(Z - g(X, \theta))^{2}}{\theta_{0}}
  \end{equation}  Inserting the model equation
  \begin{equation}
    \label{eq:41}
    Q(\theta) = \E{(g(X_{1}, \theta_{0}) - g(X,
    \theta) + \epsilon)^{2}}{\theta_{0}} = \E{g(X, \theta_{0}) - g(X, \theta)}^{2}
    + \E{\epsilon^{2}}
  \end{equation}

  Hence $Q(\theta)$ is minimized at $\theta_{0}$ if the regression
  parameterization is identifiable, that is
  \begin{equation}
    \label{eq:42}
    \theta = \theta' \iff g(\cdot, \theta) = g(\cdot, \theta')
  \end{equation} ${\Prob}_{X}$ almost surely.
\end{exmp}

\begin{exmp}
  \label{defn:parametric_statistical_models:5}
  Let $Y_{1}, \dots, Y_{n}$ be \iid copies of $Y$, and we maintain a
  parametric model
  \begin{equation}
    \label{eq:43}
    \{ f(\cdot, \theta): \theta \in \Theta \}
  \end{equation} of PDFs/PMFs and the MLE is found by minimizing
  \begin{equation}
    \label{eq:44}
    Q_{n}(\theta) = - \frac{1}{n} \sum_{i=1}^{n} \log f(Y_{i}, \theta)
  \end{equation}

  By the law of large numbers, assuming $f(y, \theta) > 0$ for all $y,
  \theta$ and
  \begin{equation}
    \label{eq:45}
    \E{|\log f(Y, \theta)|}{\theta_{0}} < \infty
  \end{equation} where $Y$ is assumed to be distributed as $f(\cdot,
  \theta_{0})$, then the limiting criterion function is
  \begin{equation}
    \label{eq:46}
    Q(\theta) = - \E{\log f(Y, \theta)}{\theta_{0}}
  \end{equation}

  Then
  \begin{align}
    \label{eq:47}
    Q(\theta_{0}) - Q(\theta) &= \mathbb{E}_{\theta_{0}} \log f(Y,
    \theta) - \mathbb{E}{\theta_{0}} - \log f(Y, \theta_{0}) \\
    &= \E{\log \frac{f(Y, \theta)}{f(Y, \theta_{0})}}{\theta_{0}} \\
    &\leq \log \E{\frac{f(Y, \theta)}{f(Y, \theta_{0})}}{\theta_{0}}
  \\
  &= \log \int \frac{f(y, \theta)}{f(y, \theta_{0})} f(y, \theta_{0})
  \, dy \\
  &= \log 1 \\
  &= 0
  \end{align} or in other words,
  \begin{equation}
    \label{eq:48}
    Q(\theta_{0}) \leq Q(\theta) \forall \theta \in \Theta
  \end{equation}
\end{exmp}

Equality in Jensen's inequality can only occur when
\begin{equation}
  \label{eq:49}
  \frac{f(\cdot, \theta)}{f(\cdot, \theta_{0})} = C \in \R
\end{equation} so since $\int f(y, \theta) dy = 1$, we see $C = 1$,
and hence if the model is identifiable in the sense that $\theta =
\theta' \iff f(\cdot, \theta) = f(\cdot, \theta')$ for all $\theta,
\theta' \in \Theta$, then the value $\theta_{0}$ that minimizes
$\theta_{0}$ is unique.

\section{Verifying uniform convergence}
\label{sec:verify-unif-conv}

\begin{proposition}
  Let $\Theta$ be compact in $\R^{p}$, and let $\mathcal{X} \subseteq \R^{d}$
  and consider observing $X_{1}, \dots, X_{n}$ \iid from $X \sim
  \Prob$ on $X$.  Let $q: \mathcal{X} \times \Theta \rightarrow \R$
  that is continuous in $\theta$ for all $x$ and measurable in $x$ for
  all $\theta \subseteq \Theta$.

  Assume
  \begin{equation}
    \label{eq:50}
    \E{\sup_{\theta \in \Theta} | q(X, \theta)|} < \infty
  \end{equation}

  Then
  \begin{equation}
    \label{eq:51}
    \sup_{\theta \in \Theta} | \frac{1}{n} q(X_{i}, \theta) - \E{q(X,
      \theta)} | \cas 0
  \end{equation} as $n \rightarrow \infty$
\end{proposition}

\begin{proof}
  We apply the uniform law of large numbers from Proposition
  \ref{defn:ulln}  and we need to cover the set
  \begin{equation}
    \label{eq:52}
    \mathcal{H} = \{ q(\cdot, \theta) : \theta \in \Theta \}
  \end{equation} by suitable brackets.

  Define open balls
  \begin{equation}
    \label{eq:53}
    B(\theta, m) = \{ \theta' \in \Theta: \| \theta -  \theta' \| <
    \eta \}
  \end{equation}
  Construct ``brute-force'' brackets
  \begin{align}
    \label{eq:54} \overline q(X, \theta, \eta) = \sup_{\theta' \in
      B(\theta, \epsilon)} q(X, \theta') \\ \underline q(X, \theta,
    \eta) = \inf_{\theta' \in B(\theta, \eta)} q(X, \theta')
  \end{align} which obviously cover all the $\{ q(\cdot, \theta') :
  \theta' \in B(\theta, \eta) \}$.

  Clearly,
  \begin{align}
    \label{eq:55} \E{\overline{\underline q}(x, \theta, \eta)} \leq
    \E{\sup_{\theta \in \Theta} | q(X, \theta) |} < \infty
  \end{align} by the domination condition.

  By continuity and compactness, the supremum/infimum above are
  attained at $\overline \theta, \underline \theta \in \Theta$ such
  that $\| \overline \theta - \theta \| \leq \eta$. So
  \begin{align}
    \label{eq:56} | \overline q (X, \theta, \eta) - \underline q(X,
    \theta, \eta) | &\leq | \overline q(X, \theta, \eta) - q(X,
    \theta) | + | q(X, \theta) - \underline q (X, \theta, \eta) |
  \end{align} which again by continuity tends to zero as $\eta
  \rightarrow 0$.

  So $| \overline q(X, \theta, \eta) - \underline q(X, \theta, \eta)|
  \rightarrow 0$ as $\eta \rightarrow 0$.

  By the dominated convergence theorem we can integrate this limit with
  respect to $\E$, (using the dominance condition).  So,
  \begin{equation}
    \label{eq:57}
    \E{|\overline q(X, \theta, \eta) - \underline q(X, \theta, \eta)}
    \rightarrow 0
  \end{equation} as $\eta \rightarrow 0$.

  Then for all $\epsilon > 0$, there exists $\eta = \eta(\epsilon,
  \theta)$ such that
  \begin{equation}
    \label{eq:58}
    \E{|\overline q(X, \theta, \eta(\epsilon, \theta)) - \underline
      q(X, \theta, \eta(\epsilon, \theta))|} < \epsilon
  \end{equation}

  The balls $\{ B(\theta, \eta(\epsilon, \theta)) : \theta \in \Theta
  \}$ form an open covering of $\Theta$, so by compactness
  (Heine-Borel theorem in $\R^{p}$), there exists a finite subcover of
  $\theta$, say with centers $\theta_{1}, \dots,
  \theta_{N(\epsilon)}$.  Then the corresponding brackets
  \begin{equation}
    \label{eq:59}
    [\underline q_{i}, \overline q_{i}] = [ \underline q(\cdot,
    \theta_{j}, \eta(\epsilon, \theta_{j})), \overline q(\cdot,
    \theta_{j}, \eta(\epsilon, \theta_{j}))]
  \end{equation} cover $\mathcal{H}$ and satisfy the conditions of
  Proposition \ref{defn:ulln}
\end{proof}

\begin{remark}
  The above result is simply a law of large numbers in the Banach
  space of continuous functions on $\Theta$, and
  \begin{equation}
    \label{eq:60}
    \E{\sup_{\theta \in \Theta} | q(X, \Theta) |} = \E{\|Z\|} < \infty
  \end{equation} which is necessary for the result to hold.
\end{remark}

\todo{Fill in missing notes from previous lecture}

\begin{defn}
  \label{defn:parametric_statistical_models:6}
  A consistent estimator $\tilde \theta$ in a model $\{ f(\cdot,
  \theta) | \theta \in \Theta \}$ is called \textbf{asymptotically
    efficient} if $\lim_{n} n \Var{\tilde \theta} = I(\theta)^{-1}$
  for all $\theta \in \interior (\Theta)$ where $I(\theta)$ is the Fisher information.
\end{defn}

\begin{thm}
  \label{defn:parametric_statistical_models:7}
  In a model satisfying Assumption B,
  \begin{equation}
    \label{eq:39}
    \sqrt{n} (\hat \theta_{MLE} - \theta_{0}) \cd N(0, I(\theta_{0})^{-1})
  \end{equation}
\end{thm}

\begin{proof}
  Let $\Prob = \Prob_{\theta_{0}}^{N}$, $\E = \mathbb{E}_{\theta}$.

  For $\ell_{n}(\theta) = -Q_{n}(\theta) = \frac{1}{n} \log f(Y_{i},
  \theta)$.  When proving $Z_{n} \cd Z$ we may restrict to events
  $E_{n}$ such that $\Prob{E_{n}} \rightarrow 1$, since
  \begin{equation}
    \label{eq:61}
    \| \Prob{Z_{n} \leq t}- \Prob{Z_{n} \leq t, E_{n}} \| \leq
    \Prob{E_{n}^{c}} \rightarrow 0
  \end{equation} as $n \rightarrow \infty$.  Since $\hat \theta_{n}
  \cp \theta_{0}$ as $n \rightarrow \infty$, we can restrict to $E_{n}
  = \{ \hat \theta_{n} \}$ where $K$ is a closed ball centered at
  $\theta_{0}$.  By the assumptions, $\ln$ is $C^{2}$ on $U$, and
  $\hat \theta_{n}$ is a maximizer on the open set $U$, so necessarily,
  \begin{equation}
    \label{eq:62}
    0 = \frac{\partial}{\partial \theta} ln(\theta)_{|_{\theta = \hat \theta_{n}}} =
    \frac{\partial}{\partial \theta} \ln (\hat \theta_{n}) =
    \begin{bmatrix}
      \frac{\partial}{\partial \theta_{1}} \ln (\hat \theta_{n}) \\
      \vdots \\
      \frac{\partial}{\partial \theta_{p}} \ln (\hat \theta_{n})
    \end{bmatrix}
  \end{equation}

  For $h: K \rightarrow \R$ and $u, v \in K$ the line segment
  \begin{equation}
    \label{eq:63}
    tu + (1-t)v
  \end{equation} for $0 < t < 1$ connection $u, v$ does lie in the
  ball $K$ by convexity, and the mean value theorem gives (for $h \in
  C^{1}(U)$),
  \begin{equation}
    \label{eq:64}
    h(u) = h(v) + \frac{\partial h}{\partial u} (\overline v)^{T} (u-v)
  \end{equation} where $\overline v$ is a mean-value on the line
  segment.

  Applying this $p$-times to $\frac{\partial}{\partial \theta_{i}}
  \ell_{n}(\theta)$ we obtain
  \begin{align}
    \label{eq:65}
    0 =
    \begin{bmatrix}
      \frac{\partial}{\partial \theta_{1}} \ell_{n}(\hat \theta_{n})  \\
      \vdots \\
      \frac{\partial}{\partial \theta_{p}} \ell_{n}(\hat \theta_{n})  \\
    \end{bmatrix} 
  \end{align} \todo{Fill this in}
o
  We have
  \begin{align}
    \label{eq:66}
    (\overline{A_{n}})_{kj} = \frac{1}{n} \sum_{i=1}^{n} \left[
    \frac{\partial^{2}}{\partial \theta_{k} \partial \theta_{j}} \log
    f(Y_{i}, \overline{\theta}_{(j)}) -
    \E{\frac{\partial^{2}}{\partial \theta_{i} \partial \theta_{j}}} \right]
  \end{align}
  \todo{Fill in rest of proof}
\end{proof}

\begin{remark}[Discussion of Theorem
  \ref{defn:parametric_statistical_models:7}]
  \begin{enumerate}
  \item One can weaken the conditions to $\theta \mapsto f(\cdot,
    \theta)$ being ``weakly $C1$'', to model the Laplace family. For
    non-differentiable parameterizations, the asymptotics of the MLE
    may be non-normal. For example, consider $U[0, \theta]$ with
    $\theta \in \Theta = (0, \infty)$.
  \item When the ``true'' $\theta_{0}$ is at the boundary of the
    parameter space, the asymptotics of the MLE are also non-normal.
    For example, $N(\theta, 1), \theta \in \Theta = [0, \infty), \hat
    \theta_{MLE} = \max(\overline{X_{n}}, 0)$
  \item Asymptotic efficiency is an optimality criterion that is
    meaningful only for ``regular'' estimators, that rules out the
    following super-efficient estimator e.g.
    \begin{equation}
      \label{eq:67}
      \tilde \theta =
      \begin{cases}
        \hat \theta_{MLE} & |\hat \theta_{MLE}| \geq n^{-\frac{1}{4}} \\
        0 & \text{otherwise}
      \end{cases}
    \end{equation}

    One shows that under $P_{\theta}, \theta \neq 0$, that
    $\sqrt{n}(\tilde \theta - \theta) = \sqrt{n}(\hat \theta -
    \theta) \cd N(0, I(\theta)^{-1})$ as $n \rightarrow \infty$.
    However, under $P_{0}$, one shows easily that $\sqrt{n}(\tilde
    \theta - \theta) \cd 0 = N(0, 0)$ which strictly beats the $N(0,
    I(0)^{-1})$-distribution (Hodges' estimator).
  \end{enumerate}
\end{remark}


\section{Asymptotic Inference based on the MLE}
\label{sec:asmpt-infer-based}

Suppose we want a confidence interval for $\theta_{j}$, $j = 1, \dots,
p$.   We can write $\theta_{j} = e_{j}^{T} \theta$, $e_{j} = (0,
\dots, 0, \underbrace{1}_{\text{$j$-th position}}, \dots, 0)$.  By the
continuous mapping theorem, we have
\begin{align}
  \label{eq:68}
  \sqrt{n} (\hat \theta_{j} - \theta_{j}) = \sqrt{n} e_{j}^{T}(\hat
  \theta - \theta) \cd N(0, e_{j}^{T} I(\theta)^{-1}e_{j}) = N(0, I^{-1}(\theta)_{jj})
\end{align}


Suggesting that
\begin{align}
  \label{eq:69}
  C_{n} = \{ v \in \R : | \hat \theta_{n, j}  - v | \leq
  \frac{(I(\theta)^{-1})^{\frac{1}{2}}_{jj} Z_{\alpha}}{\sqrt{n}} \}
\end{align} where $Z_{\alpha}$ are such that $\Prob(|Z| \leq
Z_{\alpha}) = 1 - \alpha$ is a confidence interval for $\theta_{j}$,
since
\begin{align}
  \label{eq:70}
  \Prob_{\theta}^{n}(\theta_{j} \in C_{n}) =
  \P_{\theta}^{n}(\sqrt{n}(I(\theta)^{-1})^{-\frac{1}{2}}_{jj} | \hat
  \theta_{n, j} - \theta| \leq Z_{\alpha}) \rightarrow \Prob{|Z| \leq
    Z_{\alpha}} = 1 - \alpha.
\end{align}

This can only be used if $I(\theta)$ is known, otherwise $I(\theta)$
has to estimated consistently.

\begin{defn}
  \label{defn:parametric_statistical_models:8}
  The \textbf{observed} Fisher information is defined as
  \begin{equation}
    \label{eq:71}
    i_{n}(\theta) = \frac{1}{n} \sum_{i=1}^{n}
    \frac{\partial}{\partial \theta} \log f(Y_{i}, \theta)
    \frac{\partial}{\partial \theta} \log f(Y_{i}, \theta)^{T}
  \end{equation}

  One shows as in the proof of Theorem
  \ref{defn:parametric_statistical_models:7} that
  \begin{equation}
    \label{eq:72}
    \hat i_{n} = i_{n}(\hat \theta_{MLE}) \cp I(\theta_{0})
  \end{equation} under $P_{\theta_{0}}$.

  Alternative, one can use
  \begin{align}
    \label{eq:73}
    \hat j_{n} = j_{n}(\hat \theta_{n})
  \end{align} where
  \begin{equation}
    \label{eq:74}
    j_{n}(\theta) = \frac{1}{n} \sum_{i=1}^{n}
    \frac{\partial^{2}}{\partial \theta \partial \theta^{T}} \log
    f(Y_{i}, \theta)
  \end{equation} which does estimate $I(\theta_{0})$ consistently.
\end{defn}

To construct a confidence set for $\theta \in \Theta \subseteq
\R^{p}$, it is consistent to consider the Wold-statistic
\begin{equation}
  \label{eq:75}
  W_{n}(\theta) = n(\hat \theta - \theta)^{T} \hat i_{n}(\hat \theta - \theta)
\end{equation} which can be shown to have, under $P_{\theta}^{N}$ to
have the $\chi^{2}_{p}$ distribution. Thus
\begin{equation}
  \label{eq:76}
  C_{n} = \{ \theta \in \R^{p} | W_{n}(t) \leq \xi_{\alpha} \}
\end{equation} where $\xi_{\alpha}$ are the $1-\alpha$ quartiles of
the $\chi^{2}_{p}$ distribution, is a confidence ellipsoid for
$\theta$ of asymptotic coverage probability $1-\alpha$.

To test $H_{0}: \theta = \theta_{0}$ against $H_{1}: \Theta \in \theta
\backslash \{ \theta_{0} \}$, we can refer $W_{n}(\theta_{0})$ to the
quartiles of the $\chi^{2}_{p}$ distribution, since $W_{n}(\theta_{0})
\cd \xi^{2}_{p}$ under $H_{0}$.

For such testing problems there exists an alternative approached based
on the \textbf{likelihood ratio test statistic} for $H_{0}: \theta \in
\Theta_{0}$ vs $H_{1}: \theta \in \Theta \backslash \Theta_{0}$, with
$\Theta_{0} \subseteq \Theta$ as
\begin{align}
  \label{eq:78}
  \Lambda_{n}(\Theta, \Theta_{0}) &= 2 \log \frac{\sup_{\theta \in \Theta}
    \prod_{i=1}^{n} f(Y_{i}, \theta)}{\sup_{\theta \in \Theta_{0}}
    \prod_{i=1}^{n} f(Y_{i}, \theta)} \\
  &= 2 \log \frac{\prod_{i=1}^{n} f(Y_{i}, \hat \theta_{n})}{\prod_{i=1}^{n}
    f(Y_{i}, \hat \theta_{n, 0})}
\end{align} where $\hat \theta_{n}$ is the unrestricted MLE and $\hat
\theta_{n, 0}$ is the MLE restricted to $H_{0}$.

\begin{thm}[Wilks']
  \label{defn:parametric_statistical_models:9}
  If $\dim(\theta_{0}) = p_{0} < \dim(\Theta) = p$, then
  \begin{equation}
    \label{eq:79}
    \Lambda_{n}(\Theta, \Theta_{0}) \cd \chi^{2}_{p - p_{0}}
  \end{equation} as $n \rightarrow \infty$.
\end{thm}

\begin{proof}
  (Only for $H_{0} = \{ \theta_{0} \}, \dim \theta_{0} = 0$).

  Recall
  \begin{equation}
    \label{eq:77}
    Q_{n}(\theta) = - \frac{1}{n} \sum_{i=1}^{n} \log f(Y_{i}, \theta)
    = -l_{n}(\theta)
  \end{equation} and so
  \begin{align}
    \label{eq:80}
    \Delta_{n}(\theta, \theta_{0}) &= 2n Q_{n}(\theta_{0}) - 2n Q_{n}(\hat \theta_{n}) \\
    &= 2n \frac{\partial}{\partial \theta} Q_{n} (\hat
    \theta_{n})^{T}(\theta_{0} - \hat \theta_{n}) +
    \frac{2n}{2}(\theta_{0} - \hat \theta_{n})^{T}
    \frac{\partial^{2}}{\partial \theta \partial \theta^{T}}Q_{n}(\bar
    \theta)(\theta_{0} - \hat \theta_{n}) \\
    &= \sqrt{n} (\hat \theta_{n} - \theta_{0})^{T} \bar A_{n} \sqrt{n}
    (\theta_{0} - \hat \theta_{n}) \\
    &= Z_{n}^{T} \bar A_{n} Z_{n}
  \end{align}

  where we then conclude that from Theorem 3 (in notes) that $Z_{n}
  \cd Z \sim N(0, I(\theta_{0})^{-1})$, and, as in the proof of
  Theorem 3, $\bar A_{n} \cp I(\theta_{0})$ as $n \rightarrow \infty$.
  Rewrite this as
  \begin{align}
    \label{eq:81}
    Z_{n}^{T} I(\theta_{0})Z_{n} + Z_{n}^{T}(\bar A_{n} -
    I(\theta_{0})) Z_{n}
  \end{align} which by repeated applications of Slutsky's lemma.

  The mapping $X \mapsto X^{T} I(\theta_{0})X$ is continuous from
  $\R^{p}$ into $\R$, so by the continuous mapping theorem,
  \begin{equation}
    \label{eq:82}
    Z_{n}^{T} I(\theta_{0}) Z_{n} \cd Z^{T}I(\theta_{0})Z
  \end{equation}
  and as $I(\theta_{0})$ is positive semidefinite (and so has a square
  root), we can write this as
  \begin{equation}
    \label{eq:83}
    Z^{T} I(\theta_{0}) Z = Z_{T} I(\theta_{0})^{\frac{1}{2}}
    I(\theta_{0})^{\frac{1}{2}}Z = W^{T} W = \sum_{i=1}^{p} W_{i}^{2}
    \sim \chi^{2}_{p}
  \end{equation} with $W \sim N(0, I)$.
\end{proof}

\section{Some Ideas from LeCam Theory}
\label{sec:some-ideas-from}

Consider first a Gaussian shift experiment
\begin{equation}
  \label{eq:84}
  N(g, I(\theta)^{-1}), g \in \R^{p}, I(\theta)
\end{equation} is the Fisher information of some statistical model
\begin{equation}
  \label{eq:85}
  \{ f(\cdot, \theta), \theta \in \Theta \}
\end{equation}

The log-likelihood ratio
\begin{equation}
  \label{eq:86}
  \log \frac{dN(h, I(\theta)^{-1})}{dN(0, I(\theta)^{-1})}(X) = h^{T}
  I(\theta)X - \frac{1}{2}h^{T}I(\theta) h
\end{equation} since the ratio is proportional to
\begin{equation}
  \label{eq:87}
  \exp \left( - \frac{(X-h)^{T} I(\theta)(X-h)}{2} +
    \frac{X^{T}I(\theta)X}{2} \right)
\end{equation}

\begin{defn}
  \label{defn:parametric_statistical_models:10}
  A model $\{ f(\cdot, \theta), \theta \in \Theta \}$ is called
  \textbf{locally asymptotically normal (LAN)} at $\theta_{0} \in \int
  \Theta$ if for all $h \in \R^{p}$ (small enough),
  \begin{align}
    \label{eq:88}
    \log \frac{\prod_{i=1}^{n} f(Y_{i}, \theta_{0} +
      \frac{h}{\sqrt{n}})}{\prod_{i=1}^{n} f(Y_{i}, \theta_{0})} =
    \frac{1}{\sqrt{n}} \sum_{i=1}^{n} h^{T} \frac{\partial}{\partial
      \theta} \log f(Y_{i}, \theta)_{|_{\theta = \theta_{0}}} -
    \frac{1}{2} h^{T} I(\theta_{0}) h + Z_{n}
  \end{align} as $n \rightarrow \infty$, where $Z_{n} \cp 0$ under $P_{\theta_{0}}^{n}$.
\end{defn}

\begin{remark}
  The first term in the expansion (by the CLT) converges in
  distribution to $N(0, h^{T} I(\theta_{0}) h)$ as $n \rightarrow
  \infty$.
\end{remark}


\begin{proposition}
  Any statistical model that satisfies the conditions of Theorem 3 is
  also LAN.
\end{proposition}

\begin{proof}
  The LHS of (dagger) \todo{find reference} equals
  \begin{align}
    \label{eq:89}
    n l_{n}(\theta_{0} + \frac{h}{\sqrt{n}}) - n l_{n}(\theta_{0}) &=
    \sqrt{n} \frac{1}{n} \sum_{i=1}^{n} h^{T} \frac{\partial}{\partial
    \theta} \log f(Y_{i}, \theta_{0}) + \frac{n}{2} h^{T}
  \frac{\partial^{2}}{\partial \theta \partial \theta^{T}} l_{n}(\bar
  \theta) h  \\
  &= \frac{1}{\sqrt{n}} \sum_{i=1}^{n} h^{T} \frac{\partial}{\partial
    \theta} \log f(Y_{i}, \theta_{0}) - h^{T}I(\theta_{0})h + \underbrace{o_{p}(1)}_{Z_{n}}
  \end{align}
\end{proof}

\begin{defn}
  \label{defn:parametric_statistical_models:11}
  Let $\mathbb{P}_{n}, \mathbb{Q}_{n}$ be sequences of probability
  measures.  We say $\mathbb{Q}_{n}$ is \textbf{contiguous} with
  respect to $\mathbb{P}_{n}$ ($\mathbb{Q}_{n} \lhd \mathbb{P}_{n}$) if
  \begin{equation}
    \label{eq:90}
    \mathbb{P}_{n}(A_{n}) \rightarrow 0 \Rightarrow
    \mathbb{Q}_{n}(A_{n}) \rightarrow 0
  \end{equation} for any sequence of events $A_{n}$ in the probability
  space.  We say $\mathbb{P}_{n}, \mathbb{Q}_{n}$ are mutually
  contiguous if $\mathbb{P}_{n} \lhd \mathbb{Q}_{n}$ and
  $\mathbb{P}_{n} \rhd \mathbb{Q}_{n}$ and write $\mathbb{P}_{n} \lhd
  \rhd \mathbb{Q}_{n}$.
\end{defn}

\begin{lem}[LeCam's 1st lemma]
  The following are equivalent:
  \begin{enumerate}
  \item $\mathbb{Q}_{n} \lhd \mathbb{P}_{n}$
  \item
    \begin{equation}
      \label{eq:91}
      \frac{dQ_{n}}{dP_{n}}(X_{n}) \cd U, X_{n} \sim P_{n}
    \end{equation} along a subsequence, then $P(U > 0) = 1$.
  \item
    \begin{equation}
      \label{eq:92}
      \frac{dP_{n}}{dQ_{n}}(X_{n}) \cd V, X_{n} \sim Q_{n}
    \end{equation} along a subsequence, $\E{V} = 1$.
  \item For any sequence of statistics (measurable functions $T_{n} :
    \Omega_{n} \rightarrow \R$), we have $T_{n} \cp 0$ under $P_{n}$
    then $T_{n} \cp 0$ under $Q_{n}$ as $n \rightarrow \infty$.
  \end{enumerate}
\end{lem}

\begin{remark}
  For two probability measures $P, Q$ that are absolutely continuous with
  respect to each other, the likelihood ratio is the random variable
  $\frac{dP}{dQ}(X), X \sim Q$.
\end{remark}

\begin{corollary}
  \begin{enumerate}
  \item If $\frac{dQ_{n}}{dP_{n}} \cd e^{X}$ for $X_{n} \sim P_{n}$,
        and $X \sim N(-\frac{\sigma^{2}}{2}), \sigma^{2})$, $\sigma^{2} > 0$, then
    \begin{equation}
      \label{eq:93}
      Q_{n} \lhd \rhd P_{n}
    \end{equation}
  \item In any LAN model the product measures $P_{\theta_{0} +
      \frac{h}{\sqrt{n}}}^{n}, P_{\theta_{0}}^{n}$, corresponding to
    the joint distributions of a sample of size $n$ fro the PDF/PMF
    $f(\theta_{0} + \frac{h}{\sqrt{n}}), f(\theta_{0})$ respectively,
    are mutually contiguous (for arbitrary $h \in \R^{p}$).
  \end{enumerate}
\end{corollary}


\begin{proof}
  \begin{enumerate}
  \item By LeCam's lemma, $P(e^{X} > 0) = 1$ for any normal random
    variable $X$, and $\E{e^{X}} = e^{}$.
  \end{enumerate}
  \todo{Complete proof}
\end{proof}


IN a LAN model, the product measures $\P^{n}_{\Theta} =
\otimes_{i=1}^{n} \P_{\Theta}$ and $P_{\theta + \frac{h}{\sqrt{n}}}$
are mutually contiguous.

\begin{exmp}
  \label{defn:parametric_statistical_models:12}
  Recall the Hodges' estimator
  \begin{equation}
    \label{eq:94}
    \tilde \theta_{n} = \hat \theta_{n} \I{|\hat \theta_{n}| \geq n^{-\frac{1}{4}}}
  \end{equation} in a regular parametric model, $\Theta = \R$, and
  where $\hat \theta_{n}$ is the MLE.  One shows under $P_{\theta},
  \theta \neq 0$, we have
  \begin{equation}
    \label{eq:95}
    \sqrt{n}(\tilde \theta - \theta) \cd N(0, I(\theta)^{-1})
  \end{equation} as $n \rightarrow \infty$.  But when sampling from
  $P_{0}$, then $P_{0}^{n} (\tilde \theta(X_{1}, \dots, X_{n}) \neq 0)
  = P_{0}^{n}(|\hat \theta_{n}| \geq n^{-\frac{1}{4}}) =
  P_{0}^{n}(\sqrt{n}|\theta_{n} - \theta| \geq n^{-\frac{1}{4}})
  \rightarrow 0$.
  This follows as $X_{n} \cd X \Rightarrow (X_{n}, n \in \N)$  is
  stochastically bounded, that is, there exists $M(\epsilon)$ such
  that $P(|X_{n}| > M(\epsilon)) < \epsilon)$.  Hence, under $P_{0}$,
  $\sqrt{n}(\tilde \theta - \theta) \cd N(0, 0)$ which outperforms the
  Cramer-Rao lower bound at $\theta = 0$.

  Consider now the minimax quadratic risk of $\tilde \theta$, equal to
  (for $n \in \N$ fixed),
  \begin{equation}
    \label{eq:96}
    \sup_{\theta \in \Theta} \mathbb{E}^{h}_{\theta} (\sqrt{n}(\tilde
    \theta - \theta))^{2}
  \end{equation} Consider the local alternative $0 +
  \frac{h}{\sqrt{n}}, h \in \R$ arbitrary.  Then the minimax risk
  exceeds

  \begin{align}
    \label{eq:97}
    &\geq \mathbb{E}^{n}_{\frac{h}{\sqrt{n}}} n(\tilde \theta -
    \frac{h}{\sqrt{n}})^{2} \I{\tilde \theta = 0} \\
    &= h^{2}P_{\frac{h}{\sqrt{n}}}^{n}(\tilde \theta = 0) \\
    &= h^{2}(1 - P^{n}_{\frac{h}{\sqrt{n}}(\tilde \theta \neq 0)}) \\
    &\geq \frac{h^{2}}{2}
  \end{align} by contiguity of $P_{0}^{n} \lhd \rhd P^{n}_{\frac{h}{\sqrt{n}}}$.

  Conclude that
  \begin{align}
    \label{eq:98}
    \lim_{n \rightarrow \infty} \sup_{\theta \in \Theta}
    \mathbb{E}_{\Theta}^{h}(\sqrt{n}(\tilde \theta - \theta))^{2}
    \rightarrow \infty 
  \end{align} whereas
  \begin{align}
    \label{eq:99}
    \lim_{n \rightarrow \infty} \sup_{\theta \in \Theta}
    \mathbb{E}^{n}_{\theta}(\sqrt{n}(\hat \theta - \theta))^{2} \leq
    \sup_{\theta \in \Theta} I(\theta)^{-1} < \infty
  \end{align}
\end{exmp}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "master"
%%% End: 
