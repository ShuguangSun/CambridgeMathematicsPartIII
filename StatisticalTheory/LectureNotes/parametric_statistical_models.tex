\chapter{Parametric Statistical Models}
\label{cha:param-stat-models}

Let $Y_{1}, \dots, Y_{n}$ observations.

\begin{exmp}
  \label{defn:parametric_statistical_models:3}
  $Y_{1} = (Z_{i}, X_{i})$ where the $Z_{i}$'s are response variables,
  and the covariates $X_{i}$ are related to $Z_{i}$ by the regression
  relationship $Z_{i} = g(X_{i}, \theta) + \epsilon_{i}$ for $\theta
  \in \Theta \subseteq \R^{p}$, $\epsilon_{i}$ IID with
  $\E{\epsilon_{i}} = 0$, and $g: X \times \Theta \rightarrow \R$.

  A regression function (possibly non-linear) and known - for example,
  \begin{equation}
    \label{eq:14}
    g(X_{i}, \theta) = X_{i}^{T} \theta,
  \end{equation} a linear model.
\end{exmp}


A natural way to estimate $\theta$ is by nonlinear least squares (NLS)
which finds $\hat \theta$ that minimizes
\begin{equation}
  \label{eq:20}
  Q_{n}(\theta) = \frac{1}{n} \sum_{i=1}^{n}(Z_{i}- g(X_{i}, \theta))^{2}
\end{equation}

\begin{exmp}
  \label{defn:parametric_statistical_models:2}
  We are given a model of PDF/PMF's $\{ f(\cdot, \theta): \theta \in
  \Theta \}, \Theta \subseteq \R^{p}$ for the distribution of a random
  variable $Y$.  We view $Y_{1}, \dots, Y_{n}$ as IID copies of $Y$.

  The likelihood function of the model  is defined as
  \begin{equation}
    \label{eq:30}
    L_{n}(\theta) = \Pi_{i=1}^{n} f(Y_{i}, \theta)
  \end{equation}

  The log-likelihood function $l_{n}(\theta) = \log L_{n}(\theta)$.  A
  \textbf{maximum likelihood estimator} (MLE) is any value $\hat
  \theta = \hat \theta_{MLE} \in \Theta$ that maximizes
  $L_{n}(\theta)$ over $\Theta$.  Equivalently, we minimize
  \begin{equation}
    \label{eq:31}
    Q_{n}(\theta) = -\frac{1}{n} l_{n}(\theta) = - \frac{1}{n}
    \sum_{i=1}^{n} \log f(Y_{i}, \theta)
  \end{equation}
\end{exmp}

\section{Consistency of M-Estimators}
\label{sec:cons-m-estim}

In both the examples, $\hat \theta_{n}$ is found by minimizing a
random criterion function $Q_{n}(\theta)$ over $\Theta$, and proved a
``limiting function'' $Q(\theta)$ exists, we expect these minimizers
to converge to the minimizers of $Q$.

\begin{thm}
  \label{defn:parametric_statistical_models:1}
  Let $\Theta \subseteq \R^{p}$ be compact.  Let $Q: \Theta
  \rightarrow \R$ be a continuous, non-random function that has a
  unique minimizer $\theta_{0} \in \Theta$.

  Let $Q_{n}: \Theta \rightarrow \R$ be any sequence of random
  functions such that
  \begin{equation}
    \label{eq:32}
    \sup_{\theta \in \Theta} |Q_{n}(\theta) - Q(\theta)| \cp 0
  \end{equation} as $n \rightarrow \infty$.

  If $\theta_{n}$ is \textbf{any} sequence of minimizers of $Q_{n}$,
  then $\hat \theta_{n} \cp \theta_{0}$ as $n \rightarrow \infty$.
\end{thm}

\begin{proof}
  Let $\epsilon > 0$ be arbitrary. The set $\Theta_{\epsilon} = \{
  \theta \in \Theta : \| \theta - \theta_{0} \| \geq \epsilon \}$ is
  compact and $Q$ is continuous on $\Theta_{\epsilon}$, so $Q$ attains
  its infimum
  \begin{equation}
    \label{eq:33}
    c(\epsilon) = \inf_{\theta \in \Theta_{\epsilon}} Q(\theta) =
    Q(\bar \theta_{\epsilon}) \in \Theta_{\epsilon} > Q(\theta_{0})
  \end{equation} as $\theta_{0}$ is the minimizer.

  Pick $0 < \delta(\epsilon) < \frac{c(\epsilon) - Q(\theta_{0})}{2}$,
  which implies
  \begin{equation}
    \label{eq:34}
    c(\epsilon) - \delta(\epsilon) > Q(\theta_{0}) + \delta(\epsilon)
  \end{equation}

  Define the event
  \begin{equation}
    \label{eq:35}
    A_{n}(\epsilon) = \{ \sup_{\theta \in \Theta} |Q_{n}(\theta) -
    Q(\theta)| < \delta(\epsilon) \}.
  \end{equation}  On this event we have
  \begin{align*}
    \inf_{\theta \in \Theta_{\epsilon}} Q_{n}(\theta) & = \inf_{\theta
      \in \Theta_{\epsilon}} [Q_{n}(\theta) - Q(\theta) + Q(\theta)]
                                                                                              \\
                                                      & \geq \inf_{\theta \in \Theta_{\epsilon}} Q(\theta) - \sup_{\theta
      \in \Theta} |Q_{n}(\theta) - Q(\theta)|                                                 \\
                                                      & \geq C(\epsilon) - \delta(\epsilon)   \\
                                                      & \geq Q(\theta_{0}) + \delta(\epsilon) \\
                                                      & \geq Q(\theta_{0}) + \delta(\epsilon) - |Q_{n}(\theta_{0}) -
    Q(\theta_{0})|                                                                            \\
                                                      & \geq Q_{n}(\theta_{0})
  \end{align*} since on $A_{n}(\epsilon)$, in particular
  $|Q_{n}(\theta_{0}) - Q(\theta_{0})| < \delta(\epsilon)$.

  We conclude
  \begin{equation}
    \label{eq:36}
    \inf_{\theta: \|\theta - \theta_{0} \| \geq \epsilon}
    Q_{n}(\theta) > Q_{n}(\theta_{0})
  \end{equation}

  Now suppose $\hat \theta_{n} \in \Theta_{\epsilon}$, then
  $Q_{n}(\hat \theta_{n}) \geq \inf_{\theta \in \Theta_{\epsilon}}
  Q_{n}(\theta) > Q_{n}(\theta_{0})$.

  Hence, on $A_{n}(\epsilon)$, we have $\| \hat \theta_{n} -
  \theta_{0}\| < \epsilon$, $A_{n}(\epsilon) \subseteq \{ \| \hat
  \theta_{n} - \theta_{0} \| < \epsilon \}$, so since by hypothesis
  $\Prob{A_{n}(\epsilon)} \rightarrow 1$ for all $\epsilon > 0$, we
  see $\Prob{\| \hat \theta_{n} - \theta_{0} \| < \epsilon}
  \rightarrow 1$, as $\Prob{\| \hat \theta_{n} - \theta_{0} \| \geq
    \epsilon} \rightarrow 0$ as $n \rightarrow \infty$. Since
  $\epsilon > 0$ was arbitrary, the result follows.
\end{proof}

\begin{remark}
  Uniform convergence of $Q_{n} \rightarrow Q$ is necessary. In fact,
  none of the conditions can be relaxed.
\end{remark}

\begin{exer}
  \begin{enumerate}
  \item What is $Q$ in Examples
    \ref{defn:parametric_statistical_models:3}, \ref{defn:parametric_statistical_models:2}?
  \item What is $\Theta_{0}$?
  \item When does uniform convergence occur?
  \end{enumerate}
\end{exer}

\begin{exmp}
  \label{defn:parametric_statistical_models:4}
  Let $Y = (Z, X)$ such that $Z = g(X, \theta_{0}) + \epsilon$, where
  $\E{\epsilon | X} = 0$, $\theta_{0}$ is the ``true value'', and
  based on \iid observations $Y_{1}, \dots, Y_{n}$, we minimize
  \begin{equation}
    \label{eq:38}
    Q_{n}(\theta)= \frac{1}{n} \sum_{i=1}^{n} (Z_{i} - g(X_{i}, \theta))^{2}
  \end{equation} over $\Theta$.  We expect
  \begin{equation}
    \label{eq:40}
    Q(\theta) = \E{(Z - g(X, \theta))^{2}}{\theta_{0}}
  \end{equation}  Inserting the model equation
  \begin{equation}
    \label{eq:41}
    Q(\theta) = \E{(g(X_{1}, \theta_{0}) - g(X,
    \theta) + \epsilon)^{2}}{\theta_{0}} = \E{g(X, \theta_{0}) - g(X, \theta)}^{2}
    + \E{\epsilon^{2}}
  \end{equation}

  Hence $Q(\theta)$ is minimized at $\theta_{0}$ if the regression
  parameterization is identifiable, that is
  \begin{equation}
    \label{eq:42}
    \theta = \theta' \iff g(\cdot, \theta) = g(\cdot, \theta')
  \end{equation} ${\Prob}_{X}$ almost surely.
\end{exmp}

\begin{exmp}
  \label{defn:parametric_statistical_models:5}
  Let $Y_{1}, \dots, Y_{n}$ be \iid copies of $Y$, and we maintain a
  parametric model
  \begin{equation}
    \label{eq:43}
    \{ f(\cdot, \theta): \theta \in \Theta \}
  \end{equation} of PDFs/PMFs and the MLE is found by minimizing
  \begin{equation}
    \label{eq:44}
    Q_{n}(\theta) = - \frac{1}{n} \sum_{i=1}^{n} \log f(Y_{i}, \theta)
  \end{equation}

  By the law of large numbers, assuming $f(y, \theta) > 0$ for all $y,
  \theta$ and
  \begin{equation}
    \label{eq:45}
    \E{|\log f(Y, \theta)|}{\theta_{0}} < \infty
  \end{equation} where $Y$ is assumed to be distributed as $f(\cdot,
  \theta_{0})$, then the limiting criterion function is
  \begin{equation}
    \label{eq:46}
    Q(\theta) = - \E{\log f(Y, \theta)}{\theta_{0}}
  \end{equation}

  Then
  \begin{align}
    \label{eq:47}
    Q(\theta_{0}) - Q(\theta) &= \mathbb{E}_{\theta_{0}} \log f(Y,
    \theta) - \mathbb{E}{\theta_{0}} - \log f(Y, \theta_{0}) \\
    &= \E{\log \frac{f(Y, \theta)}{f(Y, \theta_{0})}}{\theta_{0}} \\
    &\leq \log \E{\frac{f(Y, \theta)}{f(Y, \theta_{0})}}{\theta_{0}}
  \\
  &= \log \int \frac{f(y, \theta)}{f(y, \theta_{0})} f(y, \theta_{0})
  \, dy \\
  &= \log 1 \\
  &= 0
  \end{align} or in other words,
  \begin{equation}
    \label{eq:48}
    Q(\theta_{0}) \leq Q(\theta) \forall \theta \in \Theta
  \end{equation}
\end{exmp}

Equality in Jensen's inequality can only occur when
\begin{equation}
  \label{eq:49}
  \frac{f(\cdot, \theta)}{f(\cdot, \theta_{0})} = C \in \R
\end{equation} so since $\int f(y, \theta) dy = 1$, we see $C = 1$,
and hence if the model is identifiable in the sense that $\theta =
\theta' \iff f(\cdot, \theta) = f(\cdot, \theta')$ for all $\theta,
\theta' \in \Theta$, then the value $\theta_{0}$ that minimizes
$\theta_{0}$ is unique.

\section{Verifying uniform convergence}
\label{sec:verify-unif-conv}

\begin{proposition}
  Let $\Theta$ be compact in $\R^{p}$, and let $\mathcal{X} \subseteq \R^{d}$
  and consider observing $X_{1}, \dots, X_{n}$ \iid from $X \sim
  \Prob$ on $X$.  Let $q: \mathcal{X} \times \Theta \rightarrow \R$
  that is continuous in $\theta$ for all $x$ and measurable in $x$ for
  all $\theta \subseteq \Theta$.

  Assume
  \begin{equation}
    \label{eq:50}
    \E{\sup_{\theta \in \Theta} | q(X, \theta)|} < \infty
  \end{equation}

  Then
  \begin{equation}
    \label{eq:51}
    \sup_{\theta \in \Theta} | \frac{1}{n} q(X_{i}, \theta) - \E{q(X,
      \theta)} | \cas 0
  \end{equation} as $n \rightarrow \infty$
\end{proposition}

\begin{proof}
  We apply the uniform law of large numbers from Proposition
  \ref{defn:ulln}  and we need to cover the set
  \begin{equation}
    \label{eq:52}
    \mathcal{H} = \{ q(\cdot, \theta) : \theta \in \Theta \}
  \end{equation} by suitable brackets.

  Define open balls
  \begin{equation}
    \label{eq:53}
    B(\theta, m) = \{ \theta' \in \Theta: \| \theta -  \theta' \| <
    \eta \}
  \end{equation}
  Construct ``brute-force'' brackets
  \begin{align}
    \label{eq:54} \overline q(X, \theta, \eta) = \sup_{\theta' \in
      B(\theta, \epsilon)} q(X, \theta') \\ \underline q(X, \theta,
    \eta) = \inf_{\theta' \in B(\theta, \eta)} q(X, \theta')
  \end{align} which obviously cover all the $\{ q(\cdot, \theta') :
  \theta' \in B(\theta, \eta) \}$.

  Clearly,
  \begin{align}
    \label{eq:55} \E{\overline{\underline q}(x, \theta, \eta)} \leq
    \E{\sup_{\theta \in \Theta} | q(X, \theta) |} < \infty
  \end{align} by the domination condition.

  By continuity and compactness, the supremum/infimum above are
  attained at $\overline \theta, \underline \theta \in \Theta$ such
  that $\| \overline \theta - \theta \| \leq \eta$. So
  \begin{align}
    \label{eq:56} | \overline q (X, \theta, \eta) - \underline q(X,
    \theta, \eta) | &\leq | \overline q(X, \theta, \eta) - q(X,
    \theta) | + | q(X, \theta) - \underline q (X, \theta, \eta) |
  \end{align} which again by continuity tends to zero as $\eta
  \rightarrow 0$.

  So $| \overline q(X, \theta, \eta) - \underline q(X, \theta, \eta)|
  \rightarrow 0$ as $\eta \rightarrow 0$.

  By the dominated convergence theorem we can integrat this limit with
  respect to $\E$, (using the dominance condition).  So,
  \begin{equation}
    \label{eq:57}
    \E{|\overline q(X, \theta, \eta) - \underline q(X, \theta, \eta)}
    \rightarrow 0
  \end{equation} as $\eta \rightarrow 0$.

  Then for all $\epsilon > 0$, there exists $\eta = \eta(\epsilon,
  \theta)$ such that
  \begin{equation}
    \label{eq:58}
    \E{|\overline q(X, \theta, \eta(\epsilon, \theta)) - \underline
      q(X, \theta, \eta(\epsilon, \theta))|} < \epsilon
  \end{equation}

  The balls $\{ B(\theta, \eta(\epsilon, \theta)) : \theta \in \Theta
  \}$ form an open covering of $\Theta$, so by compactness
  (Heine-Borel theorem in $\R^{p}$), there exists a finite subcover of
  $\theta$, say with centres $\theta_{1}, \dots,
  \theta_{N(\epsilon)}$.  Then the corresponding brackets
  \begin{equation}
    \label{eq:59}
    [\underline q_{i}, \overline q_{i}] = [ \underline q(\cdot,
    \theta_{j}, \eta(\epsilon, \theta_{j})), \overline q(\cdot,
    \theta_{j}, \eta(\epsilon, \theta_{j}))]
  \end{equation} cover $\mathcal{H}$ and satisfy the conditions of
  Proposition \ref{defn:ulln}
\end{proof}

\begin{remark}
  The above result is simply a law of large numbers in the Banach
  space of continuous functions on $\Theta$, and
  \begin{equation}
    \label{eq:60}
    \E{\sup_{\theta \in \Theta} | q(X, \Theta) |} = \E{\|Z\|} < \infty
  \end{equation} which is necessary for the result to hold.
\end{remark}





%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "master"
%%% End: 
