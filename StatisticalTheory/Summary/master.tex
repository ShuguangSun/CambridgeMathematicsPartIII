\input{../../common/summary.tex}

\title{Statistical Theory Summary}

\begin{document}

\maketitle

\section{Basic Concepts}

\begin{defn}[Convergence almost surely]
  A sequence $X_{n}, n \in \mathbb{N}$ of random variables converges
  almost surely to a random variable $X$ if
  \begin{equation}
    \label{eq:4}
    \Prob{X_{n} \rightarrow X} = \mu(\omega \in \Omega | X_{n}(\omega)
    \rightarrow X(\omega)) = 1
  \end{equation}

  We say that $X_{n} \cas X$.
\end{defn}

\begin{defn}[Convergence in probability]
  $X_{n} \cp X$ (in probability) if for all $\epsilon > 0$,
  \begin{equation}
    \label{eq:5}
    \Prob{|X_{n} - X} > \epsilon) \rightarrow 0
  \end{equation} as $n \rightarrow \infty$.
  For random vectors, we define analogously with taking the norm in
  $\mathbb{R}^{n}$. 
\end{defn}

\begin{defn}[Convergence in distribution]
  $X_{n} \cd X$ or $X_{n}$ converges to $X$ in distribution if
  \begin{equation}
    \label{eq:6}
    \Prob{X_{n} \leq t} \rightarrow \Prob{X \leq t}
  \end{equation} whenever $t \mapsto \Prob{X \leq t}$ is continuous.
\end{defn}

\begin{proposition}
  Let $(X_{n}, n \in \N)$, $X$ taking values in $\mathcal{X} \subseteq
  \R^{d}$.
  \begin{enumerate}
  \item
    \begin{equation}
      \label{eq:9}
      X_{n} \cas X \Rightarrow X_{n} \cp X \Rightarrow X_{n} \cd X
    \end{equation}
  \item If $X_{n} \rightarrow X$ in any mode, and if $g: \mathcal{X}
    \rightarrow \R^{d}$ is continuous, then $g(X_{n}) \rightarrow
    g(X)$in the same mode.
  \item \textbf{Slutsky's lemma} If $X_{n} \cd X$ and $Y_{n} \cd c$ (a
    constant). Then
    \begin{enumerate}
    \item
      \begin{equation}
        \label{eq:10}
        Y_{n} \cp c
      \end{equation}
    \item
      \begin{equation}
        \label{eq:11}
        X_{n} + Y_{n}\cd X + c
      \end{equation}

    \item
      \begin{equation}
        \label{eq:12}
        X_{n} Y_{n} \cd cX
      \end{equation} where $Y_{n} \in \R$.
    \item
      \begin{equation}
        \label{eq:13}
        X_{n} Y_{n}^{{-1}} \cd c^{-1} X
      \end{equation} where $Y_{n} \in \R, c \neq 0$.
    \end{enumerate}
  \item If $(A_{n}, n \in \N)$ are random matrices with
    $(A_{n})_{{ij}} \cp A_{ij}$ for all $i, j$ and $X_{n} \cd X$, then
    $A_{n} X_{n} \cd AX$, and if $A$ is invertible, $A_{n}^{-1} X_{n}
    \cd A^{-1}X$, where $A = (A_{ij})$.
  \end{enumerate}
\end{proposition}

\begin{thm}[Law of Large Numbers]
  \label{defn:stochastic_convergence_concepts:1}
  let $X_{1}, \dots, X_n$ be IID copies of $X ~ \Prob$ such that
  $\E{|X_{i}|} < \infty$, then
  \begin{equation}
    \label{eq:18}
    \frac{1}{n} \sum_{i=1}^{n} X_{i} \cas \E{X}
  \end{equation}
\end{thm}

\begin{thm}[Central limit theorem]
  \label{defn:stochastic_convergence_concepts:2}
  Let $X_{1}, \dots, X_{n}$ be IID copies of $X \sim \Prob$ on $\R$
  with $\Var{X} = \sigma^{2} < \infty$. Then
  \begin{equation}
    \label{eq:19}
    \sqrt{n}\left(\frac{1}{n} \sum_{i=1}^{n} X_{i} - \E{X}\right) \cd N(0, \sigma^{2})
  \end{equation}

  In the multivariate case, where $X \sim \Prob$ on $\R^{d}$ with the
  covariance of $X$ as $\Sigma$, then
  \begin{equation}
    \label{eq:21}
    \sqrt{n} \left(\frac{1}{n} \sum_{i=1}^{n} X_{i} - \E{X}\right) \cd N(0, \Sigma)
  \end{equation}
\end{thm}

\section{Uniform Laws of Large Numbers}
\label{sec:uniform-laws-large}

\begin{thm}
  Let $\mathcal{H}$ be a class of functions from a measurable space
  $T$ to $\R$. Assume that for every $\epsilon > 0$ there exitss a
  finite set of brackets $[l_{j}, u_{j}]$, $j = 1, \dots,
  N(\epsilon)$, such that $\E{|l_{j}(X)|} < \infty$, $\E{|u_{j}(X)|} <
  \infty$, and $\E{|u_{j}(X) - l_{j}(X)|} < \epsilon$ for every $j$.
  Suppose moreover that for every $h \in H$ there exists $j$ with $h
  \in [l_{j}, u_{j}] \iff h \in \{ f: T \rightarrow \R | l(x) \leq
  f(x) \leq u(x), \forall x \in T \}$. Then we have a \textbf{uniform
    law of large numbers},
  \begin{equation}
    \label{eq:1}
    \sup_{h \in H} \left| \frac{1}{n} \sum_{i=1}^{n} (h(X_{i}) -
      \E{h(X)}) \right| \cas 0
  \end{equation}
\end{thm}

\section{Consistency of $M$-estimators}
\label{sec:cons-m-estim}

\begin{thm}
  \label{defn:parametric_statistical_models:1}
  Let $\Theta \subseteq \R^{p}$ be compact.  Let $Q: \Theta
  \rightarrow \R$ be a continuous, non-random function that has a
  unique minimizer $\theta_{0} \in \Theta$.

  Let $Q_{n}: \Theta \rightarrow \R$ be any sequence of random
  functions such that
  \begin{equation}
    \label{eq:32}
    \sup_{\theta \in \Theta} |Q_{n}(\theta) - Q(\theta)| \cp 0
  \end{equation} as $n \rightarrow \infty$.

  If $\theta_{n}$ is \textbf{any} sequence of minimizers of $Q_{n}$,
  then $\hat \theta_{n} \cp \theta_{0}$ as $n \rightarrow \infty$.
\end{thm}

\section{Verifying uniform convergence}
\label{sec:verify-unif-conv}

\begin{thm}
  Let $\Theta$ be compact in $\R^{p}$, and let $\mathcal{X} \subseteq \R^{d}$
  and consider observing $X_{1}, \dots, X_{n}$ \iid from $X \sim
  \Prob$ on $X$.  Let $q: \mathcal{X} \times \Theta \rightarrow \R$
  that is continuous in $\theta$ for all $x$ and measurable in $x$ for
  all $\theta \subseteq \Theta$.

  Assume
  \begin{equation}
    \label{eq:50}
    \E{\sup_{\theta \in \Theta} | q(X, \theta)|} < \infty
  \end{equation}

  Then
  \begin{equation}
    \label{eq:51}
    \sup_{\theta \in \Theta} | \frac{1}{n} q(X_{i}, \theta) - \E{q(X,
      \theta)} | \cas 0
  \end{equation} as $n \rightarrow \infty$
\end{thm}

\begin{thm}
  Consider the model $f(\theta, y)$, $\theta \in \Theta \subseteq
  \R^{p}$, $y \in \mathcal{Y} \subset \R^{d}$.  Assume $f(\theta, y) >
  0$ for all $y \in \mathcal{Y}$ and all $\theta \in \Theta$, and that
  $\int_{\mathcal{Y}} f(\theta, y) dy = 1$ for every $\theta \in
  \Theta$. Assume further that $\Theta$ is compact and that the map
  $\theta \mapsto f(\theta, y)$ is continuous on $\Theta$ for every $y
  \in \mathcal{Y}$.  Let $Y_{1}, \dots, Y_{n}$ be \iid with common
  density $f(\theta_{0})$, where $\theta_{0} \in \Theta$.  Suppose
  finally that the identification condition 13 and the domination
  condition
  \begin{equation}
    \label{eq:2}
    \int_{\mathcal{Y}} \sup_{\theta' \in \Theta} | \log f(\theta', y)
    | f(\theta_{0}, y) dy < \infty
  \end{equation} hold.  If $\hat \theta_{n}$ is the MLE in the model
  $\{ f(\theta, \cdot) | \theta \in \Theta \} $ based on the sample
  $Y_{1}, \dots, Y_{n}$, then $\hat \theta_{n}$ is consistent, in that
  $\hat \theta_{n} \rightarrow^{p_{\theta_{0}}} \theta_{0}$ as $n
  \rightarrow \infty$.
\end{thm}

\section{Asymptotic Distribution Theory}
\label{sec:asympt-distr-theory}



\bibliographystyle{plainnat}
\bibliography{../../common/bibliography}
\end{document}
