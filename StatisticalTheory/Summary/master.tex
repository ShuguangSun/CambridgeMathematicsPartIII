\input{../../common/summary.tex}

\title{Statistical Theory Summary}

\begin{document}

\maketitle

\section{Basic Concepts}

\begin{defn}[Convergence almost surely]
  A sequence $X_{n}, n \in \mathbb{N}$ of random variables converges
  almost surely to a random variable $X$ if
  \begin{equation}
    \label{eq:4}
    \Prob{X_{n} \rightarrow X} = \mu(\omega \in \Omega | X_{n}(\omega)
    \rightarrow X(\omega)) = 1
  \end{equation}
  We say that $X_{n} \cas X$.
\end{defn}

\begin{defn}[Convergence in probability]
  $X_{n} \cp X$ (in probability) if for all $\epsilon > 0$,
  \begin{equation}
    \label{eq:5}
    \Prob{|X_{n} - X| > \epsilon} \rightarrow 0
  \end{equation} as $n \rightarrow \infty$.
  For random vectors, we define analogously with taking the norm in
  $\mathbb{R}^{n}$.
\end{defn}

\begin{defn}[Convergence in distribution]
  $X_{n} \cd X$ or $X_{n}$ converges to $X$ in distribution if
  \begin{equation}
    \label{eq:6}
    \Prob{X_{n} \leq t} \rightarrow \Prob{X \leq t}
  \end{equation} whenever $t \mapsto \Prob{X \leq t}$ is continuous.
\end{defn}

\begin{proposition}
  Let $(X_{n}, n \in \N)$, $X$ taking values in $\mathcal{X} \subseteq
  \R^{d}$.
  \begin{enumerate}
  \item
    \begin{equation}
      \label{eq:9}
      X_{n} \cas X \Rightarrow X_{n} \cp X \Rightarrow X_{n} \cd X
    \end{equation}
  \item If $X_{n} \rightarrow X$ in any mode, and if $g: \mathcal{X}
    \rightarrow \R^{d}$ is continuous, then $g(X_{n}) \rightarrow
    g(X)$in the same mode.
  \item \textbf{Slutsky's lemma} If $X_{n} \cd X$ and $Y_{n} \cd c$ (a
    constant). Then
    \begin{enumerate}
    \item  $Y_{n} \cp c$
    \item $X_{n} + Y_{n}\cd X + c$
    \item $X_{n} Y_{n} \cd cX$ where $Y_{n} \in \R$.
    \item $X_{n} Y_{n}^{{-1}} \cd c^{-1} X$ where $Y_{n} \in \R, c
      \neq 0$.
    \end{enumerate}
  \item If $(A_{n}, n \in \N)$ are random matrices with
    $(A_{n})_{{ij}} \cp A_{ij}$ for all $i, j$ and $X_{n} \cd X$,
    then $A_{n} X_{n} \cd AX$, and if $A$ is invertible, $A_{n}^{-1}
    X_{n} \cd A^{-1}X$, where $A = (A_{ij})$.
  \end{enumerate}
\end{proposition}

\begin{thm}[Law of Large Numbers]
  \label{defn:stochastic_convergence_concepts:1}
  let $X_{1}, \dots, X_n$ be IID copies of $X ~ \Prob$ such that
  $\E{|X_{i}|} < \infty$, then
  \begin{equation}
    \label{eq:18}
    \frac{1}{n} \sum_{i=1}^{n} X_{i} \cas \E{X}
  \end{equation}
\end{thm}

\begin{thm}[Central limit theorem]
  \label{defn:stochastic_convergence_concepts:2}
  Let $X_{1}, \dots, X_{n}$ be IID copies of $X \sim \Prob$ on $\R$
  with $\Var{X} = \sigma^{2} < \infty$. Then
  \begin{equation}
    \label{eq:19}
    \sqrt{n}\left(\frac{1}{n} \sum_{i=1}^{n} X_{i} - \E{X}\right) \cd N(0, \sigma^{2})
  \end{equation}

  In the multivariate case, where $X \sim \Prob$ on $\R^{d}$ with the
  covariance of $X$ as $\Sigma$, then
  \begin{equation}
    \label{eq:21}
    \sqrt{n} \left(\frac{1}{n} \sum_{i=1}^{n} X_{i} - \E{X}\right) \cd N(0, \Sigma)
  \end{equation}
\end{thm}

\begin{thm}[Gaussian Tail Inequality]
  If $X \sim N(0, 1)$, then
  \begin{align}
    \label{eq:68}
    \Prob{|X| > \epsilon} \leq \frac{2e^{-\epsilon^{2}}}{\epsilon}.
  \end{align}
\end{thm}

\begin{thm}[Chebyshev's Inequality]
  Let $\mu = \E{X}$, $\sigma^{2} = \Var{X}$. Then
  \begin{align}
    \label{eq:69}
    \Prob{|X - \mu | \geq t} \leq \frac{\sigma^{2}}{t^{2}}
  \end{align}
\end{thm}

\begin{thm}[Markov's Inequality]
  Let $X$ be a non-negative random variable and suppose $E{X}$ exists.
  Then for any $t > 0$,
  \begin{align}
    \label{eq:67}
    \Prob{X > t} \leq \frac{\E{X}}{t}
  \end{align}
\end{thm}

\begin{thm}[Hoeffding's Inequality]
  Suppose $E{X} = 0$, and $a \leq X \leq b$. Then
  \begin{align}
    \label{eq:70}
    \E{e^{tX}} \leq e^{\frac{t^{2}(b-a)^{2}}{8}}.
  \end{align}
\end{thm}

\section{Uniform Laws of Large Numbers}
\label{sec:uniform-laws-large-1}

\begin{thm}
  Let $\mathcal{H}$ be a class of functions from a measurable space
  $T$ to $\R$. Assume that for every $\epsilon > 0$ there exists a
  finite set of brackets $[l_{j}, u_{j}]$, $j = 1, \dots,
  N(\epsilon)$, such that $\E{|l_{j}(X)|} < \infty$, $\E{|u_{j}(X)|} <
  \infty$, and $\E{|u_{j}(X) - l_{j}(X)|} < \epsilon$ for every $j$.
  Suppose moreover that for every $h \in H$ there exists $j$ with $h
  \in [l_{j}, u_{j}] \iff h \in \{ f: T \rightarrow \R | l(x) \leq
  f(x) \leq u(x), \forall x \in T \}$. Then we have a \textbf{uniform
    law of large numbers},
  \begin{equation}
    \label{eq:1}
    \sup_{h \in H} \left| \frac{1}{n} \sum_{i=1}^{n} (h(X_{i}) -
      \E{h(X)}) \right| \cas 0
  \end{equation}
\end{thm}

\begin{proof}
  Let $\epsilon > 0$, and choose brackets $[l_{j}, u_{j}]$ such that
  $\E{|u_{j} - l_{j}}(X) < \frac{\epsilon}{2}$ by hypothesis. Then for
  every $\omega \in T^{\infty}$ outside a null set $A$, there exists
  $n_{0}(\omega, \epsilon)$ such that
  \begin{align}
    \label{eq:41}
    \max_{j = 1, \dots, N(\frac{\epsilon}{2})} |\sum_{i=1}^{n} u_{j} -
    \E{u_{j}}| < \frac{\epsilon}{2}
  \end{align} by the strong law of large numbers and taking a union
  over all $j$.

  Then this gives for $h \in \mathcal{H}$, outside a set of zero
  measure,
  \begin{align}
    \label{eq:42}
    \frac{1}{n} \sum_{i=1}^{n} h(X_{i}) - \E{h(X)} & \frac{1}{n} \sum_{i=1}^{n} u_{j} -  \E{h} \\
    &= \frac{1}{n} \sum_{i=1}^{n} u_{j} - \E{u_{j}} + \E{u_{j}} - \E{h}
  \end{align} which is bounded above by $\frac{\epsilon}{2} +
  \frac{\epsilon}{2} = \epsilon$, as required.
\end{proof}

\section{Consistency of $M$-estimators}
\label{sec:cons-m-estim-1}

\begin{thm}
  \label{defn:parametric_statistical_models:1}
  Let $\Theta \subseteq \R^{p}$ be compact.  Let $Q: \Theta
  \rightarrow \R$ be a continuous, non-random function that has a
  unique minimizer $\theta_{0} \in \Theta$.

  Let $Q_{n}: \Theta \rightarrow \R$ be any sequence of random
  functions such that
  \begin{equation}
    \label{eq:32}
    \sup_{\theta \in \Theta} |Q_{n}(\theta) - Q(\theta)| \cp 0
  \end{equation} as $n \rightarrow \infty$.

  If $\theta_{n}$ is \textbf{any} sequence of minimizers of $Q_{n}$,
  then $\hat \theta_{n} \cp \theta_{0}$ as $n \rightarrow \infty$.
\end{thm}

\begin{proof}
  The key is to consider the set $S_{\epsilon} = \{ \theta \in \Theta
  : \| \theta - \theta_{0} \| \geq \epsilon \} $.  This is compact,
  with $Q$ continuous on this set, and so an infimum
  $Q(\theta_{\epsilon}) > Q(\theta_{0})$ is attained on this set.
  Choose $\delta > 0$ so $Q(\theta_{\epsilon}) - \delta >
  Q(\theta_{0}) + \delta$.  Then consider the set $A_{n}(\epsilon) =
  \{ \sup_{\theta \in \Theta} | Q_{n}(\theta) - Q(\theta) | < \delta
  \} $.  On this set, we have
  \begin{align}
    \label{eq:43}
    \inf_{S_{\epsilon}} Q_{n}(\theta) &\geq \inf_{S_{\epsilon}}
    Q(\theta) - \delta = Q(\theta_{\epsilon}) - \delta > Q(\theta_{0})
    + \delta \geq Q_{n}(\theta_{0}).
  \end{align}
  So if $\hat \theta_{n}$ lay in $S_{\epsilon}$, then
  $Q_{n}(\delta_{0})$ would be strictly smaller than $Q_{n}(\hat
  \theta_{n})$, contradicting that $\hat \theta_{n}$ is a minimizer.
  Conclude that $A_{n}(\epsilon) \Rightarrow \| \hat \theta_{n} -
  \theta_{0} \| < \epsilon$, but as $\Prob{A_{n}(\epsilon)}
  \rightarrow 1$, we have $\Prob{\| \hat \theta_{n} - \theta_{0} \| <
    \epsilon} \rightarrow 1$.
\end{proof}

\begin{thm}
  Let $\Theta$ be compact in $\R^{p}$, and let $\mathcal{X} \subseteq \R^{d}$
  and consider observing $X_{1}, \dots, X_{n}$ \iid from $X \sim
  \Prob$ on $X$.  Let $q: \mathcal{X} \times \Theta \rightarrow \R$
  that is continuous in $\theta$ for all $x$ and measurable in $x$ for
  all $\theta \subseteq \Theta$.

  Assume
  \begin{equation}
    \label{eq:50}
    \E{\sup_{\theta \in \Theta} | q(X, \theta)|} < \infty
  \end{equation}

  Then
  \begin{equation}
    \label{eq:51}
    \sup_{\theta \in \Theta} | \frac{1}{n} q(X_{i}, \theta) - \E{q(X,
      \theta)} | \cas 0
  \end{equation} as $n \rightarrow \infty$
\end{thm}

\begin{proof}
  We seek to find suitable bracketing functions and proceed via the
  uniform law of large numbers. First, define the brackets $u(x,
  \theta, \eta) = \sup_{\theta' \in B(0, \eta)}$, so $\E{|u(X, \theta,
    \eta)|} < \infty$ by assumption. By continuity of $q(\cdot, x)$,
  the supremum is achieved at points $\| \theta^{u}(\theta) - \theta
  \| < \eta$, and so $\lim_{\eta \rightarrow 0} |u(x, \theta, \eta) -
  q(\theta, x) \rightarrow 0$ at every $x$ and every $\theta$, and
  using the dominated convergence theorem gives $\lim_{\eta
    \rightarrow 0}\E{|u(X, \theta, \eta) - q(\theta, X)} \rightarrow
  0$.

  Then for $\epsilon > 0$ and $\theta \in \Theta$ we can choose
  $\eta(\epsilon, \theta)$ small so that
  \begin{equation}
    \E{u(X, \theta, \eta) - l(X, \theta, \eta)} < \epsilon.
  \end{equation}
  The open balls $\{ B(\theta, \eta(\epsilon, \theta)) \} $ are an
  open cover of the compact set $\Theta$, so there exists a finite
  subcover by Heine-Borel. This finite subcover $u_{j}(\cdot,
  \theta_{j}, \eta(\epsilon, \theta_{j}))$ constitutes a bracketing
  set of the $q$, and so we apply the uniform law of large numbers.
\end{proof}

\begin{thm}[Consistency of the Maximum Likelihood Estimator]
  Consider the model $f(\theta, y)$, $\theta \in \Theta \subseteq
  \R^{p}$, $y \in \mathcal{Y} \subset \R^{d}$.  Assume $f(\theta, y) >
  0$ for all $y \in \mathcal{Y}$ and all $\theta \in \Theta$, and that
  $\int_{\mathcal{Y}} f(\theta, y) dy = 1$ for every $\theta \in
  \Theta$. Assume further that $\Theta$ is compact and that the map
  $\theta \mapsto f(\theta, y)$ is continuous on $\Theta$ for every $y
  \in \mathcal{Y}$.  Let $Y_{1}, \dots, Y_{n}$ be \iid with common
  density $f(\theta_{0})$, where $\theta_{0} \in \Theta$.  Suppose
  finally that the identification condition 13 and the domination
  condition
  \begin{equation}
    \label{eq:2}
    \int_{\mathcal{Y}} \sup_{\theta' \in \Theta} | \log f(\theta', y)
    | f(\theta_{0}, y) dy < \infty
  \end{equation} hold.  If $\hat \theta_{n}$ is the MLE in the model
  $\{ f(\theta, \cdot) | \theta \in \Theta \} $ based on the sample
  $Y_{1}, \dots, Y_{n}$, then $\hat \theta_{n}$ is consistent, in that
  $\hat \theta_{n} \rightarrow^{p_{\theta_{0}}} \theta_{0}$ as $n
  \rightarrow \infty$.
\end{thm}

\begin{proof}
  Setting
  \begin{align}
    q(\theta, y) &= -\log f(\theta, y), \\
    Q(\theta) &= \E_{\theta_{0}}q(\theta, Y), \\
    Q_{n}(\theta) &= \frac{1}{n} \sum_{i=1}^{n} q(\theta, Y_{i}),
  \end{align}
  this follows from the previous results.
\end{proof}

\begin{defn}[Uniform Consistency]
  \label{sec:cons-model-ftheta}
  An estimator $T_{n}$ is \textbf{uniformly consistent} in $\theta \in
  \Theta$, if for every $\delta > 0$,
  \begin{equation}
    \label{eq:25}
    \sup_{\theta_{0} \in \Theta} P_{\theta_{0}}(\| T_{n} - \theta_{0}
    \| > \delta) \rightarrow 0
  \end{equation} as $n \rightarrow \infty$.
\end{defn}

\begin{thm}
  An estimator is uniformly consistent if, for every $\epsilon > 0$,
  \begin{align}
    \label{eq:44}
    \inf_{\theta_{0} \in \Theta} \inf_{\theta \in \Theta : \| \theta -
    \theta_{0} \| \geq \epsilon} (Q(\theta) - Q(\theta_{0})) > 0
  \end{align} and that
  \begin{align}
    \label{eq:46}
    \sup_{\theta_{0} \in \Theta} \Prob_{\theta_{0}} (\sup_{\theta \in
      \Theta} |Q_{n}(\theta; Y_{1}, \dots, Y_{n}) - Q(\theta) | >
    \delta) \rightarrow 0.
  \end{align}
\end{thm}

\section{Asymptotic Distribution Theory}
\label{sec:asympt-distr-theory-2}

\begin{thm}
  Consider the model $f(\theta, y), \theta \in \Theta \subset \R^{p}$,
  $y \in \mathcal{Y} \subset \R^{d}$.  Assume $f(\theta, y) > 0$for
  all $y \in \mathcal{Y}$ and all $\theta \in \Theta$, and that
  $\int_{\mathcal{Y}} f(\theta, y) dy = 1$  for every $\theta \in
  \Theta$. Let $Y_{1}, \dots, Y_{n}$ be \iid from density
  $f(\theta_{0}, y)$ for some $\theta_{0} \in \Theta$.  Assume moreover
  \begin{enumerate}
  \item $\theta_{0}$ is an interior point on $\Theta$.
  \item There exists an open set $U$ satisfying $\theta_{0} \in U
    \subset \Theta$ such that $f(\theta, y)$ is, for every $y \in
    \mathcal{Y}$, twice continuously differentiable with respect to
    $\theta$ on $U$,
  \item $\E_{\theta_{0}} \frac{\partial^{2} \log f(\theta_{0},
      Y)}{\partial \theta \partial \theta^{T}} $ is nonsingular, and
    \begin{equation}
      \label{eq:3}
      \E_{\theta_{0}} \| \frac{\partial \log f(\theta_{0},
        Y)}{\partial \theta} \|^{2}  < \infty
    \end{equation}
  \item There exists a compact ball $K \subset U$ with nonempty
    interior centered at $\theta_{0}$ such that
    \begin{align}
      \label{eq:7}
      \E_{\theta_{0}} \sup_{\theta \in K} \| \frac{\partial^{2} \log
        f(\theta, Y)}{\partial \theta \partial \theta^{T}} \| < \infty \\
      \int_{\mathcal{Y}}^{} \sup_{\theta \in K} \| \frac{\partial
        f(\theta, y)}{\partial \theta}  \| dy < \infty \\
      \label{eq:14}
      \int_{\mathcal{Y}}^{} \sup_{\theta \in K} \| \frac{\partial^{2}
        f(\theta, y)}{\partial \theta \partial \theta^{T}}  \| dy < \infty
    \end{align}
  \end{enumerate}

  Let $\hat \theta_{n}$ be the MLE in the model $\{ f(\theta, \cdot);
  \theta \in \Theta \} $ based on the sample $Y_{1}, \dots, Y_{n}$,
  and assume $\hat \theta_{n} \rightarrow^{P_{\theta_{0}}} \theta_{0}$
  as $n \rightarrow \infty$.  Define the Fisher information
  \begin{align}
    \label{eq:15}
    i(\theta_{0}) = \E_{\theta_{0}} \frac{\partial \log f(\theta_{0},
      Y)}{\partial \theta} \frac{\partial \log f(\theta_{0},
      Y)^{T}}{\partial \theta}
  \end{align}
  Then $i(\theta_{0})= - \E_{\theta_{0}} \frac{\partial^{2} \log
    f(\theta_{0}, Y)}{\partial \theta \partial \theta^{T}}$, and
  \begin{align}
    \label{eq:16}
    \sqrt{n} (\hat \theta_{n} - \theta_{0}) \cd N(0, i^{-i}(\theta_{0}))
  \end{align} as $n \rightarrow \infty$.
\end{thm}

\begin{proof}
  First, note that as $\int f(\theta, y) dy = 1$ for all $\theta \in
  J$, then $\frac{\partial}{\partial \theta} \int f(\theta, y) dy =
  \int \frac{\partial \log f(\theta, y)}{\partial \theta} f(\theta, y)
  dy= 0$
  for every $\theta \in \interior K$, so
  \begin{align}
    \label{eq:47}
    \E_{\theta_{0}} \frac{\partial \log f(\theta_{0}, Y)}{\partial
      \theta}  = 0.
  \end{align}

  Since $\hat \theta_{n} \cp \theta_{0}$, we have $\hat \theta_{n}$ is
  an interior point of $\Theta$ on events of probability approaching
  one, so $\frac{\partial Q_{n}(\hat \theta_{n})}{\partial \theta} =
  0$.  Applying the mean value theorem, we have
  \begin{equation}
    \label{eq:48}
    0 = \sqrt{n} \frac{\partial Q_{n}(\theta_{0})}{\partial \theta}  +
    \overline A_{n} \sqrt{n}(\hat \theta_{n} - \theta_{0})
  \end{equation} where $\hat A_{n}$ is the matrix of second
  derivatives of $Q_{n}$ evaluated at a mean value $\overline
  \theta_{nj}$ on the line segment between $\theta_{0}$ and $\hat
  \theta_{n}$.

  \textbf{For the first component}, we have
  \begin{align}
    \label{eq:49}
    \sqrt{n} \frac{\partial Q_{n}(\theta_{0})}{\partial \theta} = -
    \frac{1}{\sqrt{n}} \sum_{i=1}^{n} \frac{\partial \log
      f(\theta_{0}, Y_{i})}{\partial \theta} \cd N(0, i(\theta_{0}))
  \end{align} by the central limit theorem.

  \textbf{For the second component}, we show $\overline A_{n} \cp
  -\E{\frac{\partial^{2} \log f(\theta_{0}, Y)}{\partial
      \theta \partial \theta^{T}}}$, which we do component-wise. We
  have $(\overline A_{n})_{kj} = \frac{1}{n} \sum_{i=1}^{n}
  h_{kj}(\overline \theta_{nj}, Y_{i})$, where $h_{jk}$ is the second
  mixed partial derivative of $-\log f(\theta, Y_{i})$, and we seek to
  show each $h_{jk} \cp \E{h_{jk}(\theta_{0}, Y)}$. This follows by
  \begin{align}
    \label{eq:52}
    &\left| \frac{1}{n} \sum_{i=1}^{n} h_{jk}(\theta_{nj}, Y_{i}) -
      \E{h_{jk}(\theta_{0}, Y)} \right| \\
    \quad &\leq \sup_{\theta \in K}
    \left| \frac{1}{n} \sum_{i=1}^{n} h_{jk}(\theta, Y_{i}) -
      \E{h_{jk}(\theta, Y)}\right| + \left| \E{h_{jk}(\overline
        \theta_{nj}, Y)} - \E{h_{jk}(\theta_{0}, Y)} \right|
  \end{align} then by the uniform law of large numbers, the first term
  converges to zero, and the fact that $\overline \theta_{nj}
  \rightarrow \theta_{0}$ in probability implies the second term
  converges to zero.  Hence, $-\overline A_{n}
  \rightarrow^{p_{\theta_{0}}} \E{\theta_{0}} \frac{\partial^{2} \log
    f(\theta_{0}, Y)}{\partial \theta \partial \theta^{T}} \equiv
  \Sigma(\theta_{0})$.

  As the limit is invertible we have that $\overline A_{n}$ is
  invertible on sets with measure approaching one, so we can rewrite
  the previous result as
  \begin{align}
    \label{eq:53}
    \sqrt{n}(\hat \theta_{n} - \theta_{0}) = -\overline A_{n}^{-1}
    \sqrt{n} \frac{\partial Q_{n}(\theta_{0})}{\partial \theta} \cd
    N(0, \Sigma^{-1}(\theta_{0} i(\theta_{0}) \Sigma^{-1}(\theta_{0}))).
  \end{align} from Slutsky's lemma.

  \textbf{Finally}, we show $\Sigma(\theta_{0}) = i(\theta_{0})$.
  This follows from interchanging integration and differentiation to show
  \begin{equation}
    \label{eq:54}
    \frac{\partial}{\partial \theta^{T}} \int \frac{\partial f(\theta,
      y)}{\partial \theta} dy = \int \frac{\partial^{2} f(\theta,
      y)}{\partial \theta \partial \theta^{T}} dy = 0
  \end{equation} for all $\theta \in \interior K$. Then, use the chain
  rule to show
  \begin{align}
    \label{eq:55}
    \frac{\partial^{2} \log f(\theta, y)}{\partial \theta \partial
      \theta^{T}}  = \frac{1}{f(\theta, y)} \frac{\partial^{2}
      f(\theta, y)}{\partial \theta \partial \theta^{T}}   -
    \frac{\partial \log f(\theta, y)}{\partial \theta}  \frac{\partial
    \log f(\theta, y)}{\partial \theta}^{T}
  \end{align} and using this identity at $\theta_{0}$.
\end{proof}

\begin{thm}
  In the framework of the previous theorem with $p=1$ and for $n \in
  \N$ fixed, let $\tilde \theta = \tilde \theta(Y_{1}, \dots, Y_{n})$,
  be any unbiased estimator of $\theta$ --- that is, it satisfies
  $\E_{\theta} \tilde \theta = \theta$ for all $\theta \in \Theta$.
  Then
  \begin{align}
    \label{eq:17}
    \Var_{\theta}(\tilde \theta_{n}) \geq \frac{1}{ni(\theta)}
  \end{align} for all $\theta \in \interior(\Theta)$.
\end{thm}

\begin{proof}
  Cauchy-Swartz and $\E_{\theta_{0}}\frac{\partial \log f(\theta,
    Y)}{\theta}$.
  Specifically, letting $l(\theta, Y) = \sum_{i=1}^{n}
  \frac{d}{d\theta} \log f(\theta, Y_{i})$,
  \begin{align}
    \label{eq:56}
    \Var_{\theta}(\tilde \theta) \geq \frac{\Cov_{\theta}^{2}(\tilde
      \theta, l'(\theta, Y))}{\Var_{\theta}(l'(\theta, Y))} =
    \frac{1}{ni(\theta)}
  \end{align} since
  \begin{align}
    \label{eq:57}
    \Cov_{\theta}(\tilde \theta, l'(\theta, Y)) &= \int \tilde
    \theta(y) l'(\theta, y) \prod_{i=1}^{n} f(\theta, y_{i}) dy \\
    &= \int \tilde \theta(y) \frac{d}{d\theta}  f(\theta, y) dy =
    \frac{d}{d\theta}  \E_{\theta} \tilde \theta = \frac{d}{d \theta}
    \theta = 1.
  \end{align}
\end{proof}

\begin{thm}[Delta Method]
  Let $\Theta$ be an open subset of $\R^{p}$ and let $\Phi: \Theta
  \rightarrow \R^{m}$ be differentiable at $\theta \in \Theta$, with
  derivative $D \Phi_{\theta}$.  Let $r_{n}$ be a divergent sequence
  of positive real numbers and let $X_{n}$ be random variables taking
  values in $\Theta$ such that $r_{n}(X_{n} - \theta) \cd X$ as $n
  \rightarrow \infty$.  Then
  \begin{equation}
    \label{eq:8}
    r_{n}(\Phi(X_{n}) - \Phi(\theta)) \cd D \Phi_{\theta}(X)
  \end{equation} as $n \rightarrow \infty$.  If $X \sim N(0,
  i^{-1}(\theta))$, then
  \begin{equation}
    \label{eq:20}
    D \Phi_{\theta}(X) \sim N(0, \Sigma(\Phi, \Theta)).
  \end{equation}
\end{thm}

\begin{defn}[Likelihood Ratio test statistic]
  \label{sec:param-test-theory-1}
  Suppose we observe $Y_{1}, \dots, Y_{n}$ from $f(\theta, \cdot)$,
  and consider the testing problem $H_{0}: \theta \in \Theta_{0}$
  against $H_{1}: \theta \in \Theta$, where $\Theta_{0} \subset \Theta
  \subset \R^{p}$. The Neyman-Pearson theory suggests to test these
  hypothesis by the likelihood ratio test statistic
  \begin{equation}
    \label{eq:22}
    \Lambda_{n}(\Theta, \Theta_{0}) = 2 \log \frac{\sup_{\theta \in
        \Theta} \prod_{i=1}^{n} f(\theta, Y_{i})}{\sup_{\theta \in
        \Theta_{0}} \prod_{i=1}^{n} f(\theta, Y_{i})}
  \end{equation} which in terms of the maximum likelihood estimators
  $\hat \theta_{n}, \hat \theta_{n, 0}$ of the models $\Theta,
  \Theta_{0}$ is
  \begin{equation}
    \label{eq:23}
    \Lambda_{n}(\Theta, \Theta_{0}) = -2 \sum_{i=1}^{n} \log f(\hat
    \theta_{n, 0}, Y_{i}) - \log f(\hat \theta_{n}, Y_{i}).
  \end{equation}
\end{defn}

\begin{thm}
  Consider a parametric model $f(\theta, y), \theta \in \Theta \subset
  \R^{p}$ that satisfies the assumptions of the theorem on asymptotic
  normality of the MLE.  Consider the simple null hypothesis
  $\Theta_{0} = \{ \theta_{0} \} $, $\theta_{0} \in \Theta$.  Then
  under $H_{0}$, the likelihood ratio test statistic is asymptotically
  chi-squared distributed, so
  \begin{equation}
    \label{eq:24}
    \Lambda_{n}(\Theta, \Theta_{0}) \cd \chi_{p}^{2}
  \end{equation} as $n \rightarrow \infty$ under $P_{\theta_{0}}$.
\end{thm}

\begin{proof}
  Since $\Lambda_{n}(\Theta, \Theta_{0}) = 2n Q_{n}(\theta_{0}) - 2n
  Q_{n}(\hat \theta_{n})$, we can expand this in a Taylor series
  around $\hat \theta_{n}$, obtaining
  \begin{align}
    \label{eq:58}
    2 n \frac{\partial Q_{n}(\hat \theta_{n})}{\partial
      \theta}^{T}(\theta_{0} - \hat \theta_{n}) + n(\theta_{0} - \hat
    \theta_{n})^{T} \frac{\partial^{2} Q(\overline
      \theta_{n})}{\partial \theta \partial \theta^{T}} (\theta_{0} -
    \hat \theta_{n})
  \end{align} for some vector $\overline \theta_{n}$ on the line
  segment between $\hat \theta_{n}$and $\theta_{0}$.

  As in the proof of the previous theorem, we show that $\overline
  A_{n}$ converges to $i(\theta_{0})$ in probability.  Thus, by
  Slutsky's lemma and consistency, $\sqrt{n}(\hat \theta_{n} -
  \theta_{0})^{T}(\overline A_{n} - i(\theta_{0}))$ converges to zero
  in distribution and probability (as the limit is constant), so we can
  repeat the argument and obtain
  \begin{align}
    \label{eq:59}
    \sqrt{n} (\hat \theta_{n} - \theta_{0})^{T} (\overline A_{n} -
    i(\theta_{0})) \sqrt{n}(\hat \theta_{n} - \theta_{0})
    \rightarrow^{p_{\theta_{0}}} 0
  \end{align} and so $\Lambda_{n}(\Theta, \Theta_{0})$ has the same
  limit distribution as the random variable
  \begin{equation}
    \label{eq:60}
    \sqrt{n}(\hat \theta_{n} - \theta_{0})^{T} i(\theta_{0})
    \sqrt{n}(\hat \theta_{n} - \theta_{0}).
  \end{equation}  By continuity of $x \mapsto x^{T} i(\theta_{0} x)$,
  we obtain the limiting distribution is $X^{T} i(\theta_{0} X)$, with
  $X \sim N(0, i^{-1})(\theta_{0})$, which is the squared Euclidean
  norm of the MVN  $N(0, I_{p})$, which has a $\chi^{2}_{p}$ distribution.
\end{proof}

% \begin{thm}
%   Let $\{ f(\theta), \theta \in \Theta \} $ in which uniformly
%   consistent estimators $T_{n} = T(Y_{1}, \dots, Y_{n})$ exist.  Then
%   there exist tests $\phi_{n} = \phi(Y_{1}, \dots, Y_{n}, \theta_{0})$
%   for $H_{0}: \theta = \theta_{0}$ against $H_{1}: \theta \in \Theta
%   \backslash \{ \theta_{0} \} $ such that for every $\theta$
%   satisfying $\| \theta - \theta_{0} \| > \delta > 0$, for some
%   universal constant $C$,
%   \begin{equation}
%     \label{eq:26}
%     \max(\E_{\theta_{0}} \phi_{n}, \E_{\theta}(1 - \phi_{n})) \leq e^{-Cn}.
%   \end{equation}
% \end{thm}

% \begin{proof}
%   Consider $\psi_{n} = 1_{\{ \| T_{n} - \theta_{0} \| \geq
%     \frac{\delta}{2} \} }$ which converges to zero under
%   $P_{\theta_{0}}$.  By the triangle inequality, we have
%   \begin{align}
%     \label{eq:62}
%     \sup_{\theta : \| \theta - \theta_{0} \| \geq \delta}
%     \E_{\theta}(1 - \psi_{n}) \leq \sup_{\theta: \| \theta -
%       \theta_{0} \| \geq \delta} P^{n}_{\theta}(\| T_{n} - \theta \| >
%     \frac{\delta}{2}) \rightarrow 0
%   \end{align} as $n \rightarrow \infty$.

%   Then set $k \in \N$ so $\E_{\theta_{0}} \psi_{k}$ and $\E_{\theta}(1
%   - \psi_{k})$ for all $\| \theta - \theta_{0} \| > \delta$.  Then
%   group each sample into groups of size $k$, with $Y_{nj}$ the
%   statistic $\psi_{n}$ for the $j-th$ group, and consider the sample
%   average $\overline Y_{nm} = \frac{1}{m} \sum_{j=1}^{m} Y_{nj}$.

%   As $\E_{\theta} Y_{nj} \geq \frac{3}{4}$, we can use the test
%   $\phi_{n} = 1_{\overline Y_{nm} < \frac{1}{2}} \leq \exp(-\frac{m}{8})$ by Hoeffding's inequality.
% \end{proof}

% \begin{thm}
%   Let $\{ f(\theta) : \theta \in \Theta \} $ be a parametric model
%   that satisfies the conditions of the theorem on the asymptotic
%   behavior of the likelihood test statistic. Let $M_{n} \rightarrow
%   \infty$ as $n \rightarrow \infty$.  Then there exist test $\phi_{n}$
%   for $H_{0}: \theta = \theta_{0}$ against $H_{1}: \theta \in \Theta
%   \backslash \{ \theta_{0} \} $ such that $\E_{\theta_{0}} \phi_{n}
%   \rightarrow 0$ as $n \rightarrow \infty$ and, for every $\theta$
%   satisfying $\frac{M_{n}}{\sqrt{n}} < \| \theta - \theta_{0} \| \leq
%   \delta$ for some $\delta < 1$, we have for some universal constant
%   $D$,
%   \begin{equation}
%     \label{eq:27}
%     \E_{\theta}(1 - \phi_{n}) \leq \frac{1}{D} e^{-Dn \| \theta -
%       \theta_{0} \|^{2}} \rightarrow 0
%   \end{equation} as $n \rightarrow \infty$.
% \end{thm}
\subsection{Local Asymptotic Normality and Contiguity}
\label{sec:loccal-asympt-norm}

\begin{defn}[Local Asymptotic Normality]
  \label{sec:local-asympt-norm-1}
  Consider a parametric model $f(\theta) \equiv f(\theta, \cdot),
  \theta \in \Theta \subset \R^{p}$ and let $q(\theta, y) = - \log
  f(\theta, y)$.  Suppose $\frac{\partial}{\partial \theta}
  q(\theta_{0}, y)$ and the Fisher information $i(\theta_{0})$ exist
  at the interior point $\theta_{0} \in \Theta$.  We say that the
  model $\{ f(\theta): \theta \in \Theta \} $ is locally
  asymptotically normal at $\theta_{0}$ if for every convergent
  sequence $h_{n} \rightarrow h$ and for $Y_{1}, \dots, Y_{n}$ \iid
  $\sim f(\theta_{0})$, we have, as $n \rightarrow \infty$,
  \begin{align}
    \label{eq:28}
    \log \prod_{i=1}^{n} \frac{f(\theta_{0} +
      \frac{h_{n}}{\sqrt{n}})}{f(\theta_{0})}(Y_{i}) = -
    \frac{1}{\sqrt{n}} \sum_{i=1}^{n} h^{T} \frac{\partial
      q(\theta_{0}, Y_{i})}{\partial \theta} - \frac{1}{2} h^{T}
    i(\theta_{0}) h + o_{P_{\theta_{0}}}(1).
  \end{align}

  We say that the model $\{ f(\theta): \theta \in \Theta \} $ is
  locally asymptotically normal if it is locally asymptotically normal
  for every $\theta \in \interior \Theta$.
\end{defn}

\begin{thm}
  Consider a parametric model $\{ f(\theta), \theta \in \Theta \} $,
  $\Theta \subset \R^{p}$, that satisfies the assumptions of the
  theorem on the asymptotic normality of the MLE.  Then $\{ f(\theta):
\theta \in \Theta_{0} \} $ is locally asymptotically normal for every
open subset $\Theta_{0}$ of $\Theta$.
\end{thm}

\begin{proof}
  We prove for $n_{n} = h$ fixed.  As before, we can expand $\log
  f(\theta_{0} + \frac{h}{\sqrt{n}})$ about $\log f(\theta_{0})$ up to
  second order, and obtain
  \begin{align}
    \label{eq:10}
    -\frac{1}{\sqrt{n}} \sum_{i=1}^{n} h^{T} \frac{\partial
      q(\theta_{0}, Y_{i})}{\partial \theta}  - \frac{1}{2n} h^{T}
    \sum_{i=1}^{n} \frac{\partial^{2} q(\overline \theta_{n},
      Y_{i})}{\partial \theta \partial \theta^{T}} h
  \end{align} for some vector $\overline \theta_{n}$ on the line
  segment between $\theta_{0}$ and $\theta_{0} + \frac{h}{\sqrt{n}}$.
  By the uniform law of large numbers, we have
  \begin{align}
    \label{eq:11}
    \frac{1}{2n} h^{T} \sum_{i=1}^{n} \frac{\partial^{2} q(\overline
      \theta_{n}, Y_{i})}{\partial \theta \partial \theta^{T}}  h -
    \frac{1}{2} h^{T} i(\theta_{0} h) \rightarrow^{p_{\theta_{0}}} 0
  \end{align} as $n \rightarrow \infty$.
\end{proof}

\begin{defn}[Contiguity]
  \label{sec:cons-param-model}
  Let $P_{n}$, $Q_{n}$ be two sequences of probability measures. We
  say that $Q_{n}$ is contiguous with respect to $P-n$ if for every
  sequence of measurable sets $A_{n}$, the hypothesis $P_{n}(A_{n}
  \rightarrow 0)$ as $n \rightarrow \infty$ implies $Q_{n}(A_{n})
  \rightarrow 0$  as $n \rightarrow \infty$, and write $Q_{n}
  \triangleleft P-n$.  The sequences are mutually contiguous if both
  $Q_{n} \triangleleft P_{n}$ and $P_{n} \triangleleft Q_{n}$, and
  write $P_{n} \triangleleft \triangleright Q_{n}$.
\end{defn}

\begin{thm}[LeCam's First Lemma]
  Let $P-n$, $Q_{n}$ be probability measures on measurable spaces
  $(\Omega_{n}, \mathcal{A}_{n})$.  Then the following are equivalent:
  \begin{enumerate}
  \item\label{item:1} $Q_{n} \triangleleft P_{n}$.
  \item\label{item:2} If $\frac{dP_{n}}{dQ_{n}} \rightarrow^{d}_{Q_{n}} U$ along a
    subsequence of $n$, then $P(U > 0) = 1$.
  \item\label{item:3} If $\frac{dQ_{n}}{dP_{n}} \rightarrow^{d}_{P_{n}} V$ along a
    subsequence of $n$, then $\E{V} = 1$.
  \item\label{item:4} For any sequence of statistics (measurable functions $T_{n}:
    \Omega_{n} \rightarrow \R^{k}$), we have $T_{n}
    \rightarrow^{P_{n}} 0$ as $n \rightarrow \infty$ implies $T_{n}
    \rightarrow^{Q_{n}} 0$ as $n \rightarrow \infty$.
  \end{enumerate}
\end{thm}

\begin{proof}
  \ref{item:1} $\iff$ \ref{item:4}: follows by taking $A_{n} =
    \{ \| T_{n} \| > \epsilon \},$ so $Q_{n}(A_{n}) \rightarrow 0$
    implies $T_{n} \rightarrow^{Q_{n}} 0$.  Conversely, take $T_{n} = 1_{A_{n}}$.
  \ref{item:1} $\Rightarrow$ \ref{item:2}
  \todo{Complete proof?}
\end{proof}


\begin{thm}
  Let $P_{n}$, $Q_{n}$ be sequences of probability measures on
  measurable spaces $(\Omega_{n}, \mathcal{A}_{n})$ such that
  $\frac{dP_{n}}{dQ_{n}} \rightarrow^{d}_{Q_{n}} e^{X}$ where $X \sim
  N(-\frac{1}{2} \sigma^{2}, \sigma^{2})$, for some $\sigma^{2} > 0$
  as $n \rightarrow \infty$. Then $P_{n} \triangleleft \triangleright
  Q_{n}$.
\end{thm}

\begin{proof}
  Since $P(e^{X} > 0) = 1$, so $Q_{n} \triangleleft P_{n}$ from
  \ref{item:2} and since $\E{e^{N(\mu, \sigma^{2})}} = 1 \iff \mu = -
  \frac{\sigma^{2}}{2}$, this follows from \ref{item:3}.
\end{proof}

\begin{thm}
  If $\{ f(\theta): \theta \in \Theta \}$ is locally asymptotically
  normal and if $h_{n} \rightarrow h \in \R^{p}$, then the product
  measures $P^{n}_{\theta + \frac{h_{n}}{\sqrt{n}} }$ and
  $P^{n}_{\theta}$ corresponding to samples $X_{1}, \dots, X_{n}$ from
  densities $f(\theta + \frac{h_{n}}{\sqrt{n}})$ and $f(\theta)$,
  respectively, are mutually contiguous. In particular, if a statistic
  $T(Y_{1}, \dots, Y_{n})$ converges to zero in probability under
  $P^{n}_{\theta}$ then it also converges to zero in $P^{n}_{\theta +
    \frac{h_{n}}{\sqrt{n}} }$-probability.
\end{thm}

\begin{proof}
  This follows from the fact that the asymptotic expansion converges
  to $N(-\frac{h^{T} i(\theta) h}{2}, h^{T} i(\theta) h)$ under
  $P_{\theta}$.  Then we can apply \ref{item:4}.
\end{proof}

\subsection{Bayesian Inference}
\label{sec:bayesian-inference}

\begin{defn}
  \label{sec:beginenumerate-}
  $\| P - Q \|_{TV} = \sup_{B \in \mathcal{B}(\R^{p})} |P(B) - Q(B)|$
  is the total variation distance on the set of probability measures
  on the Borel $\sigma$-algebra $\mathcal{B}(\R^{p})$ of $\R^{p}$.
\end{defn}

\begin{thm}[Bernstein-von Mises Theorem]
  Consider a parametric model $\{ f(\theta), \theta \in \Theta \} $,
  $\Theta \subset \R^{p}$, that satisfies the assumptions of the
  theorem on the asymptotic normality of the MLE.  Suppose the model
  admits a uniformly consistent estimator $T_{n}$.  Let $X_{1}, \dots,
  X_{n}$ be \iid from density $f(\theta_{0})$, let $\hat \theta_{n}$
  be the MLE based on that sample, assume the prior measure $\Pi$ is
  defined on the Borel sets of $\R^{p}$ and that $\Pi$ possesses a
  Lebesgue-density $\pi$ that is continuous and positive in a
  neighborhood of $\theta_{0}$. Then, if $\Pi(\cdot | X_{1}, \dots,
  X_{n})$ is the posterior distribution given the sample, we have
  \begin{equation}
    \label{eq:29}
    \| \Pi(\cdot | X_{1}, \dots, X_{n}) - N(\hat \theta_{n},
    \frac{1}{n} i^{-1}(\theta_{0})) \|_{TV}
    \rightarrow^{P_{\theta_{0}}} 0
  \end{equation} as $n \rightarrow \infty$.
\end{thm}


\section{High Dimensional Linear Models}
\label{sec:high-dimens-line-1}

Here, we consider the model $Y = X \theta = \epsilon$, $\epsilon \sim
N(0, \sigma^{2} I_{n})$, $\theta \in \Theta = \R^{p}, \sigma^{2} > 0$,
where $X$ is an $n \times p$ design matrix, and $\epsilon$ is a
standard Gaussian noise vector in $\R^{n}$.  Throughout, we denote the
resulting $p \times p$ \textbf{Gram matrix} as $\hat \Sigma =
\frac{1}{n} X^{T} X$ which is symmetric and positive semidefinite.

Write $a \leqtilde b$ for $a \leq C b$ for some fixed (ideally
harmless) constant $C > 0$.

\begin{thm}
  In the case $p \leq n$, the classical least squares estimator
  introduced by Gauss solves the problem
  \begin{equation}
    \label{eq:30} \min_{\theta \in \R^{p}} \frac{\| Y - X \theta
      \|^{2}}{n}
  \end{equation}
  with the solution $\hat \theta = (X^{T}X)^{-1} X^{T}Y \sim N(0,
  \sigma^{2}(X^{T} X)^{-1})$ where we rely on $X$ having full column
  rank so $X^{T} X$ is invertible.  Assuming $\frac{X^{T} X}{n} =
  I_{p}$, we have
  \begin{align}
    \label{eq:13}
    \frac{1}{n} \E_{\theta} \| X(\hat \theta - \theta) \|_{2e^{2}} =
    \E_{\theta} \| \hat \theta - \theta \|_{2}^{2} \
    \frac{\sigma^{2}}{n} \tr(I_{p}) = \frac{\sigma^{2} p }{n}.
  \end{align}
\end{thm}

\begin{defn}
  \label{sec:case-p-leq}
  $\theta^{0} \in B_{0}(k) \equiv \{ \theta \in \R^{p}, \text{at most
    $k$ nonzero entries} \} $.

  For $\theta^{0} \in B_{0}(k)$, call $S_{0} = \{ j : \theta_{j}^{0}
  \neq 0 \} $ the \textbf{active set} of $\theta^{0}$.
\end{defn}

\subsection{The LASSO}
\label{sec:lasso}

\begin{defn}[The LASSO]
  \label{sec:case-p-leq-1}
  The $\tilde \theta = \tilde \theta_{LASSO} = \argmin_{\theta \in
    \R^{p}} \frac{\| Y - X \theta \|_{2}^{2}}{n}  + \lambda \| \theta \|_{1}$.
\end{defn}

\begin{thm}[The LASSO performs almost as well as the LS estimator]
  Let $\theta^{0} \in B_{0}(k)$ be a $k$-sparse vector in $\R^{p}$
  with active set $S_{0}$.  Suppose $Y = X \theta_{0} + \epsilon$
  where $\epsilon \sim N(0, I_{n})$, and let $\tilde \theta$ be the
  LASSO estimator with penalization parameter
  \begin{align}
    \label{eq:31}
    \lambda = 4 \overline \sigma \sqrt{\frac{t^{2} + 2 \log p}{n} },
    \overline \sigma^{2} = \max_{j = 1, \dots, p} \hat \Sigma_{jj},
  \end{align} and assume the $n \times p$ matrix $X$ is such that, for
  some $r_{0} > 0$,
  \begin{equation}
    \label{eq:33}
    \frac{1}{n} \| \tilde \theta_{S_{0}} - \theta^{0} \|_{1}^{2} \leq
    k r_{0}(\tilde \theta - \theta^{0})^{T} \hat \Sigma (\tilde \theta
    - \theta^{0})
  \end{equation} on an event of probability at least $1 - \beta$.
  Then with probability at least $1 - \beta - \exp^{-\frac{t^{2}}{2}}$
  we have
  \begin{align}
    \label{eq:34}
    \frac{1}{n} \| X(\tilde \theta - \theta^{0}) \|_{2}^{2} + \lambda
    \| \tilde \theta - \theta^{0} \|_{1} \leq 4 \lambda^{2} k r_{0}
    \leqtilde \frac{k}{n} \times \log p.
  \end{align}
\end{thm}

\begin{proof}
  Note that by definition we have
  \begin{align}
    \frac{1}{n} \| Y - X \tilde{ \theta{ \|_2}}^2 + \lambda{ \|
      \tilde{ \theta{ \|_1}}} \leq \frac{1}{n} \| Y - X \theta{^0}
    \|_{2}^{2} + \lambda \| \theta^{0} \|_{1} \\
    \frac{1}{n} \| X(\theta^{0} - \tilde \theta) \|_{2}^{2} + \lambda
    \| \tilde \theta \|_{1} \leq \frac{2}{n} \epsilon^{T}X(\tilde
    \theta - \theta^{0}) + \lambda \| \theta^{0} \|_{1}.
  \end{align} by inserting the model equation.

  Using the tail bound on the next theorem, we have on an event $A$,
  \begin{align}
    \label{eq:12}
    |\frac{2 \epsilon^{T} X(\tilde \theta - \theta^{0})}{n}| \leq
    \frac{\lambda}{2} \| \tilde \theta - \theta^{0} \|_{1}.
  \end{align}
  and thus combining with the above result obtain
  \begin{align}
    \label{eq:45}
    \frac{2}{n} \| X(\theta^{0} - \tilde \theta) \|_{2}^{2} + 2
    \lambda \| \tilde \theta \|_{1} \leq \setminus \| \tilde \theta -
    \theta^{0} \|_{1} + 2 \lambda \| \theta^{0} \|_{1}.
  \end{align}

  Using $\|\tilde \theta \|_{1} = \| \tilde \theta_{S_{0}} \|_{1} + \|
  \tilde \theta_{S_{0}^{c}} \|_{1} \geq \| \theta_{S_{0}}^{0} \|_{1} -
  \| \tilde \theta_{S_{0}} - \theta^{0}_{S_{0}} \|_{1} + \| \tilde
  \theta_{S_{0}}^{c} \|_{1}$ we obtain on this event, noting
  $\theta_{S_{0}^{c}}^{0} = 0$ by definition of $S_{0}$
  \begin{align}
    \label{eq:61}
    \frac{2}{n} \| X(\theta^{0} - \tilde \theta) \|_{2}^{2} + 2
    \lambda \| \tilde \theta_{S_{0}^{c}} \|_{1} \leq 3 \lambda \|
    \tilde \theta_{S_{0}} - \theta^{0}_{S_{0}} \|_{1} + \lambda \|
    \tilde \theta_{S_{0}^{c}} \|_{1}
  \end{align} and so
  \begin{align}
    \label{eq:63}
    \frac{2}{n} \|X(\theta^{0} - \tilde \theta) \|_{2}^{2} + \lambda
    \| \tilde \theta_{S_{0}^{c}} \|_{1} \leq 3 \lambda \| \tilde
    \theta_{S_{0}} - \theta^{0}_{S_{0}} \|_{1}
  \end{align} holds on the event.

  Then we have
  \begin{align}
    \label{eq:64}
    \frac{2}{n} \| X(\tilde \theta - \theta^{0})\|_{2}^{2} + \lambda
    \| \tilde \theta - \theta^{0} \|_{1} &= \frac{2}{n} \| X(\tilde
    \theta - \theta^{0}) \|_{2}^{2} + \lambda\ |\tilde \theta_{S_{0}}
    - \theta^{0}_{S_{0}} \|_{1} + \lambda \| \tilde \theta_{S_{0}^{c}}
    \|_{1} \\
    &\leq 4 \lambda \| \tilde \theta_{S_{0}} - \theta^{0}_{S_{0}}
    \|_{1} \\
    &\leq 4 \lambda \sqrt{\frac{kr_{0}}{n}} \| X(\tilde \theta -
    \theta^{0}) \|_{2} \\
    &\leq \frac{1}{n} \| X(\tilde \theta - \theta^{0}) \|_{2}^{2} + 4
    \lambda^{2} k r_{0}
  \end{align} using the previous inequalities and $4ab \leq a^{2} + 4b^{2}$.
\end{proof}

\begin{thm}
  Let $\lambda_{0} = \frac{\lambda}{2}$.  The for all $t > 0$,
  \begin{equation}
    \label{eq:35}
    \Prob{\max_{j = 1, \dots, p} \frac{2}{n} |(\epsilon^{T} X)_{j}|
      \leq \lambda_{0}} \geq 1 - \exp(-\frac{t^{2}}{2}).
  \end{equation}
\end{thm}

\begin{proof}
  Note that $\frac{\epsilon^{T} X}{\sqrt{n}}$ are $N(0, \hat \Sigma)$
  distributed.  We then have the probability in questions exceeds one
  minus
  \begin{align}
    \label{eq:65}
    &\Prob{\max_{j = 1, \dots, p} \frac{1}{\sqrt{n}} |(\epsilon^{T} X)
      | > \overline \sigma \sqrt{t^{2} + 2 \log p}} \\
    &\leq \sum_{j=1}^{p} \Prob{|Z| > \sqrt{t^{2} + 2 \log p}} \\
    &\leq pe^{-\frac{t^{2}}{2} \exp^{-\log p}} = e^{-\frac{t^{2}}{2}}.
  \end{align}
\end{proof}

\subsection{Coherence Conditions for Design Matrices}
\label{sec:coher-cond-design}

The critical condition is
\begin{equation}
  \label{eq:36}
  \| \tilde \theta_{S_{0}} - \theta^{0} \|_{1}^{2} \leq k r_{0}(\tilde
  \theta - \theta^{0})^{T} \hat \Sigma (\tilde \theta - \theta^{0})
\end{equation} holding true with high probability.

\begin{thm}
  The theorem on LASSO holds true with the crucial condition
  (\ref{eq:36}) replaced with the following condition: For $S_{0}$,
  the active set of $\theta^{0} \in B_{0}(k)$, $k \leq p$, assume the
  $n \times p$ matrix $X$ satisfies, for all $\theta$ in
  \begin{equation}
    \label{eq:37}
    \{ \theta \in \R^{p} : \| \theta_{S_{0}^{c}} \|_{1} \leq 3 \|
    \theta_{S_{0}} - \theta_{S_{0}}^{0} \|_{1} \}
  \end{equation} and some universal constant $r_{0}$,
  \begin{equation}
    \label{eq:38}
    \| \theta_{S_{0}} - \theta^{0} \|_{1}^{2} \leq k r_{0} (\theta -
    \theta^{0})^{T} \hat \Sigma (\theta - \theta^{0}).
  \end{equation}
\end{thm}

\begin{thm}
  Let the $n \times p$ matrix $X$ have entries $(X_{ij}) \sim^{\iid}
  N(0, 1)$  and let $\hat \Sigma = \frac{X^{T} X}{n}$.  Suppose
  $\frac{n}{\log p}  \rightarrow \infty$ as $\min(p, n) \rightarrow
  \infty$.  Then for every $k \in \N$ fixed and every $0 < C <
  \infty$, there exists $n$ large enough such that $\Prob{\theta^{T}
    \hat \Sigma \theta \geq \frac{1}{2} \| \theta \|_{2}^{2}\, \forall
  \theta \in B_{0}(k)} \geq 1 - 2 \exp(-Ck \log p)$.
\end{thm}

\begin{proof}
  For $\theta = 0$, the result is trivial.  Thus, it suffices to bound
  \begin{align}
    \label{eq:71}
    &\Prob{\theta^{T} \hat \Sigma \theta \geq \frac{1}{2} \| \theta
      \|_{2}^{2} \forall \theta \in B_{0}(k) \backslash \{ 0 \}} \\
    &= \Prob{\frac{\theta^{T} \hat \Sigma \theta}{\| \theta
        \|_{2}^{2}} - 1 \geq -\frac{1}{2} \forall \theta \in B_{0}(k)
      \backslash \{ 0 \}  } \\
    &\geq \Prob{\sup_{\theta \in B_{0}(k), \| \theta \|_{2}^{2} \neq
        0} \left| \frac{\theta^{T} \hat \Sigma \theta}{\theta^{T}
          \theta} - 1 \right| \leq \frac{1}{2}}
  \end{align} from below by $1 - 2 \exp(-C k \log p)$.  We can then do
  this over each $k$-dimensional subspace $\R_{S}^{p}$ for each $S
  \subset \{ 1, \dots, p \} $ with $|S| = k$, then use
  \begin{align}
    \label{eq:72}
    &\Prob{\sup_{\theta \in B_{0}(k), \| \theta \|_{2}^{2} \neq
        0} \left| \frac{\theta^{T} \hat \Sigma \theta}{\theta^{T}
          \theta} - 1 \right| \leq \frac{1}{2}} \\
    &\leq \sum_{S
      \subseteq \{ 1, \dots, p \}}^{} \Prob{\sup_{\theta \in
        \R^{p}_{S}, \| \theta \|_{2}^{2} \neq 0} \left|
        \frac{\theta^{T} \hat \Sigma \theta}{\theta^{T} \theta} - 1
      \right| \geq \frac{1}{2} }.
  \end{align}
  Then we just need a bound of $2e^{-(C+1) k \log p)} = 2e^{-Ck \log
    p} p^{-k}$  and sum over the ${p \choose k}  \leq p^{k}$ subsets.

  Using the below result and taking $t = (C+1) k \log p$ is then sufficient.
\end{proof}

\begin{thm}
  Under the conditions of the previous theorem, we have for some
  universal constant $c_{0} > 0$, every $S \subset \{ 1, \dots, p \}$
  such that $|S| = k$ and every $t > 0$,
  \begin{equation}
    \label{eq:39}
    \Prob{\sup_{\theta \in \R^{p}_{S}, \| \theta \|_{2}^{2} \neq 0} |
      \frac{\theta^{T} \hat \Sigma \theta}{\theta^{T} \theta}  - 1 |
      \geq 18 (\sqrt{\frac{t + c_{0} k}{n}} + \frac{t + c_{0} k}{n} )
    } \leq 2 e^{-t}.
  \end{equation}
\end{thm}

\begin{proof}[Nontrivial!]
  Note
  \begin{align}
    \label{eq:73}
    \sup_{\theta \in \R_{S}^{p}, \| \theta \|_{2}^{2} \neq 0} \left|
      \frac{\theta^{T} \hat \Sigma \theta}{\theta^{T} \theta} - 1
    \right| = \sup_{\theta \in \R^{p}_{S}, \| \theta \|_{2}^{2} \leq
      1}  | \theta^{T} (\hat \Sigma - 1) \theta|
  \end{align}

  By compactness, we can cover the unit ball $B(S) = \{ \theta \in
  \R^{p}_{S} : \| \theta \|_{2} \leq 1 \}$ by a net of points
  $\theta^{l}$  such that for every $\theta \in B(S)$ there exists $l$
  with $\| \theta - \theta^{l}\|_{2} \leq \delta$.  Then with $\Phi =
  \hat \Sigma - I$, we have
  \begin{align}
    \label{eq:74}
    \theta^{T}  \Phi \theta = (\theta - \theta^{l}) \Phi (\theta -
    \theta^{l}) + (\lambda^{l})^{T} \Phi \theta^{l} + 2(\theta -
    \theta^{l})^{T} \Phi \theta^{l}.
  \end{align}
  The second term is bounded by $\delta^{2} \sup_{v \in B(S)}  |v^{T}
  \Phi v|$. The third term is bounded by $2 \delta \sup_{v \in B(S)} |
  v^{T} \Phi v |$.  Thus,
  \begin{align}
    \label{eq:75}
    \sup_{\theta \in B(S)} | \theta^{T} \Phi \theta | \leq \max_{l \in
    1, \dots, n(\theta)} | \theta^{l} \Phi \theta^{l}| + (\delta^{2} +
  2 \delta)\sup_{v \in B(S)} |v^{T} \Phi v |
  \end{align} which gives the bound
  \begin{align}
    \label{eq:76}
    \sup_{\theta \in B(S)} | \theta^{T} \Phi \theta | \leq \frac{9}{2}
    \max_{l = 1, \dots, N(\delta)} |\theta^{l} \Phi \theta^{l}|
  \end{align} at $\delta = \frac{1}{3}$.

  At $\theta^{l} \in B(S)$ fixed, we have
  \begin{align}
    \label{eq:77}
    (\theta^{l})^{T} \Phi \theta^{l} = \frac{1}{n} \sum_{i=1}^{n} ((X
    \theta^{l})_{i}^{2} - \E{(X \theta^{l})_{i}^{2}})
  \end{align} and the random variables $(X \theta^{l})_{i}$ are IID
  $N(0, \| \theta^{l}\|_{2}^{2})$ distributed with variance $\|
  \theta^{l} \|_{2}^{2} \leq 1$.  Thus for $g_{i} \iid N(0, 1)$, we
  have
  \begin{align}
    \label{eq:78}
    &\Prob{\frac{9}{2} \max_{l = 1, \dots, N(\frac{1}{3})} |
      (\theta^{l})^{T} \Phi \theta^{l} | > 18 \left(\sqrt{\frac{t +
            c_{0} k}{n}} + \frac{t + c_{0} k}{n} \right)} \\
    &\leq \sum_{l=1}^{N(\frac{1}{3})} \Prob{|(\theta^{l})^{T} \Phi
      \theta^{l}| > 4 \| \theta^{l} \|_{2}^{2} \left( \sqrt{\frac{t +
            c_{0} k}{n}} + \frac{t + c_{0} k}{n} \right)} \\
    &= \sum_{l=1}^{N(\frac{1}{3})}  \Prob{\left|\sum_{i=1}^{n} (g_{i}^{2} -
        1) \right| > 4 (\sqrt{n(t+ c_{0}k)} + t + c_{0}k)} \\
    &\leq 2N(\frac{1}{3} )e^{-t} e^{-c_{0} k} \\
    &\leq 2e^{-t}
  \end{align} where we apply the next inequality with $z = t +
  c_{0}k$, and rely on the covering numbers of hte unit ball in
  $k$-dimensional Euclidean space satisfying $N(\delta) \leq
  \left(\frac{A}{\delta}\right)^{k}$ for some universal $A > 0$.
\end{proof}

\begin{thm}
  Let $g_{i}, $i = 1, \dots, n be \iid $N(0, 1)$, and set $X =
  \sum_{i=1}^{n} (g_{i}^{2} - 1)$.  Then for all $t \geq 0$ and $n \in
  \N$,
  \begin{equation}
    \label{eq:40}
    \Prob{|X| \geq t} \leq 2 \exp(-\frac{t^{2}}{4(n+t)}).
  \end{equation}
\end{thm}

\begin{proof}
  For $| \lambda | < \frac{1}{2}$, we can compute the MGF of
  $\E{e^{\lambda(g^{2} - 1)}} = \frac{e^{-\lambda}}{\sqrt{1 -
      2\lambda}} = \exp(\frac{1}{2} -\log(1 - 2 \lambda) - 2
  \lambda)$.

  Then taking Taylor expansions, we have
  \begin{align}
    \label{eq:66}
    \frac{1}{2} [- \log (1 - 2 \lambda) - 2 \lambda] = \lambda^{2} (1
    + \frac{2}{3} 2 \lambda + \dots + \frac{2}{k+2}(2\lambda)^{k} +
    \dots) \leq \frac{\lambda^{2}}{1 - 2 \lambda}
  \end{align} and by IID, $\log \E{e^{\lambda X}} \leq \frac{n
    \lambda^{2}}{1 - 2 \lambda} $.

  Then by Markov's inequality, $\Prob{X > t} \leq \E{e^{\lambda X -
      \lambda t}} \leq \exp(\frac{n\lambda^{2}}{1-2\lambda} - \lambda
  t) = \exp(-\frac{t^{2}}{4(n+t)})$, when taking $\lambda =
  \frac{t}{2n + 2t}$.

  Then taking $t = 4(\sqrt{nz} + z)$, we obtain the required result.
\end{proof}

\bibliographystyle{plainnat}
\bibliography{../../common/bibliography}
\end{document}