\chapter{Stochastic Convergence Concepts}
\label{cha:stoch-conv-conc}

\begin{defn}[Random Variable]
  A random variable is a (measurable) mapping $$X: (\Omega, \mathcal{A},
  \mu) \rightarrow \mathbb{R}.$$

  The \textbf{distribution function} is defined by
  \begin{equation}
    \label{eq:7}
    F(t) = P(X \leq t) = \mu(w \in \Omega | X(w) \leq t), t \in
  \mathbb{R}
  \end{equation}
  where $P = \mu \circ X^{-1}$ is the law of $X$.

  A random vector $\mathbf{X}$ is a vector of random variables with joint
  distribution
  \begin{equation}
    \label{eq:8}
F(\mathbf{t}) = P(\mathbf{X} \leq \mathbf{t}) = P(\mathbf{X}_{i} \leq \mathbf{t}_{i}, 1 \leq i \leq n)
  \end{equation}
\end{defn}

\begin{defn}[Convergence almost surely]
A sequence $X_{n}, n \in \mathbb{N}$ of random variables converges
almost surely to a random variable $X$ if
\begin{equation}
  \label{eq:4}
  P(X_{n} \rightarrow X) = \mu(\omega \in \Omega | X_{n}(\omega)
  \rightarrow X(\omega)) = 1
\end{equation}

We say that $X_{n} \cas X$.
\end{defn}

\begin{defn}[Convergence in probability]
$X_{n} \cp X$ (in probability) if for all $\epsilon > 0$,
\begin{equation}
  \label{eq:5}
  P(|X_{n} - X| > \epsilon) \rightarrow 0
\end{equation} as $n \rightarrow \infty$.
For random vectors, we define analogously with taking the norm in
$\mathbb{R}^{n}$. 
\end{defn}

\begin{defn}[Convergence in distribution]
$X_{n} \cd X$ or $X_{n}$ converges to $X$ in distribution if
\begin{equation}
  \label{eq:6}
  P(X_{n} \leq t) \rightarrow P(X \leq t)
\end{equation} whenever $t \mapsto P(X \leq t)$ is continuous.
\end{defn}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "master"
%%% End: 
